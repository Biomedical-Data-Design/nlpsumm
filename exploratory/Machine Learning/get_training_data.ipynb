{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91f12cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from os import listdir\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65c0c47",
   "metadata": {},
   "source": [
    "# Split text and tag\n",
    "\n",
    "Smoker and Family_hist can have no tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be1a3360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(521, 2)\n"
     ]
    }
   ],
   "source": [
    "file_path = \"E:/JHU/课程/datadesign/NLP/data/training-RiskFactors-Complete-Set1/\"\n",
    "names = [f for f in listdir(file_path) if f.endswith('.xml')]\n",
    "train = pd.DataFrame(np.zeros((len(names), 2), dtype=object), columns=['text', 'annotation'])\n",
    "print(train.shape)\n",
    "n = 0\n",
    "for name in names:\n",
    "    tree = ET.parse(file_path + name)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    ## Get the text\n",
    "    nt = re.sub('\\n',' ',root[0].text)\n",
    "    nt = re.sub('\\t',' ',nt) \n",
    "    nt = re.sub('\"',\"'\",nt)\n",
    "    ## sample 214 has a weird character\n",
    "    nt = re.sub('>','&gt;',nt) \n",
    "    nt = re.sub('<','&lt;',nt)\n",
    "    train['text'][n] = nt\n",
    "    n+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a827526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>annotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Record date: 2067-05-03  Narrative History ...</td>\n",
       "      <td>[(ZESTRIL, MEDICATION), (ZESTRIL , MEDICATION)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Record date: 2068-12-05  Narrative History ...</td>\n",
       "      <td>[(Zestril, MEDICATION), (Zestril (LISINOPRIL),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Record Date: 2070-12-01  Narrative History ...</td>\n",
       "      <td>[(Zestril, MEDICATION), (Zestril , MEDICATION)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Record date: 2072-07-27   Narrative History...</td>\n",
       "      <td>[(Zestril (LISINOPRIL), MEDICATION), (Zestril,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Record date: 2075-01-31   Narrative History...</td>\n",
       "      <td>[(Zestril (LISINOPRIL), MEDICATION), (Zestril,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0     Record date: 2067-05-03  Narrative History ...   \n",
       "1     Record date: 2068-12-05  Narrative History ...   \n",
       "2     Record Date: 2070-12-01  Narrative History ...   \n",
       "3     Record date: 2072-07-27   Narrative History...   \n",
       "4     Record date: 2075-01-31   Narrative History...   \n",
       "\n",
       "                                          annotation  \n",
       "0  [(ZESTRIL, MEDICATION), (ZESTRIL , MEDICATION)...  \n",
       "1  [(Zestril, MEDICATION), (Zestril (LISINOPRIL),...  \n",
       "2  [(Zestril, MEDICATION), (Zestril , MEDICATION)...  \n",
       "3  [(Zestril (LISINOPRIL), MEDICATION), (Zestril,...  \n",
       "4  [(Zestril (LISINOPRIL), MEDICATION), (Zestril,...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 0\n",
    "for name in names:\n",
    "    tree = ET.parse(file_path + name)\n",
    "    root = tree.getroot()\n",
    "    ## Get the labels\n",
    "    # skip FAMILY_HIST: doesn't have a text\n",
    "    #skip = [root[1][x].tag for x in range(len(root[1]))].index('FAMILY_HIST')\n",
    "    PHI = [root[1][x].tag for x in range(len(root[1]))].index('PHI') \n",
    "    # get all labels except for PHI\n",
    "    tag_list = []\n",
    "    for k in range(len(root[1])):\n",
    "        for m in range(len(root[1][k])):\n",
    "            if root[1][k][m].attrib.keys().__contains__('text') == False:\n",
    "                continue\n",
    "            tag_list.append((root[1][k][m].attrib['text'],root[1][k][m].tag))\n",
    "    # get PHI labels\n",
    "    for k in range(PHI,len(root[1])):\n",
    "        tag_list.append((root[1][k].attrib['text'],root[1][k].tag))\n",
    "    train['annotation'][n] = tag_list\n",
    "    n+=1\n",
    "\n",
    "train.head()\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93ebd4f",
   "metadata": {},
   "source": [
    "# Functions to convert dataframe into input format\n",
    "## use wisely, very slow (520 files takes 40 min to create labels)\n",
    " Please use create_labs function instead of mark_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4e3a81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "[4]\n",
      "[5]\n",
      "[6]\n",
      "[7]\n",
      "[8]\n",
      "[9]\n",
      "[10]\n",
      "[11]\n",
      "[12]\n",
      "[13]\n",
      "[14]\n",
      "[15]\n",
      "[16]\n",
      "[17]\n",
      "[18]\n",
      "[19]\n",
      "[20]\n",
      "[21]\n",
      "[22]\n",
      "[23]\n",
      "[24]\n",
      "[25]\n",
      "[26]\n",
      "[27]\n",
      "[28]\n",
      "[29]\n",
      "[30]\n",
      "[31]\n",
      "[32]\n",
      "[33]\n",
      "[34]\n",
      "[35]\n",
      "[36]\n",
      "[37]\n",
      "[38]\n",
      "[39]\n",
      "[40]\n",
      "[41]\n",
      "[42]\n",
      "[43]\n",
      "[44]\n",
      "[45]\n",
      "[46]\n",
      "[47]\n",
      "[48]\n",
      "[49]\n",
      "[50]\n",
      "[51]\n",
      "[52]\n",
      "[53]\n",
      "[54]\n",
      "[55]\n",
      "[56]\n",
      "[57]\n",
      "[58]\n",
      "[59]\n",
      "[60]\n",
      "[61]\n",
      "[62]\n",
      "[63]\n",
      "[64]\n",
      "[65]\n",
      "[66]\n",
      "[67]\n",
      "[68]\n",
      "[69]\n",
      "[70]\n",
      "[71]\n",
      "[72]\n",
      "[73]\n",
      "[74]\n",
      "[75]\n",
      "[76]\n",
      "[77]\n",
      "[78]\n",
      "[79]\n",
      "[80]\n",
      "[81]\n",
      "[82]\n",
      "[83]\n",
      "[84]\n",
      "[85]\n",
      "[86]\n",
      "[87]\n",
      "[88]\n",
      "[89]\n",
      "[90]\n",
      "[91]\n",
      "[92]\n",
      "[93]\n",
      "[94]\n",
      "[95]\n",
      "[96]\n",
      "[97]\n",
      "[98]\n",
      "[99]\n",
      "[100]\n",
      "[101]\n",
      "[102]\n",
      "[103]\n",
      "[104]\n",
      "[105]\n",
      "[106]\n",
      "[107]\n",
      "[108]\n",
      "[109]\n",
      "[110]\n",
      "[111]\n",
      "[112]\n",
      "[113]\n",
      "[114]\n",
      "[115]\n",
      "[116]\n",
      "[117]\n",
      "[118]\n",
      "[119]\n",
      "[120]\n",
      "[121]\n",
      "[122]\n",
      "[123]\n",
      "[124]\n",
      "[125]\n",
      "[126]\n",
      "[127]\n",
      "[128]\n",
      "[129]\n",
      "[130]\n",
      "[131]\n",
      "[132]\n",
      "[133]\n",
      "[134]\n",
      "[135]\n",
      "[136]\n",
      "[137]\n",
      "[138]\n",
      "[139]\n",
      "[140]\n",
      "[141]\n",
      "[142]\n",
      "[143]\n",
      "[144]\n",
      "[145]\n",
      "[146]\n",
      "[147]\n",
      "[148]\n",
      "[149]\n",
      "[150]\n",
      "[151]\n",
      "[152]\n",
      "[153]\n",
      "[154]\n",
      "[155]\n",
      "[156]\n",
      "[157]\n",
      "[158]\n",
      "[159]\n",
      "[160]\n",
      "[161]\n",
      "[162]\n",
      "[163]\n",
      "[164]\n",
      "[165]\n",
      "[166]\n",
      "[167]\n",
      "[168]\n",
      "[169]\n",
      "[170]\n",
      "[171]\n",
      "[172]\n",
      "[173]\n",
      "[174]\n",
      "[175]\n",
      "[176]\n",
      "[177]\n",
      "[178]\n",
      "[179]\n",
      "[180]\n",
      "[181]\n",
      "[182]\n",
      "[183]\n",
      "[184]\n",
      "[185]\n",
      "[186]\n",
      "[187]\n",
      "[188]\n",
      "[189]\n",
      "[190]\n",
      "[191]\n",
      "[192]\n",
      "[193]\n",
      "[194]\n",
      "[195]\n",
      "[196]\n",
      "[197]\n",
      "[198]\n",
      "[199]\n",
      "[200]\n",
      "[201]\n",
      "[202]\n",
      "[203]\n",
      "[204]\n",
      "[205]\n",
      "[206]\n",
      "[207]\n",
      "[208]\n",
      "[209]\n",
      "[210]\n",
      "[211]\n",
      "[212]\n",
      "[213]\n",
      "[214]\n",
      "[215]\n",
      "[216]\n",
      "[217]\n",
      "[218]\n",
      "[219]\n",
      "[220]\n",
      "[221]\n",
      "[222]\n",
      "[223]\n",
      "[224]\n",
      "[225]\n",
      "[226]\n",
      "[227]\n",
      "[228]\n",
      "[229]\n",
      "[230]\n",
      "[231]\n",
      "[232]\n",
      "[233]\n",
      "[234]\n",
      "[235]\n",
      "[236]\n",
      "[237]\n",
      "[238]\n",
      "[239]\n",
      "[240]\n",
      "[241]\n",
      "[242]\n",
      "[243]\n",
      "[244]\n",
      "[245]\n",
      "[246]\n",
      "[247]\n",
      "[248]\n",
      "[249]\n",
      "[250]\n",
      "[251]\n",
      "[252]\n",
      "[253]\n",
      "[254]\n",
      "[255]\n",
      "[256]\n",
      "[257]\n",
      "[258]\n",
      "[259]\n",
      "[260]\n",
      "[261]\n",
      "[262]\n",
      "[263]\n",
      "[264]\n",
      "[265]\n",
      "[266]\n",
      "[267]\n",
      "[268]\n",
      "[269]\n",
      "[270]\n",
      "[271]\n",
      "[272]\n",
      "[273]\n",
      "[274]\n",
      "[275]\n",
      "[276]\n",
      "[277]\n",
      "[278]\n",
      "[279]\n",
      "[280]\n",
      "[281]\n",
      "[282]\n",
      "[283]\n",
      "[284]\n",
      "[285]\n",
      "[286]\n",
      "[287]\n",
      "[288]\n",
      "[289]\n",
      "[290]\n",
      "[291]\n",
      "[292]\n",
      "[293]\n",
      "[294]\n",
      "[295]\n",
      "[296]\n",
      "[297]\n",
      "[298]\n",
      "[299]\n",
      "[300]\n",
      "[301]\n",
      "[302]\n",
      "[303]\n",
      "[304]\n",
      "[305]\n",
      "[306]\n",
      "[307]\n",
      "[308]\n",
      "[309]\n",
      "[310]\n",
      "[311]\n",
      "[312]\n",
      "[313]\n",
      "[314]\n",
      "[315]\n",
      "[316]\n",
      "[317]\n",
      "[318]\n",
      "[319]\n",
      "[320]\n",
      "[321]\n",
      "[322]\n",
      "[323]\n",
      "[324]\n",
      "[325]\n",
      "[326]\n",
      "[327]\n",
      "[328]\n",
      "[329]\n",
      "[330]\n",
      "[331]\n",
      "[332]\n",
      "[333]\n",
      "[334]\n",
      "[335]\n",
      "[336]\n",
      "[337]\n",
      "[338]\n",
      "[339]\n",
      "[340]\n",
      "[341]\n",
      "[342]\n",
      "[343]\n",
      "[344]\n",
      "[345]\n",
      "[346]\n",
      "[347]\n",
      "[348]\n",
      "[349]\n",
      "[350]\n",
      "[351]\n",
      "[352]\n",
      "[353]\n",
      "[354]\n",
      "[355]\n",
      "[356]\n",
      "[357]\n",
      "[358]\n",
      "[359]\n",
      "[360]\n",
      "[361]\n",
      "[362]\n",
      "[363]\n",
      "[364]\n",
      "[365]\n",
      "[366]\n",
      "[367]\n",
      "[368]\n",
      "[369]\n",
      "[370]\n",
      "[371]\n",
      "[372]\n",
      "[373]\n",
      "[374]\n",
      "[375]\n",
      "[376]\n",
      "[377]\n",
      "[378]\n",
      "[379]\n",
      "[380]\n",
      "[381]\n",
      "[382]\n",
      "[383]\n",
      "[384]\n",
      "[385]\n",
      "[386]\n",
      "[387]\n",
      "[388]\n",
      "[389]\n",
      "[390]\n",
      "[391]\n",
      "[392]\n",
      "[393]\n",
      "[394]\n",
      "[395]\n",
      "[396]\n",
      "[397]\n",
      "[398]\n",
      "[399]\n",
      "[400]\n",
      "[401]\n",
      "[402]\n",
      "[403]\n",
      "[404]\n",
      "[405]\n",
      "[406]\n",
      "[407]\n",
      "[408]\n",
      "[409]\n",
      "[410]\n",
      "[411]\n",
      "[412]\n",
      "[413]\n",
      "[414]\n",
      "[415]\n",
      "[416]\n",
      "[417]\n",
      "[418]\n",
      "[419]\n",
      "[420]\n",
      "[421]\n",
      "[422]\n",
      "[423]\n",
      "[424]\n",
      "[425]\n",
      "[426]\n",
      "[427]\n",
      "[428]\n",
      "[429]\n",
      "[430]\n",
      "[431]\n",
      "[432]\n",
      "[433]\n",
      "[434]\n",
      "[435]\n",
      "[436]\n",
      "[437]\n",
      "[438]\n",
      "[439]\n",
      "[440]\n",
      "[441]\n",
      "[442]\n",
      "[443]\n",
      "[444]\n",
      "[445]\n",
      "[446]\n",
      "[447]\n",
      "[448]\n",
      "[449]\n",
      "[450]\n",
      "[451]\n",
      "[452]\n",
      "[453]\n",
      "[454]\n",
      "[455]\n",
      "[456]\n",
      "[457]\n",
      "[458]\n",
      "[459]\n",
      "[460]\n",
      "[461]\n",
      "[462]\n",
      "[463]\n",
      "[464]\n",
      "[465]\n",
      "[466]\n",
      "[467]\n",
      "[468]\n",
      "[469]\n",
      "[470]\n",
      "[471]\n",
      "[472]\n",
      "[473]\n",
      "[474]\n",
      "[475]\n",
      "[476]\n",
      "[477]\n",
      "[478]\n",
      "[479]\n",
      "[480]\n",
      "[481]\n",
      "[482]\n",
      "[483]\n",
      "[484]\n",
      "[485]\n",
      "[486]\n",
      "[487]\n",
      "[488]\n",
      "[489]\n",
      "[490]\n",
      "[491]\n",
      "[492]\n",
      "[493]\n",
      "[494]\n",
      "[495]\n",
      "[496]\n",
      "[497]\n",
      "[498]\n",
      "[499]\n",
      "[500]\n",
      "[501]\n",
      "[502]\n",
      "[503]\n",
      "[504]\n",
      "[505]\n",
      "[506]\n",
      "[507]\n",
      "[508]\n",
      "[509]\n",
      "[510]\n",
      "[511]\n",
      "[512]\n",
      "[513]\n",
      "[514]\n",
      "[515]\n",
      "[516]\n",
      "[517]\n",
      "[518]\n",
      "[519]\n",
      "[520]\n"
     ]
    }
   ],
   "source": [
    "def matcher(string, pattern):\n",
    "    '''\n",
    "    Return the start and end index of any pattern present in the text.\n",
    "    '''\n",
    "    match_list = []\n",
    "    pattern = pattern.strip()\n",
    "    seqMatch = SequenceMatcher(None, string, pattern, autojunk=False)\n",
    "    match = seqMatch.find_longest_match(0, len(string), 0, len(pattern))\n",
    "    if (match.size == len(pattern)):\n",
    "        start = match.a\n",
    "        end = match.a + match.size\n",
    "        match_tup = (start, end)\n",
    "        string = string.replace(pattern, \"X\" * len(pattern), 1)\n",
    "        match_list.append(match_tup)\n",
    "        \n",
    "    return match_list, string\n",
    "\n",
    "\n",
    "def mark_sentence(s, match_list):\n",
    "    '''\n",
    "    Marks all the entities in the sentence as per the BIO scheme. \n",
    "    '''\n",
    "    word_dict = {}\n",
    "    for word in s.split():\n",
    "        word_dict[word] = 'O'\n",
    "        \n",
    "    for start, end, e_type in match_list:\n",
    "        temp_str = s[start:end]\n",
    "        tmp_list = temp_str.split()\n",
    "        if len(tmp_list) > 1:\n",
    "            word_dict[tmp_list[0]] = 'B-' + e_type\n",
    "            for w in tmp_list[1:]:\n",
    "                word_dict[w] = 'I-' + e_type\n",
    "        else:\n",
    "            word_dict[temp_str] = 'B-' + e_type\n",
    "    return word_dict\n",
    "\n",
    "## replace !!mark_sentence!! to better label the text with more than one word\n",
    "def create_labs(s,match_list):\n",
    "    labs = ['O' for i in range(len(s.split()))]\n",
    "    word_dict = pd.DataFrame({'word':s.split(),'label':labs})\n",
    "\n",
    "    for start, end, e_type in match_list:\n",
    "        index = len(re.findall(r' +',s[0:start]))-1\n",
    "        num_words = len(s[start:end].split())\n",
    "\n",
    "        if num_words > 1:\n",
    "            word_dict.loc[index,'label'] = 'B-' + e_type\n",
    "            for i in range(1,num_words):\n",
    "                word_dict.loc[index+i,'label'] = 'I-' + e_type\n",
    "        else:\n",
    "            word_dict.loc[index,'label'] = 'B-' + e_type\n",
    "    return word_dict\n",
    "\n",
    "def clean(text):\n",
    "    '''\n",
    "    Just a helper fuction to add a space before the punctuations for better tokenization\n",
    "    '''\n",
    "    filters = [\"!\", \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"/\", \"*\", \".\", \":\", \";\", \"<\", \"=\", \">\", \"?\", \"@\", \"[\",\n",
    "               \"\\\\\", \"]\", \"_\", \"`\", \"{\", \"}\", \"~\", \"'\"]\n",
    "    for i in text:\n",
    "        if i in filters:\n",
    "            text = text.replace(i, \" \" + i)\n",
    "            \n",
    "    return text\n",
    "\n",
    "def create_data(df, filepath):\n",
    "    '''\n",
    "    The function responsible for the creation of data in the said format.\n",
    "    '''\n",
    "\n",
    "    with open(filepath , 'w') as f:\n",
    "        for text, annotation in zip(df.text, df.annotation): \n",
    "            #text = clean(text)\n",
    "            #text_ = text    \n",
    "            print(train.index[train['text']== text].tolist())    \n",
    "            match_list = []\n",
    "            for i in annotation: \n",
    "                a, text_ = matcher(text, i[0])\n",
    "                match_list.append((a[0][0], a[0][1], i[1]))\n",
    "        \n",
    "            d = create_labs(text, match_list)\n",
    "\n",
    "            #for i in d.keys():\n",
    "            #    f.writelines(i + ' ' + d[i] +'\\n')\n",
    "            #f.writelines('\\n')\n",
    "            for i in range(d.shape[0]):\n",
    "                f.writelines(d['word'][i] + ' ' + d['label'][i] +'\\n')\n",
    "            f.writelines('\\n')\n",
    "def main():\n",
    "    ## An example dataframe.\n",
    "    #data = pd.DataFrame([[\"Horses are too tall and they pretend to care about your feelings\", [(\"Horses\", \"ANIMAL\")]],\n",
    "    #              [\"Who is Shaka Khan?\", [(\"Shaka Khan\", \"PERSON\")]],\n",
    "    #              [\"I like London and Berlin.\", [(\"London\", \"LOCATION\"), (\"Berlin\", \"LOCATION\")]],\n",
    "    #              [\"There is a banyan tree in the courtyard\", [(\"banyan tree\", \"TREE\")]]], columns=['text', 'annotation'])\n",
    "    data = train.copy()\n",
    "    #print(data.head())\n",
    "    ## path to save the txt file.\n",
    "    filepath = 'E:/JHU/课程/datadesign/NLP/machine_learning/tttt.txt'\n",
    "    ## creating the file.\n",
    "    create_data(data, filepath)\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adae8747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(336882, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Record</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>date:</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2067-05-03</td>\n",
       "      <td>B-PHI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Narrative</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>History</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         text  label\n",
       "0      Record      O\n",
       "1       date:      O\n",
       "2  2067-05-03  B-PHI\n",
       "3   Narrative      O\n",
       "4     History      O"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read txt file\n",
    "def read_txt(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        data = f.readlines()\n",
    "    return data\n",
    "\n",
    "list = read_txt('E:/JHU/课程/datadesign/NLP/machine_learning/trainingdata.txt')\n",
    "#print(list)\n",
    "for i in range(len(list)):\n",
    "    list[i] = list[i].split()\n",
    "#print(list)\n",
    "dataframe = pd.DataFrame(list,columns=['text','label'])\n",
    "print(dataframe.shape)\n",
    "dataframe.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fe773fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42418, 19)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "df = dataframe.fillna(method='ffill')\n",
    "df.text.nunique(), df.label.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f6ea5ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-CAD</td>\n",
       "      <td>839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B-DIABETES</td>\n",
       "      <td>704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B-FAMILY_HIST</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B-HYPERLIPIDEMIA</td>\n",
       "      <td>334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B-HYPERTENSION</td>\n",
       "      <td>831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B-MEDICATION</td>\n",
       "      <td>2937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B-OBESE</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>B-PHI</td>\n",
       "      <td>8455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>B-SMOKER</td>\n",
       "      <td>511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I-CAD</td>\n",
       "      <td>2903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I-DIABETES</td>\n",
       "      <td>827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I-FAMILY_HIST</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>I-HYPERLIPIDEMIA</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>I-HYPERTENSION</td>\n",
       "      <td>629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>I-MEDICATION</td>\n",
       "      <td>1651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>I-OBESE</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>I-PHI</td>\n",
       "      <td>3266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>I-SMOKER</td>\n",
       "      <td>1015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>O</td>\n",
       "      <td>311627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               label  counts\n",
       "0              B-CAD     839\n",
       "1         B-DIABETES     704\n",
       "2      B-FAMILY_HIST      10\n",
       "3   B-HYPERLIPIDEMIA     334\n",
       "4     B-HYPERTENSION     831\n",
       "5       B-MEDICATION    2937\n",
       "6            B-OBESE     153\n",
       "7              B-PHI    8455\n",
       "8           B-SMOKER     511\n",
       "9              I-CAD    2903\n",
       "10        I-DIABETES     827\n",
       "11     I-FAMILY_HIST      67\n",
       "12  I-HYPERLIPIDEMIA      87\n",
       "13    I-HYPERTENSION     629\n",
       "14      I-MEDICATION    1651\n",
       "15           I-OBESE      36\n",
       "16             I-PHI    3266\n",
       "17          I-SMOKER    1015\n",
       "18                 O  311627"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('label').size().reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8cc63a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              text\n",
      "0           Record\n",
      "1            date:\n",
      "2       2067-05-03\n",
      "3        Narrative\n",
      "4          History\n",
      "...            ...\n",
      "336877          MD\n",
      "336878      PGY-1,\n",
      "336879       Pager\n",
      "336880      #65261\n",
      "336881      #65261\n",
      "\n",
      "[336882 rows x 1 columns]\n",
      "  (0, 21175)\t1.0\n",
      "  (1, 28179)\t1.0\n",
      "  (2, 6127)\t1.0\n",
      "  (3, 18798)\t1.0\n",
      "  (4, 16015)\t1.0\n",
      "  (5, 8799)\t1.0\n",
      "  (6, 42331)\t1.0\n",
      "  (7, 42122)\t1.0\n",
      "  (8, 42042)\t1.0\n",
      "  (9, 36874)\t1.0\n",
      "  (10, 30743)\t1.0\n",
      "  (11, 30229)\t1.0\n",
      "  (12, 21972)\t1.0\n",
      "  (13, 32275)\t1.0\n",
      "  (14, 12678)\t1.0\n",
      "  (15, 37930)\t1.0\n",
      "  (16, 33491)\t1.0\n",
      "  (17, 33143)\t1.0\n",
      "  (18, 41914)\t1.0\n",
      "  (19, 25101)\t1.0\n",
      "  (20, 11665)\t1.0\n",
      "  (21, 5478)\t1.0\n",
      "  (22, 22888)\t1.0\n",
      "  (23, 26636)\t1.0\n",
      "  (24, 41421)\t1.0\n",
      "  :\t:\n",
      "  (336857, 25101)\t1.0\n",
      "  (336858, 37704)\t1.0\n",
      "  (336859, 33662)\t1.0\n",
      "  (336860, 20619)\t1.0\n",
      "  (336861, 19897)\t1.0\n",
      "  (336862, 15167)\t1.0\n",
      "  (336863, 35170)\t1.0\n",
      "  (336864, 32377)\t1.0\n",
      "  (336865, 13574)\t1.0\n",
      "  (336866, 11176)\t1.0\n",
      "  (336867, 11664)\t1.0\n",
      "  (336868, 12975)\t1.0\n",
      "  (336869, 21987)\t1.0\n",
      "  (336870, 36943)\t1.0\n",
      "  (336871, 12403)\t1.0\n",
      "  (336872, 15069)\t1.0\n",
      "  (336873, 27302)\t1.0\n",
      "  (336874, 24424)\t1.0\n",
      "  (336875, 11096)\t1.0\n",
      "  (336876, 18158)\t1.0\n",
      "  (336877, 17790)\t1.0\n",
      "  (336878, 19695)\t1.0\n",
      "  (336879, 20014)\t1.0\n",
      "  (336880, 98)\t1.0\n",
      "  (336881, 98)\t1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((269505, 42418), (269505,))"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop('label',axis=1)\n",
    "print (X)\n",
    "\n",
    "v = DictVectorizer(sparse=True)\n",
    "X = v.fit_transform(X.to_dict('records'))\n",
    "print(X)\n",
    "y = df.label.values\n",
    "\n",
    "classes = np.unique(y)\n",
    "classes = classes.tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=0)\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "5ab7aef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 12.81, NNZs: 164, Bias: -0.060000, T: 269505, Avg. loss: 0.001726\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 8.94, NNZs: 80, Bias: -0.080000, T: 269505, Avg. loss: 0.000349\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 28.34, NNZs: 803, Bias: -0.090000, T: 269505, Avg. loss: 0.002359\n",
      "Total training time: 0.04 seconds.\n",
      "Norm: 16.00, NNZs: 256, Bias: -0.020000, T: 269505, Avg. loss: 0.001099\n",
      "Total training time: 0.05 seconds.\n",
      "Norm: 9.90, NNZs: 98, Bias: -0.040000, T: 269505, Avg. loss: 0.001062\n",
      "Total training time: 0.04 seconds.\n",
      "Norm: 2.00, NNZs: 4, Bias: -0.020000, T: 269505, Avg. loss: 0.000026\n",
      "Total training time: 0.05 seconds.\n",
      "Norm: 6.78, NNZs: 46, Bias: -0.020000, T: 269505, Avg. loss: 0.000162\n",
      "Total training time: 0.07 seconds.\n",
      "Norm: 88.53, NNZs: 7837, Bias: -0.030000, T: 269505, Avg. loss: 0.004867\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 12.45, NNZs: 155, Bias: -0.050000, T: 269505, Avg. loss: 0.001070\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 21.40, NNZs: 458, Bias: -0.100000, T: 269505, Avg. loss: 0.006644\n",
      "Total training time: 0.03 seconds.\n",
      "Norm: 15.00, NNZs: 225, Bias: -0.010000, T: 269505, Avg. loss: 0.001721\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 17.41, NNZs: 303, Bias: -0.030000, T: 269505, Avg. loss: 0.001223\n",
      "Total training time: 0.03 seconds.\n",
      "Norm: 6.32, NNZs: 40, Bias: -0.060000, T: 269505, Avg. loss: 0.000195\n",
      "Total training time: 0.02 seconds.\n",
      "Norm: 3.46, NNZs: 12, Bias: -0.040000, T: 269505, Avg. loss: 0.000165\n",
      "Total training time: 0.03 seconds.\n",
      "Norm: 18.03, NNZs: 325, Bias: -0.030000, T: 269505, Avg. loss: 0.003720\n",
      "Total training time: 0.02 seconds.\n",
      "Norm: 4.00, NNZs: 16, Bias: -0.040000, T: 269505, Avg. loss: 0.000082\n",
      "Total training time: 0.02 seconds.\n",
      "Norm: 46.27, NNZs: 2141, Bias: -0.010000, T: 269505, Avg. loss: 0.002667\n",
      "Total training time: 0.03 seconds.\n",
      "Norm: 13.86, NNZs: 192, Bias: -0.140000, T: 269505, Avg. loss: 0.002087\n",
      "Total training time: 0.03 seconds.\n",
      "Norm: 111.30, NNZs: 12387, Bias: 0.010000, T: 269505, Avg. loss: 0.027655\n",
      "Total training time: 0.03 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of  19 | elapsed:    0.0s remaining:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  19 | elapsed:    0.0s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  19 | elapsed:    0.0s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  19 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  19 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  19 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  14 out of  19 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  19 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  19 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Perceptron(max_iter=5, n_jobs=-1, verbose=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Perceptron</label><div class=\"sk-toggleable__content\"><pre>Perceptron(max_iter=5, n_jobs=-1, verbose=10)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "Perceptron(max_iter=5, n_jobs=-1, verbose=10)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per = Perceptron(verbose=10, n_jobs=-1, max_iter=5) #this is the perceptron model\n",
    "per.partial_fit(X_train, y_train,classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "70c4c306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-CAD',\n",
       " 'B-DIABETES',\n",
       " 'B-FAMILY_HIST',\n",
       " 'B-HYPERLIPIDEMIA',\n",
       " 'B-HYPERTENSION',\n",
       " 'B-MEDICATION',\n",
       " 'B-OBESE',\n",
       " 'B-PHI',\n",
       " 'B-SMOKER',\n",
       " 'I-CAD',\n",
       " 'I-DIABETES',\n",
       " 'I-FAMILY_HIST',\n",
       " 'I-HYPERLIPIDEMIA',\n",
       " 'I-HYPERTENSION',\n",
       " 'I-MEDICATION',\n",
       " 'I-OBESE',\n",
       " 'I-PHI',\n",
       " 'I-SMOKER']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_classes = classes.copy()\n",
    "new_classes.pop()\n",
    "new_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "7cd2eb57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "           B-CAD       0.18      0.18      0.18       171\n",
      "      B-DIABETES       0.56      0.36      0.44       141\n",
      "   B-FAMILY_HIST       0.00      0.00      0.00         2\n",
      "B-HYPERLIPIDEMIA       0.49      0.28      0.36        60\n",
      "  B-HYPERTENSION       0.36      0.35      0.35       195\n",
      "    B-MEDICATION       0.72      0.63      0.67       563\n",
      "         B-OBESE       0.58      0.58      0.58        36\n",
      "           B-PHI       0.62      0.29      0.39      1692\n",
      "        B-SMOKER       0.52      0.17      0.25       103\n",
      "           I-CAD       0.24      0.14      0.18       580\n",
      "      I-DIABETES       0.03      0.37      0.05       155\n",
      "   I-FAMILY_HIST       0.00      0.00      0.00        15\n",
      "I-HYPERLIPIDEMIA       0.17      0.08      0.11        12\n",
      "  I-HYPERTENSION       0.03      0.02      0.02       122\n",
      "    I-MEDICATION       0.26      0.19      0.22       298\n",
      "         I-OBESE       0.00      0.00      0.00         7\n",
      "           I-PHI       0.50      0.33      0.40       676\n",
      "        I-SMOKER       0.54      0.24      0.33       218\n",
      "               O       0.96      0.95      0.95     62331\n",
      "\n",
      "        accuracy                           0.90     67377\n",
      "       macro avg       0.35      0.27      0.29     67377\n",
      "    weighted avg       0.92      0.90      0.91     67377\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (classification_report(y_pred=per.predict(X_test), y_true=y_test, labels=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "7d54bca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "           B-CAD       0.00      0.00      0.00       171\n",
      "      B-DIABETES       0.80      0.14      0.24       141\n",
      "   B-FAMILY_HIST       0.00      0.00      0.00         2\n",
      "B-HYPERLIPIDEMIA       0.88      0.25      0.39        60\n",
      "  B-HYPERTENSION       0.79      0.10      0.17       195\n",
      "    B-MEDICATION       0.79      0.14      0.24       563\n",
      "         B-OBESE       0.00      0.00      0.00        36\n",
      "           B-PHI       0.00      0.00      0.00      1692\n",
      "        B-SMOKER       0.00      0.00      0.00       103\n",
      "           I-CAD       0.00      0.00      0.00       580\n",
      "      I-DIABETES       0.95      0.12      0.22       155\n",
      "   I-FAMILY_HIST       0.00      0.00      0.00        15\n",
      "I-HYPERLIPIDEMIA       0.00      0.00      0.00        12\n",
      "  I-HYPERTENSION       0.00      0.00      0.00       122\n",
      "    I-MEDICATION       0.00      0.00      0.00       298\n",
      "         I-OBESE       0.00      0.00      0.00         7\n",
      "           I-PHI       0.85      0.05      0.09       676\n",
      "        I-SMOKER       0.00      0.00      0.00       218\n",
      "               O       0.93      1.00      0.96     62331\n",
      "\n",
      "        accuracy                           0.93     67377\n",
      "       macro avg       0.32      0.09      0.12     67377\n",
      "    weighted avg       0.88      0.93      0.89     67377\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#support vector machine\n",
    "svm = SGDClassifier(alpha=.0001, max_iter=50,penalty=\"elasticnet\")\n",
    "svm.partial_fit(X_train, y_train,classes=classes)\n",
    "\n",
    "print (classification_report(y_pred=svm.predict(X_test), y_true=y_test, labels=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "fdd86d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "           B-CAD       0.52      0.08      0.14       171\n",
      "      B-DIABETES       0.63      0.28      0.38       141\n",
      "   B-FAMILY_HIST       0.00      0.00      0.00         2\n",
      "B-HYPERLIPIDEMIA       0.83      0.48      0.61        60\n",
      "  B-HYPERTENSION       0.70      0.31      0.43       195\n",
      "    B-MEDICATION       0.74      0.71      0.72       563\n",
      "         B-OBESE       1.00      0.19      0.33        36\n",
      "           B-PHI       0.68      0.32      0.43      1692\n",
      "        B-SMOKER       0.80      0.12      0.20       103\n",
      "           I-CAD       0.48      0.06      0.11       580\n",
      "      I-DIABETES       0.69      0.17      0.28       155\n",
      "   I-FAMILY_HIST       0.00      0.00      0.00        15\n",
      "I-HYPERLIPIDEMIA       0.00      0.00      0.00        12\n",
      "  I-HYPERTENSION       0.22      0.02      0.03       122\n",
      "    I-MEDICATION       0.49      0.10      0.16       298\n",
      "         I-OBESE       0.00      0.00      0.00         7\n",
      "           I-PHI       0.65      0.36      0.46       676\n",
      "        I-SMOKER       0.76      0.31      0.44       218\n",
      "               O       0.95      0.99      0.97     62331\n",
      "\n",
      "        accuracy                           0.94     67377\n",
      "       macro avg       0.53      0.24      0.30     67377\n",
      "    weighted avg       0.92      0.94      0.93     67377\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#naive bayes\n",
    "nb = MultinomialNB(alpha=.01)\n",
    "nb.partial_fit(X_train, y_train,classes=classes)\n",
    "\n",
    "print (classification_report(y_pred=nb.predict(X_test), y_true=y_test, labels=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c578a169",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "Vocabulary not fitted or provided",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [118], line 14\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39m#X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.33, random_state=42)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m text_clf \u001b[39m=\u001b[39m Pipeline([(\u001b[39m'\u001b[39m\u001b[39mvect\u001b[39m\u001b[39m'\u001b[39m, CountVectorizer()),\n\u001b[0;32m      8\u001b[0m                         (\u001b[39m'\u001b[39m\u001b[39mtfidf\u001b[39m\u001b[39m'\u001b[39m, TfidfTransformer()),\n\u001b[0;32m      9\u001b[0m                         (\u001b[39m'\u001b[39m\u001b[39mclf\u001b[39m\u001b[39m'\u001b[39m, SGDClassifier(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhinge\u001b[39m\u001b[39m'\u001b[39m, penalty\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39ml2\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     10\u001b[0m                                             alpha\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m,\n\u001b[0;32m     11\u001b[0m                                             max_iter\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, tol\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)),\n\u001b[0;32m     12\u001b[0m                         ])\n\u001b[1;32m---> 14\u001b[0m predicted \u001b[39m=\u001b[39m text_clf\u001b[39m.\u001b[39;49mpredict(X_test)\n\u001b[0;32m     15\u001b[0m np\u001b[39m.\u001b[39mmean(predicted \u001b[39m==\u001b[39m y_test)\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:457\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[1;34m(self, X, **predict_params)\u001b[0m\n\u001b[0;32m    455\u001b[0m Xt \u001b[39m=\u001b[39m X\n\u001b[0;32m    456\u001b[0m \u001b[39mfor\u001b[39;00m _, name, transform \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter(with_final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m--> 457\u001b[0m     Xt \u001b[39m=\u001b[39m transform\u001b[39m.\u001b[39;49mtransform(Xt)\n\u001b[0;32m    458\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mpredict(Xt, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpredict_params)\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1384\u001b[0m, in \u001b[0;36mCountVectorizer.transform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(raw_documents, \u001b[39mstr\u001b[39m):\n\u001b[0;32m   1381\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1382\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIterable over raw text documents expected, string object received.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m     )\n\u001b[1;32m-> 1384\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_vocabulary()\n\u001b[0;32m   1386\u001b[0m \u001b[39m# use the same matrix-building strategy as fit_transform\u001b[39;00m\n\u001b[0;32m   1387\u001b[0m _, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_count_vocab(raw_documents, fixed_vocab\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:506\u001b[0m, in \u001b[0;36m_VectorizerMixin._check_vocabulary\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    504\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_vocabulary()\n\u001b[0;32m    505\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfixed_vocabulary_:\n\u001b[1;32m--> 506\u001b[0m         \u001b[39mraise\u001b[39;00m NotFittedError(\u001b[39m\"\u001b[39m\u001b[39mVocabulary not fitted or provided\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocabulary_) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    509\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mVocabulary is empty\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNotFittedError\u001b[0m: Vocabulary not fitted or provided"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.33, random_state=42)\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                        ('tfidf', TfidfTransformer()),\n",
    "                        ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                            alpha=1e-3, random_state=42,\n",
    "                                            max_iter=5, tol=None)),\n",
    "                        ])\n",
    "\n",
    "predicted = text_clf.predict(X_test)\n",
    "np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0fdc50dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "lower not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [117], line 9\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m classification_report\n\u001b[0;32m      3\u001b[0m text_clf_svm \u001b[39m=\u001b[39m Pipeline([(\u001b[39m'\u001b[39m\u001b[39mvect\u001b[39m\u001b[39m'\u001b[39m, CountVectorizer()),\n\u001b[0;32m      4\u001b[0m                         (\u001b[39m'\u001b[39m\u001b[39mtfidf\u001b[39m\u001b[39m'\u001b[39m, TfidfTransformer()),\n\u001b[0;32m      5\u001b[0m                         (\u001b[39m'\u001b[39m\u001b[39mclf-svm\u001b[39m\u001b[39m'\u001b[39m, SGDClassifier(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhinge\u001b[39m\u001b[39m'\u001b[39m, penalty\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39ml2\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      6\u001b[0m                                                     alpha\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m,\n\u001b[0;32m      7\u001b[0m                                                     max_iter\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, tol\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)),\n\u001b[0;32m      8\u001b[0m                         ])\n\u001b[1;32m----> 9\u001b[0m text_clf_svm\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     11\u001b[0m predicted_svm \u001b[39m=\u001b[39m text_clf_svm\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[0;32m     12\u001b[0m y \u001b[39m=\u001b[39m classification_report(y_test, predicted_svm)\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:378\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[39m\"\"\"Fit the model.\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \n\u001b[0;32m    354\u001b[0m \u001b[39mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[39m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[0;32m    376\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    377\u001b[0m fit_params_steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_fit_params(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m--> 378\u001b[0m Xt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_steps)\n\u001b[0;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(\u001b[39m\"\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)):\n\u001b[0;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    334\u001b[0m     cloned_transformer \u001b[39m=\u001b[39m clone(transformer)\n\u001b[0;32m    335\u001b[0m \u001b[39m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m--> 336\u001b[0m X, fitted_transformer \u001b[39m=\u001b[39m fit_transform_one_cached(\n\u001b[0;32m    337\u001b[0m     cloned_transformer,\n\u001b[0;32m    338\u001b[0m     X,\n\u001b[0;32m    339\u001b[0m     y,\n\u001b[0;32m    340\u001b[0m     \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    341\u001b[0m     message_clsname\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    342\u001b[0m     message\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message(step_idx),\n\u001b[0;32m    343\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_steps[name],\n\u001b[0;32m    344\u001b[0m )\n\u001b[0;32m    345\u001b[0m \u001b[39m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[39m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    347\u001b[0m \u001b[39m# from the cache.\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[step_idx] \u001b[39m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\memory.py:349\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 349\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:870\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m    869\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(transformer, \u001b[39m\"\u001b[39m\u001b[39mfit_transform\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 870\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mfit_transform(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    871\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    872\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1338\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1330\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1331\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1332\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1333\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1334\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1335\u001b[0m             )\n\u001b[0;32m   1336\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1338\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1340\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1341\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1209\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1207\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[0;32m   1208\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[1;32m-> 1209\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1210\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1211\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:111\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    110\u001b[0m     \u001b[39mif\u001b[39;00m preprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[0;32m    112\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    113\u001b[0m         doc \u001b[39m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:69\u001b[0m, in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[39mapply to a document.\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[39m    preprocessed string\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[39mif\u001b[39;00m lower:\n\u001b[1;32m---> 69\u001b[0m     doc \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39;49mlower()\n\u001b[0;32m     70\u001b[0m \u001b[39mif\u001b[39;00m accent_function \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     71\u001b[0m     doc \u001b[39m=\u001b[39m accent_function(doc)\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\sparse\\_base.py:764\u001b[0m, in \u001b[0;36mspmatrix.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    762\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgetnnz()\n\u001b[0;32m    763\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 764\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(attr \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m not found\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: lower not found"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer()),\n",
    "                        ('tfidf', TfidfTransformer()),\n",
    "                        ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                                    alpha=1e-3, random_state=42,\n",
    "                                                    max_iter=5, tol=None)),\n",
    "                        ])\n",
    "text_clf_svm.fit(X_train, y_train)\n",
    "\n",
    "predicted_svm = text_clf_svm.predict(X_test)\n",
    "y = classification_report(y_test, predicted_svm)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "31e6b9ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nAll the 40 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n40 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py\", line 378, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py\", line 336, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n    return self.func(*args, **kwargs)\n  File \"c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py\", line 870, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1338, in fit_transform\n    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n  File \"c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1209, in _count_vocab\n    for feature in analyze(doc):\n  File \"c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 111, in _analyze\n    doc = preprocessor(doc)\n  File \"c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 69, in _preprocess\n    doc = doc.lower()\n  File \"c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\sparse\\_base.py\", line 764, in __getattr__\n    raise AttributeError(attr + \" not found\")\nAttributeError: lower not found\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [147], line 8\u001b[0m\n\u001b[0;32m      2\u001b[0m parameters_svm \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mvect__ngram_range\u001b[39m\u001b[39m'\u001b[39m: [(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m), (\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)],\n\u001b[0;32m      3\u001b[0m                     \u001b[39m'\u001b[39m\u001b[39mtfidf__use_idf\u001b[39m\u001b[39m'\u001b[39m: (\u001b[39mTrue\u001b[39;00m, \u001b[39mFalse\u001b[39;00m),\n\u001b[0;32m      4\u001b[0m                     \u001b[39m'\u001b[39m\u001b[39mclf-svm__alpha\u001b[39m\u001b[39m'\u001b[39m: (\u001b[39m1e-2\u001b[39m, \u001b[39m1e-3\u001b[39m),\n\u001b[0;32m      5\u001b[0m                     }\n\u001b[0;32m      7\u001b[0m gs_clf_svm \u001b[39m=\u001b[39m GridSearchCV(text_clf_svm, parameters_svm, n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m gs_clf_svm \u001b[39m=\u001b[39m gs_clf_svm\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     10\u001b[0m gs_clf_svm\u001b[39m.\u001b[39mbest_score_\n\u001b[0;32m     11\u001b[0m gs_clf_svm\u001b[39m.\u001b[39mbest_params_\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    869\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    870\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    871\u001b[0m     )\n\u001b[0;32m    873\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 875\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    877\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    879\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1375\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1373\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1374\u001b[0m     \u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1375\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:852\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    845\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m!=\u001b[39m n_candidates \u001b[39m*\u001b[39m n_splits:\n\u001b[0;32m    846\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    847\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcv.split and cv.get_n_splits returned \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    848\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minconsistent results. Expected \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    849\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msplits, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(n_splits, \u001b[39mlen\u001b[39m(out) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m n_candidates)\n\u001b[0;32m    850\u001b[0m     )\n\u001b[1;32m--> 852\u001b[0m _warn_or_raise_about_fit_failures(out, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror_score)\n\u001b[0;32m    854\u001b[0m \u001b[39m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m    855\u001b[0m \u001b[39m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m    856\u001b[0m \u001b[39m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m    857\u001b[0m \u001b[39m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[39mif\u001b[39;00m callable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscoring):\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:367\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[39mif\u001b[39;00m num_failed_fits \u001b[39m==\u001b[39m num_fits:\n\u001b[0;32m    361\u001b[0m     all_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[0;32m    362\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mAll the \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIt is very likely that your model is misconfigured.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    364\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou can try to debug the error by setting error_score=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    365\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    366\u001b[0m     )\n\u001b[1;32m--> 367\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    369\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    370\u001b[0m     some_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[0;32m    371\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mnum_failed_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed out of a total of \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe score on these train-test partitions for these parameters\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    377\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 40 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n40 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py\", line 378, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py\", line 336, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n    return self.func(*args, **kwargs)\n  File \"c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py\", line 870, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1338, in fit_transform\n    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n  File \"c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1209, in _count_vocab\n    for feature in analyze(doc):\n  File \"c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 111, in _analyze\n    doc = preprocessor(doc)\n  File \"c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 69, in _preprocess\n    doc = doc.lower()\n  File \"c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\sparse\\_base.py\", line 764, in __getattr__\n    raise AttributeError(attr + \" not found\")\nAttributeError: lower not found\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters_svm = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "                    'tfidf__use_idf': (True, False),\n",
    "                    'clf-svm__alpha': (1e-2, 1e-3),\n",
    "                    }\n",
    "\n",
    "gs_clf_svm = GridSearchCV(text_clf_svm, parameters_svm, n_jobs=-1)\n",
    "gs_clf_svm = gs_clf_svm.fit(X_train, y_train)\n",
    "\n",
    "gs_clf_svm.best_score_\n",
    "gs_clf_svm.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cca7eea8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'float' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [12], line 55\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msent2tokens\u001b[39m(sent):\n\u001b[0;32m     53\u001b[0m     \u001b[39mreturn\u001b[39;00m [token \u001b[39mfor\u001b[39;00m token, label \u001b[39min\u001b[39;00m sent]\n\u001b[1;32m---> 55\u001b[0m X_train \u001b[39m=\u001b[39m [sent2features(s) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m X_train]\n\u001b[0;32m     56\u001b[0m X_test \u001b[39m=\u001b[39m [sent2features(s) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m X_test]\n\u001b[0;32m     58\u001b[0m crf \u001b[39m=\u001b[39m CRF(algorithm\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlbfgs\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     59\u001b[0m             c1\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m,     \n\u001b[0;32m     60\u001b[0m             c2\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m,\n\u001b[0;32m     61\u001b[0m             max_iterations\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m,\n\u001b[0;32m     62\u001b[0m             all_possible_transitions\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn [12], line 55\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msent2tokens\u001b[39m(sent):\n\u001b[0;32m     53\u001b[0m     \u001b[39mreturn\u001b[39;00m [token \u001b[39mfor\u001b[39;00m token, label \u001b[39min\u001b[39;00m sent]\n\u001b[1;32m---> 55\u001b[0m X_train \u001b[39m=\u001b[39m [sent2features(s) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m X_train]\n\u001b[0;32m     56\u001b[0m X_test \u001b[39m=\u001b[39m [sent2features(s) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m X_test]\n\u001b[0;32m     58\u001b[0m crf \u001b[39m=\u001b[39m CRF(algorithm\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlbfgs\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     59\u001b[0m             c1\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m,     \n\u001b[0;32m     60\u001b[0m             c2\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m,\n\u001b[0;32m     61\u001b[0m             max_iterations\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m,\n\u001b[0;32m     62\u001b[0m             all_possible_transitions\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn [12], line 49\u001b[0m, in \u001b[0;36msent2features\u001b[1;34m(sent)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msent2features\u001b[39m(sent):\n\u001b[1;32m---> 49\u001b[0m     \u001b[39mreturn\u001b[39;00m [word2features(sent, i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39;49m(sent))]\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'float' has no len()"
     ]
    }
   ],
   "source": [
    "#Traing model with CRF\n",
    "from sklearn_crfsuite import CRF\n",
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn_crfsuite import metrics\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "X = dataframe['text'].values.tolist()\n",
    "y = dataframe['label'].values.tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "def word2features(sent, i):\n",
    "    word = sent[i]\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "def sent2labels(sent):\n",
    "    return [label for token, label in sent]\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, label in sent]\n",
    "\n",
    "X_train = [sent2features(s) for s in X_train]\n",
    "X_test = [sent2features(s) for s in X_test]\n",
    "\n",
    "crf = CRF(algorithm='lbfgs',\n",
    "            c1=0.1,     \n",
    "            c2=0.1,\n",
    "            max_iterations=100,\n",
    "            all_possible_transitions=True)\n",
    "\n",
    "crf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = crf.predict(X_test)\n",
    "print(metrics.flat_classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538e09a0",
   "metadata": {},
   "source": [
    "# Bunch of code for testing errors\n",
    "## trash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e5bb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matcher(string, pattern):\n",
    "    '''\n",
    "    Return the start and end index of any pattern present in the text.\n",
    "    '''\n",
    "    match_list = []\n",
    "    pattern = pattern.strip()\n",
    "    seqMatch = SequenceMatcher(None, string, pattern, autojunk=False)\n",
    "    match = seqMatch.find_longest_match(0, len(string), 0, len(pattern))\n",
    "    if (match.size == len(pattern)):\n",
    "        start = match.a\n",
    "        end = match.a + match.size\n",
    "        match_tup = (start, end)\n",
    "        string = string.replace(pattern, \"X\" * len(pattern), 1)\n",
    "        match_list.append(match_tup)\n",
    "        \n",
    "    return match_list, string\n",
    "text = train['text'][0]\n",
    "annotation = train['annotation'][0]\n",
    "#pattern = pattern.strip()\n",
    "match_list = []\n",
    "for i in annotation:\n",
    "    a, text_ = matcher(text, i[0])\n",
    "    match_list.append((a[0][0], a[0][1], i[1]))\n",
    "\n",
    "\n",
    "match_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2201cccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labs(s,match_list):\n",
    "    labs = ['O' for i in range(len(s.split()))]\n",
    "    word_dict = pd.DataFrame({'word':s.split(),'label':labs})\n",
    "\n",
    "    for start, end, e_type in match_list:\n",
    "        index = len(re.findall(r' +',s[0:start]))-1\n",
    "        num_words = len(s[start:end].split())\n",
    "        index,num_words\n",
    "\n",
    "        if num_words > 1:\n",
    "            word_dict.loc[index,'label'] = 'B-' + e_type\n",
    "            for i in range(1,num_words):\n",
    "                word_dict.loc[index+i,'label'] = 'I-' + e_type\n",
    "        else:\n",
    "            word_dict.loc[index,'label'] = 'B-' + e_type\n",
    "    return word_dict\n",
    "\n",
    "x = create_labs(train['text'][0],match_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561cf224",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f855646",
   "metadata": {},
   "source": [
    "## test how to find the word using index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3754ca0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "a = s.split()\n",
    "a.index('smoking')\n",
    "len(re.findall(r' +',s[0:start]))\n",
    "a[86],len(s[start:end].split())\n",
    "#temp_str = pd.DataFrame(s.split(), columns=['word'])\n",
    "#temp_str['len'] = temp_str['word'].apply(lambda x: len(x))\n",
    "#temp_str['cumsum'] = temp_str['len'].cumsum()\n",
    "#temp_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2de0ab",
   "metadata": {},
   "source": [
    "## find which label went crazy and can't match text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7771983",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\JHU\\课程\\datadesign\\NLP\\machine_learning\\get_data_.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/JHU/%E8%AF%BE%E7%A8%8B/datadesign/NLP/machine_learning/get_data_.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m#text = \"Horses are too tall and they pretend to care about your feelings\"\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/JHU/%E8%AF%BE%E7%A8%8B/datadesign/NLP/machine_learning/get_data_.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m annotation \u001b[39m=\u001b[39m train[\u001b[39m'\u001b[39m\u001b[39mannotation\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m176\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/JHU/%E8%AF%BE%E7%A8%8B/datadesign/NLP/machine_learning/get_data_.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m annotation:\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/JHU/%E8%AF%BE%E7%A8%8B/datadesign/NLP/machine_learning/get_data_.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mprint\u001b[39m(i)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/JHU/%E8%AF%BE%E7%A8%8B/datadesign/NLP/machine_learning/get_data_.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     a, text_ \u001b[39m=\u001b[39m matcher(text, i[\u001b[39m0\u001b[39m])\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "match_list = []\n",
    "text = train['text'][176]\n",
    "#text = \"Horses are too tall and they pretend to care about your feelings\"\n",
    "annotation = train['annotation'][176]\n",
    "for i in annotation:\n",
    "    print(i)\n",
    "    a, text_ = matcher(text, i[0])\n",
    "    print(a)\n",
    "    match_list.append((a[0][0], a[0][1], i[1]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafaa13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'][214]\n",
    "#tree = ET.parse(file_path + names[62])\n",
    "#root = tree.getroot()\n",
    "#root[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f3dd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "names[214]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1f304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ET.parse(file_path + names[0])\n",
    "root = tree.getroot()\n",
    "## Get the labels\n",
    "# skip FAMILY_HIST: doesn't have a text\n",
    "#skip = [root[1][x].tag for x in range(len(root[1]))].index('FAMILY_HIST')\n",
    "PHI = [root[1][x].tag for x in range(len(root[1]))].index('PHI')\n",
    "# get all labels except for PHI\n",
    "tag_list = []\n",
    "root[1][5][0].attrib['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5abe06",
   "metadata": {},
   "source": [
    "# create a list of tag as tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b84b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.datasets import ColumnCorpus\n",
    "# define columns\n",
    "columns = {0 : 'text', 1 : 'ner'}\n",
    "# directory where the data resides\n",
    "data_folder = 'C:/Users/Leste/Desktop/BDD data'\n",
    "# initializing the corpus\n",
    "corpus: Corpus = ColumnCorpus(data_folder, columns,\n",
    "                              train_file = 'train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23eb1177",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.train[0].to_tagged_string('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5aa772",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"C:/Users/Leste/Desktop/BDD data/extracted/training-RiskFactors-Complete-Set1/\"\n",
    "names = [f for f in listdir(file_path) if f.endswith('.xml')]\n",
    "tree = ET.parse(file_path + names[0])\n",
    "root = tree.getroot()\n",
    "\n",
    "## skip FAMILY_HIST: doesn't have a text\n",
    "skip = [root[1][x].tag for x in range(len(root[1]))].index('FAMILY_HIST')\n",
    "PHI = [root[1][x].tag for x in range(len(root[1]))].index('PHI')\n",
    "\n",
    "## get all labels except for PHI\n",
    "tag_list = []\n",
    "for n in range(len(root[1])):\n",
    "    if n == skip:\n",
    "        continue\n",
    "    for m in range(len(root[1][n])):\n",
    "        tag_list.append((root[1][n][m].attrib['text'],root[1][n][m].tag))\n",
    "\n",
    "## get PHI labels\n",
    "for n in range(PHI,len(root[1])):\n",
    "    tag_list.append((root[1][n].attrib['text'],root[1][n].tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d8dbe9",
   "metadata": {},
   "source": [
    "## clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b14f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "file_path = \"C:/Users/Leste/Desktop/BDD data/extracted/training-RiskFactors-Complete-Set1/\"\n",
    "names = [f for f in listdir(file_path) if f.endswith('.xml')]\n",
    "tree = ET.parse(file_path + names[0])\n",
    "root = tree.getroot()\n",
    "nt = re.sub('\\n','',root[0].text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72322f28",
   "metadata": {},
   "source": [
    "## Use trained model from flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac7ffed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.models import SequenceTagger\n",
    "from flair.tokenization import SegtokSentenceSplitter\n",
    "splitter = SegtokSentenceSplitter()\n",
    "tagger = SequenceTagger.load('ner')\n",
    "test = text_list[names[0]]\n",
    "sentences = splitter.split(test)\n",
    "sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8afa84",
   "metadata": {},
   "source": [
    "## Try training NER with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533dcfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import TransformerWordEmbeddings\n",
    "from flair.models import SequenceTagger\n",
    "from flair.trainers import ModelTrainer\n",
    "from flair.datasets import UD_ENGLISH\n",
    "# 1. get the corpus\n",
    "corpus = UD_ENGLISH().downsample(0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94621d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "print(pd.DataFrame(corpus.test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cd688e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. what label do we want to predict?\n",
    "label_type = 'upos'\n",
    "\n",
    "\n",
    "# 3. make the label dictionary from the corpus\n",
    "label_dict = corpus.make_label_dictionary(label_type=label_type)\n",
    "print(label_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eae9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. initialize fine-tuneable transformer embeddings WITH document context\n",
    "embeddings = TransformerWordEmbeddings(model='xlm-roberta-large',\n",
    "                                       layers=\"-1\",\n",
    "                                       subtoken_pooling=\"first\",\n",
    "                                       fine_tune=True,\n",
    "                                       use_context=True,\n",
    "                                       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a929f9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. initialize bare-bones sequence tagger (no CRF, no RNN, no reprojection)\n",
    "tagger = SequenceTagger(hidden_size=256,\n",
    "                        embeddings=embeddings,\n",
    "                        tag_dictionary=label_dict,\n",
    "                        tag_type=label_type,\n",
    "                        use_crf=False,\n",
    "                        use_rnn=False,\n",
    "                        reproject_embeddings=False,\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a755c1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6. initialize trainer\n",
    "trainer = ModelTrainer(tagger, corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5553834b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 7. run fine-tuning\n",
    "trainer.fine_tune('resources/taggers/example-roberta',\n",
    "                  learning_rate=0.1,\n",
    "                  mini_batch_size=32,\n",
    "                  mini_batch_chunk_size=1,  # remove this parameter to speed up computation if you have a big GPU\n",
    "                  max_epochs = 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5 (tags/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "beedbe2faf2f7048d727558d0bc3221e7eba2a0b921cac4d4771b2feb8f74b30"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
