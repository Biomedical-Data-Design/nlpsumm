{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f65c0c47",
   "metadata": {},
   "source": [
    "# Split text and tag\n",
    "\n",
    "Smoker and Family_hist can have no tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be1a3360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390 66 65\n",
      "201 34 34\n"
     ]
    }
   ],
   "source": [
    "#according to flair corpus \"UD_ENGLISH\", train:test:dev = 6:1:1\n",
    "from os import listdir\n",
    "from sklearn.model_selection import train_test_split\n",
    "file_path_1 = \"C:/Users/Leste/Desktop/BDD data/extracted/training-RiskFactors-Complete-Set1/\"\n",
    "file_path_2 = \"C:/Users/Leste/Desktop/BDD data/extracted/training-RiskFactors-Complete-Set2/\"\n",
    "names_1 = [f for f in listdir(file_path_1) if f.endswith('.xml')]\n",
    "names_2 = [f for f in listdir(file_path_2) if f.endswith('.xml')]\n",
    "train_n_1, test_n_1 = train_test_split(names_1, test_size=0.125, random_state=42)\n",
    "train_n_1, dev_n_1 = train_test_split(train_n_1, test_size=1/7, random_state=42)\n",
    "print(len(train_n_1), len(test_n_1), len(dev_n_1))\n",
    "\n",
    "train_n_2, test_n_2 = train_test_split(names_2, test_size=0.125, random_state=42)\n",
    "train_n_2, dev_n_2 = train_test_split(train_n_2, test_size=1/7, random_state=42)\n",
    "print(len(train_n_2), len(test_n_2), len(dev_n_2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18994392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "train_df_1 = pd.DataFrame(np.zeros((len(train_n_1), 2), dtype=object), columns=['text', 'annotation'])\n",
    "dev_df_1 = pd.DataFrame(np.zeros((len(dev_n_1), 2), dtype=object), columns=['text', 'annotation'])\n",
    "test_df_1 = pd.DataFrame(np.zeros((len(test_n_1), 2), dtype=object), columns=['text', 'annotation'])\n",
    "train_df_2 = pd.DataFrame(np.zeros((len(train_n_2), 2), dtype=object), columns=['text', 'annotation'])\n",
    "dev_df_2 = pd.DataFrame(np.zeros((len(dev_n_2), 2), dtype=object), columns=['text', 'annotation'])\n",
    "test_df_2 = pd.DataFrame(np.zeros((len(test_n_2), 2), dtype=object), columns=['text', 'annotation'])\n",
    "\n",
    "def to_df(names,df, file_path,PHI_status = True):\n",
    "    #get text\n",
    "    n = 0\n",
    "    for name in names:\n",
    "        tree = ET.parse(file_path + name)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        ## Get the text\n",
    "        nt = re.sub('\\n',' ',root[0].text)\n",
    "        nt = re.sub('\\t',' ',nt) \n",
    "        nt = re.sub('\"',\"'\",nt)\n",
    "        ## sample 214 has a weird character\n",
    "        nt = re.sub('>','&gt;',nt) \n",
    "        nt = re.sub('<','&lt;',nt)\n",
    "        df['text'][n] = nt\n",
    "        n+=1\n",
    "    \n",
    "    #get annotations\n",
    "    n = 0\n",
    "    for name in names:\n",
    "        tree = ET.parse(file_path + name)\n",
    "        root = tree.getroot()\n",
    "        ## Get the labels\n",
    "\n",
    "        tag_list = []\n",
    "        # get PHI labels if there are any\n",
    "        if PHI_status == True:\n",
    "            PHI = [root[1][x].tag for x in range(len(root[1]))].index('PHI')\n",
    "            for k in range(PHI,len(root[1])):\n",
    "                tag_list.append((root[1][k].attrib['text'],root[1][k].tag))\n",
    "        \n",
    "        # get the rest of labels\n",
    "        for k in range(len(root[1])):\n",
    "            for m in range(len(root[1][k])):\n",
    "                if root[1][k][m].attrib.keys().__contains__('text') == False:\n",
    "                    continue\n",
    "                tag_list.append((root[1][k][m].attrib['text'],root[1][k][m].tag))\n",
    "        \n",
    "        df['annotation'][n] = tag_list\n",
    "        n+=1\n",
    "    return df\n",
    "\n",
    "train_1 = to_df(train_n_1,train_df_1, file_path_1)\n",
    "dev_1 = to_df(dev_n_1,dev_df_1, file_path_1)\n",
    "test_1 = to_df(test_n_1,test_df_1, file_path_1)\n",
    "train_2 = to_df(train_n_2,train_df_2, file_path_2, PHI_status = False)\n",
    "dev_2 = to_df(dev_n_2,dev_df_2, file_path_2, PHI_status = False)\n",
    "test_2 = to_df(test_n_2,test_df_2, file_path_2, PHI_status = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b40ace78",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train_1, train_2], ignore_index=True)\n",
    "dev = pd.concat([dev_1, dev_2], ignore_index=True)\n",
    "test = pd.concat([test_1, test_2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a032ef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tree = ET.parse(file_path + train_n[365])\n",
    "root = tree.getroot()\n",
    "PHI = [root[1][x].tag for x in range(len(root[1]))].index('PHI')\n",
    "for k in range(PHI,len(root[1])):\n",
    "    print((root[1][k].attrib['text'],root[1][k].tag))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677e0d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['annotation'][365]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93ebd4f",
   "metadata": {},
   "source": [
    "# Functions to convert dataframe into input format\n",
    "520 files takes 3 min\\\n",
    "Please use create_labs function instead of mark_sentences\\\n",
    "source: https://medium.com/thecyphy/training-custom-ner-model-using-flair-df1f9ea9c762\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4e3a81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "[4]\n",
      "[5]\n",
      "[6]\n",
      "[7]\n",
      "[8]\n",
      "[9]\n",
      "[10]\n",
      "[11]\n",
      "[12]\n",
      "[13]\n",
      "[14]\n",
      "[15]\n",
      "[16]\n",
      "[17]\n",
      "[18]\n",
      "[19]\n",
      "[20]\n",
      "[21]\n",
      "[22]\n",
      "[23]\n",
      "[24]\n",
      "[25]\n",
      "[26]\n",
      "[27]\n",
      "[28]\n",
      "[29]\n",
      "[30]\n",
      "[31]\n",
      "[32]\n",
      "[33]\n",
      "[34]\n",
      "[35]\n",
      "[36]\n",
      "[37]\n",
      "[38]\n",
      "[39]\n",
      "[40]\n",
      "[41]\n",
      "[42]\n",
      "[43]\n",
      "[44]\n",
      "[45]\n",
      "[46]\n",
      "[47]\n",
      "[48]\n",
      "[49]\n",
      "[50]\n",
      "[51]\n",
      "[52]\n",
      "[53]\n",
      "[54]\n",
      "[55]\n",
      "[56]\n",
      "[57]\n",
      "[58]\n",
      "[59]\n",
      "[60]\n",
      "[61]\n",
      "[62]\n",
      "[63]\n",
      "[64]\n",
      "[65]\n",
      "[66]\n",
      "[67]\n",
      "[68]\n",
      "[69]\n",
      "[70]\n",
      "[71]\n",
      "[72]\n",
      "[73]\n",
      "[74]\n",
      "[75]\n",
      "[76]\n",
      "[77]\n",
      "[78]\n",
      "[79]\n",
      "[80]\n",
      "[81]\n",
      "[82]\n",
      "[83]\n",
      "[84]\n",
      "[85]\n",
      "[86]\n",
      "[87]\n",
      "[88]\n",
      "[89]\n",
      "[90]\n",
      "[91]\n",
      "[92]\n",
      "[93]\n",
      "[94]\n",
      "[95]\n",
      "[96]\n",
      "[97]\n",
      "[98]\n",
      "[99]\n",
      "[100]\n",
      "[101]\n",
      "[102]\n",
      "[103]\n",
      "[104]\n",
      "[105]\n",
      "[106]\n",
      "[107]\n",
      "[108]\n",
      "[109]\n",
      "[110]\n",
      "[111]\n",
      "[112]\n",
      "[113]\n",
      "[114]\n",
      "[115]\n",
      "[116]\n",
      "[117]\n",
      "[118]\n",
      "[119]\n",
      "[120]\n",
      "[121]\n",
      "[122]\n",
      "[123]\n",
      "[124]\n",
      "[125]\n",
      "[126]\n",
      "[127]\n",
      "[128]\n",
      "[129]\n",
      "[130]\n",
      "[131]\n",
      "[132]\n",
      "[133]\n",
      "[134]\n",
      "[135]\n",
      "[136]\n",
      "[137]\n",
      "[138]\n",
      "[139]\n",
      "[140]\n",
      "[141]\n",
      "[142]\n",
      "[143]\n",
      "[144]\n",
      "[145]\n",
      "[146]\n",
      "[147]\n",
      "[148]\n",
      "[149]\n",
      "[150]\n",
      "[151]\n",
      "[152]\n",
      "[153]\n",
      "[154]\n",
      "[155]\n",
      "[156]\n",
      "[157]\n",
      "[158]\n",
      "[159]\n",
      "[160]\n",
      "[161]\n",
      "[162]\n",
      "[163]\n",
      "[164]\n",
      "[165]\n",
      "[166]\n",
      "[167]\n",
      "[168]\n",
      "[169]\n",
      "[170]\n",
      "[171]\n",
      "[172]\n",
      "[173]\n",
      "[174]\n",
      "[175]\n",
      "[176]\n",
      "[177]\n",
      "[178]\n",
      "[179]\n",
      "[180]\n",
      "[181]\n",
      "[182]\n",
      "[183]\n",
      "[184]\n",
      "[185]\n",
      "[186]\n",
      "[187]\n",
      "[188]\n",
      "[189]\n",
      "[190]\n",
      "[191]\n",
      "[192]\n",
      "[193]\n",
      "[194]\n",
      "[195]\n",
      "[196]\n",
      "[197]\n",
      "[198]\n",
      "[199]\n",
      "[200]\n",
      "[201]\n",
      "[202]\n",
      "[203]\n",
      "[204]\n",
      "[205]\n",
      "[206]\n",
      "[207]\n",
      "[208]\n",
      "[209]\n",
      "[210]\n",
      "[211]\n",
      "[212]\n",
      "[213]\n",
      "[214]\n",
      "[215]\n",
      "[216]\n",
      "[217]\n",
      "[218]\n",
      "[219]\n",
      "[220]\n",
      "[221]\n",
      "[222]\n",
      "[223]\n",
      "[224]\n",
      "[225]\n",
      "[226]\n",
      "[227]\n",
      "[228]\n",
      "[229]\n",
      "[230]\n",
      "[231]\n",
      "[232]\n",
      "[233]\n",
      "[234]\n",
      "[235]\n",
      "[236]\n",
      "[237]\n",
      "[238]\n",
      "[239]\n",
      "[240]\n",
      "[241]\n",
      "[242]\n",
      "[243]\n",
      "[244]\n",
      "[245]\n",
      "[246]\n",
      "[247]\n",
      "[248]\n",
      "[249]\n",
      "[250]\n",
      "[251]\n",
      "[252]\n",
      "[253]\n",
      "[254]\n",
      "[255]\n",
      "[256]\n",
      "[257]\n",
      "[258]\n",
      "[259]\n",
      "[260]\n",
      "[261]\n",
      "[262]\n",
      "[263]\n",
      "[264]\n",
      "[265]\n",
      "[266]\n",
      "[267]\n",
      "[268]\n",
      "[269]\n",
      "[270]\n",
      "[271]\n",
      "[272]\n",
      "[273]\n",
      "[274]\n",
      "[275]\n",
      "[276]\n",
      "[277]\n",
      "[278]\n",
      "[279]\n",
      "[280]\n",
      "[281]\n",
      "[282]\n",
      "[283]\n",
      "[284]\n",
      "[285]\n",
      "[286]\n",
      "[287]\n",
      "[288]\n",
      "[289]\n",
      "[290]\n",
      "[291]\n",
      "[292]\n",
      "[293]\n",
      "[294]\n",
      "[295]\n",
      "[296]\n",
      "[297]\n",
      "[298]\n",
      "[299]\n",
      "[300]\n",
      "[301]\n",
      "[302]\n",
      "[303]\n",
      "[304]\n",
      "[305]\n",
      "[306]\n",
      "[307]\n",
      "[308]\n",
      "[309]\n",
      "[310]\n",
      "[311]\n",
      "[312]\n",
      "[313]\n",
      "[314]\n",
      "[315]\n",
      "[316]\n",
      "[317]\n",
      "[318]\n",
      "[319]\n",
      "[320]\n",
      "[321]\n",
      "[322]\n",
      "[323]\n",
      "[324]\n",
      "[325]\n",
      "[326]\n",
      "[327]\n",
      "[328]\n",
      "[329]\n",
      "[330]\n",
      "[331]\n",
      "[332]\n",
      "[333]\n",
      "[334]\n",
      "[335]\n",
      "[336]\n",
      "[337]\n",
      "[338]\n",
      "[339]\n",
      "[340]\n",
      "[341]\n",
      "[342]\n",
      "[343]\n",
      "[344]\n",
      "[345]\n",
      "[346]\n",
      "[347]\n",
      "[348]\n",
      "[349]\n",
      "[350]\n",
      "[351]\n",
      "[352]\n",
      "[353]\n",
      "[354]\n",
      "[355]\n",
      "[356]\n",
      "[357]\n",
      "[358]\n",
      "[359]\n",
      "[360]\n",
      "[361]\n",
      "[362]\n",
      "[363]\n",
      "[364]\n",
      "[365]\n",
      "[366]\n",
      "[367]\n",
      "[368]\n",
      "[369]\n",
      "[370]\n",
      "[371]\n",
      "[372]\n",
      "[373]\n",
      "[374]\n",
      "[375]\n",
      "[376]\n",
      "[377]\n",
      "[378]\n",
      "[379]\n",
      "[380]\n",
      "[381]\n",
      "[382]\n",
      "[383]\n",
      "[384]\n",
      "[385]\n",
      "[386]\n",
      "[387]\n",
      "[388]\n",
      "[389]\n",
      "[390]\n",
      "[391]\n",
      "[392]\n",
      "[393]\n",
      "[394]\n",
      "[395]\n",
      "[396]\n",
      "[397]\n",
      "[398]\n",
      "[399]\n",
      "[400]\n",
      "[401]\n",
      "[402]\n",
      "[403]\n",
      "[404]\n",
      "[405]\n",
      "[406]\n",
      "[407]\n",
      "[408]\n",
      "[409]\n",
      "[410]\n",
      "[411]\n",
      "[412]\n",
      "[413]\n",
      "[414]\n",
      "[415]\n",
      "[416]\n",
      "[417]\n",
      "[418]\n",
      "[419]\n",
      "[420]\n",
      "[421]\n",
      "[422]\n",
      "[423]\n",
      "[424]\n",
      "[425]\n",
      "[426]\n",
      "[427]\n",
      "[428]\n",
      "[429]\n",
      "[430]\n",
      "[431]\n",
      "[432]\n",
      "[433]\n",
      "[434]\n",
      "[435]\n",
      "[436]\n",
      "[437]\n",
      "[438]\n",
      "[439]\n",
      "[440]\n",
      "[441]\n",
      "[442]\n",
      "[443]\n",
      "[444]\n",
      "[445]\n",
      "[446]\n",
      "[447]\n",
      "[448]\n",
      "[449]\n",
      "[450]\n",
      "[451]\n",
      "[452]\n",
      "[453]\n",
      "[454]\n",
      "[455]\n",
      "[456]\n",
      "[457]\n",
      "[458]\n",
      "[459]\n",
      "[460]\n",
      "[461]\n",
      "[462]\n",
      "[463]\n",
      "[464]\n",
      "[465]\n",
      "[466]\n",
      "[467]\n",
      "[468]\n",
      "[469]\n",
      "[470]\n",
      "[471]\n",
      "[472]\n",
      "[473]\n",
      "[474]\n",
      "[475]\n",
      "[476]\n",
      "[477]\n",
      "[478]\n",
      "[479]\n",
      "[480]\n",
      "[481]\n",
      "[482]\n",
      "[483]\n",
      "[484]\n",
      "[485]\n",
      "[486]\n",
      "[487]\n",
      "[488]\n",
      "[489]\n",
      "[490]\n",
      "[491]\n",
      "[492]\n",
      "[493]\n",
      "[494]\n",
      "[495]\n",
      "[496]\n",
      "[497]\n",
      "[498]\n",
      "[499]\n",
      "[500]\n",
      "[501]\n",
      "[502]\n",
      "[503]\n",
      "[504]\n",
      "[505]\n",
      "[506]\n",
      "[507]\n",
      "[508]\n",
      "[509]\n",
      "[510]\n",
      "[511]\n",
      "[512]\n",
      "[513]\n",
      "[514]\n",
      "[515]\n",
      "[516]\n",
      "[517]\n",
      "[518]\n",
      "[519]\n",
      "[520]\n",
      "[521]\n",
      "[522]\n",
      "[523]\n",
      "[524]\n",
      "[525]\n",
      "[526]\n",
      "[527]\n",
      "[528]\n",
      "[529]\n",
      "[530]\n",
      "[531]\n",
      "[532]\n",
      "[533]\n",
      "[534]\n",
      "[535]\n",
      "[536]\n",
      "[537]\n",
      "[538]\n",
      "[539]\n",
      "[540]\n",
      "[541]\n",
      "[542]\n",
      "[543]\n",
      "[544]\n",
      "[545]\n",
      "[546]\n",
      "[547]\n",
      "[548]\n",
      "[549]\n",
      "[550]\n",
      "[551]\n",
      "[552]\n",
      "[553]\n",
      "[554]\n",
      "[555]\n",
      "[556]\n",
      "[557]\n",
      "[558]\n",
      "[559]\n",
      "[560]\n",
      "[561]\n",
      "[562]\n",
      "[563]\n",
      "[564]\n",
      "[565]\n",
      "[566]\n",
      "[567]\n",
      "[568]\n",
      "[569]\n",
      "[570]\n",
      "[571]\n",
      "[572]\n",
      "[573]\n",
      "[574]\n",
      "[575]\n",
      "[576]\n",
      "[577]\n",
      "[578]\n",
      "[579]\n",
      "[580]\n",
      "[581]\n",
      "[582]\n",
      "[583]\n",
      "[584]\n",
      "[585]\n",
      "[586]\n",
      "[587]\n",
      "[588]\n",
      "[589]\n",
      "[590]\n",
      "[0]\n",
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "[4]\n",
      "[5]\n",
      "[6]\n",
      "[7]\n",
      "[8]\n",
      "[9]\n",
      "[10]\n",
      "[11]\n",
      "[12]\n",
      "[13]\n",
      "[14]\n",
      "[15]\n",
      "[16]\n",
      "[17]\n",
      "[18]\n",
      "[19]\n",
      "[20]\n",
      "[21]\n",
      "[22]\n",
      "[23]\n",
      "[24]\n",
      "[25]\n",
      "[26]\n",
      "[27]\n",
      "[28]\n",
      "[29]\n",
      "[30]\n",
      "[31]\n",
      "[32]\n",
      "[33]\n",
      "[34]\n",
      "[35]\n",
      "[36]\n",
      "[37]\n",
      "[38]\n",
      "[39]\n",
      "[40]\n",
      "[41]\n",
      "[42]\n",
      "[43]\n",
      "[44]\n",
      "[45]\n",
      "[46]\n",
      "[47]\n",
      "[48]\n",
      "[49]\n",
      "[50]\n",
      "[51]\n",
      "[52]\n",
      "[53]\n",
      "[54]\n",
      "[55]\n",
      "[56]\n",
      "[57]\n",
      "[58]\n",
      "[59]\n",
      "[60]\n",
      "[61]\n",
      "[62]\n",
      "[63]\n",
      "[64]\n",
      "[65]\n",
      "[66]\n",
      "[67]\n",
      "[68]\n",
      "[69]\n",
      "[70]\n",
      "[71]\n",
      "[72]\n",
      "[73]\n",
      "[74]\n",
      "[75]\n",
      "[76]\n",
      "[77]\n",
      "[78]\n",
      "[79]\n",
      "[80]\n",
      "[81]\n",
      "[82]\n",
      "[83]\n",
      "[84]\n",
      "[85]\n",
      "[86]\n",
      "[87]\n",
      "[88]\n",
      "[89]\n",
      "[90]\n",
      "[91]\n",
      "[92]\n",
      "[93]\n",
      "[94]\n",
      "[95]\n",
      "[96]\n",
      "[97]\n",
      "[98]\n",
      "[0]\n",
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "[4]\n",
      "[5]\n",
      "[6]\n",
      "[7]\n",
      "[8]\n",
      "[9]\n",
      "[10]\n",
      "[11]\n",
      "[12]\n",
      "[13]\n",
      "[14]\n",
      "[15]\n",
      "[16]\n",
      "[17]\n",
      "[18]\n",
      "[19]\n",
      "[20]\n",
      "[21]\n",
      "[22]\n",
      "[23]\n",
      "[24]\n",
      "[25]\n",
      "[26]\n",
      "[27]\n",
      "[28]\n",
      "[29]\n",
      "[30]\n",
      "[31]\n",
      "[32]\n",
      "[33]\n",
      "[34]\n",
      "[35]\n",
      "[36]\n",
      "[37]\n",
      "[38]\n",
      "[39]\n",
      "[40]\n",
      "[41]\n",
      "[42]\n",
      "[43]\n",
      "[44]\n",
      "[45]\n",
      "[46]\n",
      "[47]\n",
      "[48]\n",
      "[49]\n",
      "[50]\n",
      "[51]\n",
      "[52]\n",
      "[53]\n",
      "[54]\n",
      "[55]\n",
      "[56]\n",
      "[57]\n",
      "[58]\n",
      "[59]\n",
      "[60]\n",
      "[61]\n",
      "[62]\n",
      "[63]\n",
      "[64]\n",
      "[65]\n",
      "[66]\n",
      "[67]\n",
      "[68]\n",
      "[69]\n",
      "[70]\n",
      "[71]\n",
      "[72]\n",
      "[73]\n",
      "[74]\n",
      "[75]\n",
      "[76]\n",
      "[77]\n",
      "[78]\n",
      "[79]\n",
      "[80]\n",
      "[81]\n",
      "[82]\n",
      "[83]\n",
      "[84]\n",
      "[85]\n",
      "[86]\n",
      "[87]\n",
      "[88]\n",
      "[89]\n",
      "[90]\n",
      "[91]\n",
      "[92]\n",
      "[93]\n",
      "[94]\n",
      "[95]\n",
      "[96]\n",
      "[97]\n",
      "[98]\n",
      "[99]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from difflib import SequenceMatcher\n",
    "import pickle\n",
    "\n",
    "def matcher(string, pattern):\n",
    "    '''\n",
    "    Return the start and end index of any pattern present in the text.\n",
    "    '''\n",
    "    match_list = []\n",
    "    pattern = pattern.strip()\n",
    "    seqMatch = SequenceMatcher(None, string, pattern, autojunk=False)\n",
    "    match = seqMatch.find_longest_match(0, len(string), 0, len(pattern))\n",
    "    if (match.size == len(pattern)):\n",
    "        start = match.a\n",
    "        end = match.a + match.size\n",
    "        match_tup = (start, end)\n",
    "        string = string.replace(pattern, \"X\" * len(pattern), 1)\n",
    "        match_list.append(match_tup)\n",
    "        \n",
    "    return match_list, string\n",
    "\n",
    "\n",
    "def mark_sentence(s, match_list):\n",
    "    '''\n",
    "    Marks all the entities in the sentence as per the BIO scheme. \n",
    "    '''\n",
    "    word_dict = {}\n",
    "    for word in s.split():\n",
    "        word_dict[word] = 'O'\n",
    "        \n",
    "    for start, end, e_type in match_list:\n",
    "        temp_str = s[start:end]\n",
    "        tmp_list = temp_str.split()\n",
    "        if len(tmp_list) > 1:\n",
    "            word_dict[tmp_list[0]] = 'B-' + e_type\n",
    "            for w in tmp_list[1:]:\n",
    "                word_dict[w] = 'I-' + e_type\n",
    "        else:\n",
    "            word_dict[temp_str] = 'B-' + e_type\n",
    "    return word_dict\n",
    "\n",
    "## replace !!mark_sentence!! to better label the text with more than one word\n",
    "def create_labs(s,match_list):\n",
    "    labs = ['O' for i in range(len(s.split()))]\n",
    "    word_dict = pd.DataFrame({'word':s.split(),'label':labs})\n",
    "\n",
    "    for start, end, e_type in match_list:\n",
    "        index = len(re.findall(r' +',s[0:start]))-1\n",
    "        num_words = len(s[start:end].split())\n",
    "\n",
    "        if num_words > 1:\n",
    "            word_dict.loc[index,'label'] = 'B-' + e_type\n",
    "            for i in range(1,num_words):\n",
    "                word_dict.loc[index+i,'label'] = 'I-' + e_type\n",
    "        else:\n",
    "            word_dict.loc[index,'label'] = 'B-' + e_type\n",
    "    return word_dict\n",
    "\n",
    "def clean(text):\n",
    "    '''\n",
    "    Just a helper fuction to add a space before the punctuations for better tokenization\n",
    "    '''\n",
    "    filters = [\"!\", \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"/\", \"*\", \".\", \":\", \";\", \"<\", \"=\", \">\", \"?\", \"@\", \"[\",\n",
    "               \"\\\\\", \"]\", \"_\", \"`\", \"{\", \"}\", \"~\", \"'\"]\n",
    "    for i in text:\n",
    "        if i in filters:\n",
    "            text = text.replace(i, \" \" + i)\n",
    "            \n",
    "    return text\n",
    "\n",
    "def create_data(df, filepath):\n",
    "    '''\n",
    "    The function responsible for the creation of data in the said format.\n",
    "    '''\n",
    "    with open(filepath , 'w') as f:\n",
    "        for text, annotation in zip(df.text, df.annotation):\n",
    "            #text = clean(text)\n",
    "            #text_ = text    \n",
    "            print(df.index[df['text']== text].tolist())    \n",
    "            match_list = []\n",
    "            for i in annotation:\n",
    "                a, text_ = matcher(text, i[0])\n",
    "                match_list.append((a[0][0], a[0][1], i[1]))\n",
    "\n",
    "            d = create_labs(text, match_list)\n",
    "\n",
    "            #for i in d.keys():\n",
    "            #    f.writelines(i + ' ' + d[i] +'\\n')\n",
    "            #f.writelines('\\n')\n",
    "            for i in range(d.shape[0]):\n",
    "                f.writelines(d['word'][i] + ' ' + d['label'][i] +'\\n')\n",
    "            f.writelines('\\n')\n",
    "def to_csv(df, filepath):\n",
    "    for text, annotation in zip(df.text, df.annotation):\n",
    "            #text = clean(text)\n",
    "            #text_ = text    \n",
    "            print(df.index[df['text']== text].tolist())    \n",
    "            match_list = []\n",
    "            for i in annotation:\n",
    "                a, text_ = matcher(text, i[0])\n",
    "                match_list.append((a[0][0], a[0][1], i[1]))\n",
    "\n",
    "            d = create_labs(text, match_list)\n",
    "            d.to_csv(filepath, index = False, header = False, mode = 'a')\n",
    "            \n",
    "def main(input,save_path):\n",
    "    ## An example dataframe.\n",
    "    #data = pd.DataFrame([[\"Horses are too tall and they pretend to care about your feelings\", [(\"Horses\", \"ANIMAL\")]],\n",
    "    #              [\"Who is Shaka Khan?\", [(\"Shaka Khan\", \"PERSON\")]],\n",
    "    #              [\"I like London and Berlin.\", [(\"London\", \"LOCATION\"), (\"Berlin\", \"LOCATION\")]],\n",
    "    #              [\"There is a banyan tree in the courtyard\", [(\"banyan tree\", \"TREE\")]]], columns=['text', 'annotation'])\n",
    "    data = input\n",
    "    ## path to save the txt file.\n",
    "    ## creating the file.\n",
    "    to_csv(data, save_path)\n",
    "    \n",
    "#if __name__ == '__main__':\n",
    "path = 'C:/Users/Leste/Documents/GitHub/nlpsumm/BERT/data_processed/'\n",
    "main(train,path+'train.csv')\n",
    "main(dev,path+'dev.csv')\n",
    "main(test,path+'test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538e09a0",
   "metadata": {},
   "source": [
    "# Bunch of code for testing errors\n",
    "## trash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e5bb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matcher(string, pattern):\n",
    "    '''\n",
    "    Return the start and end index of any pattern present in the text.\n",
    "    '''\n",
    "    match_list = []\n",
    "    pattern = pattern.strip()\n",
    "    seqMatch = SequenceMatcher(None, string, pattern, autojunk=False)\n",
    "    match = seqMatch.find_longest_match(0, len(string), 0, len(pattern))\n",
    "    if (match.size == len(pattern)):\n",
    "        start = match.a\n",
    "        end = match.a + match.size\n",
    "        match_tup = (start, end)\n",
    "        string = string.replace(pattern, \"X\" * len(pattern), 1)\n",
    "        match_list.append(match_tup)\n",
    "        \n",
    "    return match_list, string\n",
    "text = train['text'][0]\n",
    "annotation = train['annotation'][0]\n",
    "#pattern = pattern.strip()\n",
    "match_list = []\n",
    "for i in annotation:\n",
    "    a, text_ = matcher(text, i[0])\n",
    "    match_list.append((a[0][0], a[0][1], i[1]))\n",
    "\n",
    "\n",
    "match_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2201cccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labs(s,match_list):\n",
    "    labs = ['O' for i in range(len(s.split()))]\n",
    "    word_dict = pd.DataFrame({'word':s.split(),'label':labs})\n",
    "\n",
    "    for start, end, e_type in match_list:\n",
    "        index = len(re.findall(r' +',s[0:start]))-1\n",
    "        num_words = len(s[start:end].split())\n",
    "        index,num_words\n",
    "\n",
    "        if num_words > 1:\n",
    "            word_dict.loc[index,'label'] = 'B-' + e_type\n",
    "            for i in range(1,num_words):\n",
    "                word_dict.loc[index+i,'label'] = 'I-' + e_type\n",
    "        else:\n",
    "            word_dict.loc[index,'label'] = 'B-' + e_type\n",
    "    return word_dict\n",
    "\n",
    "x = create_labs(train['text'][0],match_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f855646",
   "metadata": {},
   "source": [
    "## test how to find the word using index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3754ca0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "a = s.split()\n",
    "a.index('smoking')\n",
    "len(re.findall(r' +',s[0:start]))\n",
    "a[86],len(s[start:end].split())\n",
    "#temp_str = pd.DataFrame(s.split(), columns=['word'])\n",
    "#temp_str['len'] = temp_str['word'].apply(lambda x: len(x))\n",
    "#temp_str['cumsum'] = temp_str['len'].cumsum()\n",
    "#temp_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2de0ab",
   "metadata": {},
   "source": [
    "## find which label went crazy and can't match text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7771983",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_list = []\n",
    "text = train['text'][176]\n",
    "#text = \"Horses are too tall and they pretend to care about your feelings\"\n",
    "annotation = train['annotation'][176]\n",
    "for i in annotation:\n",
    "    print(i)\n",
    "    a, text_ = matcher(text, i[0])\n",
    "    print(a)\n",
    "    match_list.append((a[0][0], a[0][1], i[1]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafaa13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'][214]\n",
    "#tree = ET.parse(file_path + names[62])\n",
    "#root = tree.getroot()\n",
    "#root[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f3dd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "names[214]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1f304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ET.parse(file_path + names[0])\n",
    "root = tree.getroot()\n",
    "## Get the labels\n",
    "# skip FAMILY_HIST: doesn't have a text\n",
    "#skip = [root[1][x].tag for x in range(len(root[1]))].index('FAMILY_HIST')\n",
    "PHI = [root[1][x].tag for x in range(len(root[1]))].index('PHI')\n",
    "# get all labels except for PHI\n",
    "tag_list = []\n",
    "root[1][5][0].attrib['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5abe06",
   "metadata": {},
   "source": [
    "# create a list of tag as tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b84b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.datasets import ColumnCorpus\n",
    "# define columns\n",
    "columns = {0 : 'text', 1 : 'ner'}\n",
    "# directory where the data resides\n",
    "data_folder = 'C:/Users/Leste/Desktop/BDD data'\n",
    "# initializing the corpus\n",
    "corpus: Corpus = ColumnCorpus(data_folder, columns,\n",
    "                              train_file = 'train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23eb1177",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.train[0].to_tagged_string('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5aa772",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"C:/Users/Leste/Desktop/BDD data/extracted/training-RiskFactors-Complete-Set1/\"\n",
    "names = [f for f in listdir(file_path) if f.endswith('.xml')]\n",
    "tree = ET.parse(file_path + names[0])\n",
    "root = tree.getroot()\n",
    "\n",
    "## skip FAMILY_HIST: doesn't have a text\n",
    "skip = [root[1][x].tag for x in range(len(root[1]))].index('FAMILY_HIST')\n",
    "PHI = [root[1][x].tag for x in range(len(root[1]))].index('PHI')\n",
    "\n",
    "## get all labels except for PHI\n",
    "tag_list = []\n",
    "for n in range(len(root[1])):\n",
    "    if n == skip:\n",
    "        continue\n",
    "    for m in range(len(root[1][n])):\n",
    "        tag_list.append((root[1][n][m].attrib['text'],root[1][n][m].tag))\n",
    "\n",
    "## get PHI labels\n",
    "for n in range(PHI,len(root[1])):\n",
    "    tag_list.append((root[1][n].attrib['text'],root[1][n].tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d8dbe9",
   "metadata": {},
   "source": [
    "## clean the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72322f28",
   "metadata": {},
   "source": [
    "## Use trained model from flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac7ffed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.models import SequenceTagger\n",
    "from flair.tokenization import SegtokSentenceSplitter\n",
    "splitter = SegtokSentenceSplitter()\n",
    "tagger = SequenceTagger.load('ner')\n",
    "test = text_list[names[0]]\n",
    "sentences = splitter.split(test)\n",
    "sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8afa84",
   "metadata": {},
   "source": [
    "## Try training NER with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a684b7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create my corpus\n",
    "from flair.data import Corpus\n",
    "from flair.datasets import ColumnCorpus\n",
    "\n",
    "# this is the folder in which train, test and dev files reside\n",
    "data_folder = 'C:/Users/Leste/Documents/GitHub/nlpsumm/BERT/data_processed'\n",
    "\n",
    "# column format indicating which columns hold the text and label(s)\n",
    "columns = {0: 'text', 1: 'ner'}\n",
    "\n",
    "# load corpus containing training, test and dev data and if CSV has a header, you can skip it\n",
    "corpus: Corpus = ColumnCorpus(data_folder,columns,\n",
    "                              train_file = 'train.txt',\n",
    "                              test_file = 'test.txt',\n",
    "                              dev_file = 'new_dev.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7715f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-13 00:07:50,027 Reading data from C:\\Users\\Leste\\Documents\\GitHub\\nlpsumm\\BERT\\data_processed\n",
      "2022-11-13 00:07:50,028 Train: C:\\Users\\Leste\\Documents\\GitHub\\nlpsumm\\BERT\\data_processed\\train.txt\n",
      "2022-11-13 00:07:50,029 Dev: C:\\Users\\Leste\\Documents\\GitHub\\nlpsumm\\BERT\\data_processed\\new_dev.txt\n",
      "2022-11-13 00:07:50,032 Test: C:\\Users\\Leste\\Documents\\GitHub\\nlpsumm\\BERT\\data_processed\\test.txt\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [15], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m column_name_map \u001b[39m=\u001b[39m {\u001b[39m1\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m2\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mlabel_topic\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[0;32m     10\u001b[0m \u001b[39m# load corpus containing training, test and dev data and if CSV has a header, you can skip it\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m corpus: Corpus \u001b[39m=\u001b[39m CSVClassificationCorpus(data_folder,\n\u001b[0;32m     12\u001b[0m                                          column_name_map,\n\u001b[0;32m     13\u001b[0m                                          delimiter \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m,\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     14\u001b[0m                                          label_type\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mheart\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Leste\\anaconda3\\envs\\dataweek2\\lib\\site-packages\\flair\\datasets\\document_classification.py:362\u001b[0m, in \u001b[0;36mCSVClassificationCorpus.__init__\u001b[1;34m(self, data_folder, column_name_map, label_type, train_file, test_file, dev_file, max_tokens_per_doc, max_chars_per_doc, tokenizer, in_memory, skip_header, encoding, no_class_label, **fmtparams)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[39m# find train, dev and test files if not specified\u001b[39;00m\n\u001b[0;32m    360\u001b[0m dev_file, test_file, train_file \u001b[39m=\u001b[39m find_train_dev_test_files(data_folder, dev_file, test_file, train_file)\n\u001b[1;32m--> 362\u001b[0m train: FlairDataset \u001b[39m=\u001b[39m CSVClassificationDataset(\n\u001b[0;32m    363\u001b[0m     train_file,\n\u001b[0;32m    364\u001b[0m     column_name_map,\n\u001b[0;32m    365\u001b[0m     label_type\u001b[39m=\u001b[39mlabel_type,\n\u001b[0;32m    366\u001b[0m     tokenizer\u001b[39m=\u001b[39mtokenizer,\n\u001b[0;32m    367\u001b[0m     max_tokens_per_doc\u001b[39m=\u001b[39mmax_tokens_per_doc,\n\u001b[0;32m    368\u001b[0m     max_chars_per_doc\u001b[39m=\u001b[39mmax_chars_per_doc,\n\u001b[0;32m    369\u001b[0m     in_memory\u001b[39m=\u001b[39min_memory,\n\u001b[0;32m    370\u001b[0m     skip_header\u001b[39m=\u001b[39mskip_header,\n\u001b[0;32m    371\u001b[0m     encoding\u001b[39m=\u001b[39mencoding,\n\u001b[0;32m    372\u001b[0m     no_class_label\u001b[39m=\u001b[39mno_class_label,\n\u001b[0;32m    373\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfmtparams,\n\u001b[0;32m    374\u001b[0m )\n\u001b[0;32m    376\u001b[0m test \u001b[39m=\u001b[39m (\n\u001b[0;32m    377\u001b[0m     CSVClassificationDataset(\n\u001b[0;32m    378\u001b[0m         test_file,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    391\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    392\u001b[0m )\n\u001b[0;32m    394\u001b[0m dev \u001b[39m=\u001b[39m (\n\u001b[0;32m    395\u001b[0m     CSVClassificationDataset(\n\u001b[0;32m    396\u001b[0m         dev_file,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    409\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    410\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Leste\\anaconda3\\envs\\dataweek2\\lib\\site-packages\\flair\\datasets\\document_classification.py:503\u001b[0m, in \u001b[0;36mCSVClassificationDataset.__init__\u001b[1;34m(self, path_to_file, column_name_map, label_type, max_tokens_per_doc, max_chars_per_doc, tokenizer, in_memory, skip_header, encoding, no_class_label, **fmtparams)\u001b[0m\n\u001b[0;32m    501\u001b[0m has_label \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    502\u001b[0m \u001b[39mfor\u001b[39;00m column \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumn_name_map:\n\u001b[1;32m--> 503\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumn_name_map[column]\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m row[column]:\n\u001b[0;32m    504\u001b[0m         has_label \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    505\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.datasets import CSVClassificationCorpus\n",
    "\n",
    "# this is the folder in which train, test and dev files reside\n",
    "data_folder = 'C:/Users/Leste/Documents/GitHub/nlpsumm/BERT/data_processed'\n",
    "\n",
    "# column format indicating which columns hold the text and label(s)\n",
    "column_name_map = {1: \"text\", 2: \"label_topic\"}\n",
    "\n",
    "# load corpus containing training, test and dev data and if CSV has a header, you can skip it\n",
    "corpus: Corpus = CSVClassificationCorpus(data_folder,\n",
    "                                         column_name_map,\n",
    "                                         delimiter = ',',\n",
    "                                         label_type='heart')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90afbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = open('C:/Users/Leste/Documents/GitHub/nlpsumm/BERT/data_processed/dev.txt', 'r')\n",
    "text = test.read()\n",
    "f = open('C:/Users/Leste/Documents/GitHub/nlpsumm/BERT/data_processed/new_dev.txt', 'w')\n",
    "f.write(text)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533dcfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import TransformerWordEmbeddings\n",
    "from flair.models import SequenceTagger\n",
    "from flair.trainers import ModelTrainer\n",
    "from flair.datasets import UD_ENGLISH\n",
    "# 1. get the corpus\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94621d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the number of Sentences in the train split\n",
    "print(len(corpus.train))\n",
    "\n",
    "# print the number of Sentences in the test split\n",
    "print(len(corpus.test))\n",
    "\n",
    "# print the number of Sentences in the dev split\n",
    "print(len(corpus.dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cd688e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. what label do we want to predict?\n",
    "label_type = 'upos'\n",
    "\n",
    "\n",
    "# 3. make the label dictionary from the corpus\n",
    "label_dict = corpus.make_label_dictionary(label_type=label_type)\n",
    "print(label_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eae9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. initialize fine-tuneable transformer embeddings WITH document context\n",
    "embeddings = TransformerWordEmbeddings(model='xlm-roberta-large',\n",
    "                                       layers=\"-1\",\n",
    "                                       subtoken_pooling=\"first\",\n",
    "                                       fine_tune=True,\n",
    "                                       use_context=True,\n",
    "                                       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a929f9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. initialize bare-bones sequence tagger (no CRF, no RNN, no reprojection)\n",
    "tagger = SequenceTagger(hidden_size=256,\n",
    "                        embeddings=embeddings,\n",
    "                        tag_dictionary=label_dict,\n",
    "                        tag_type=label_type,\n",
    "                        use_crf=False,\n",
    "                        use_rnn=False,\n",
    "                        reproject_embeddings=False,\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a755c1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6. initialize trainer\n",
    "trainer = ModelTrainer(tagger, corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5553834b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 7. run fine-tuning\n",
    "trainer.fine_tune('resources/taggers/example-roberta',\n",
    "                  learning_rate=0.1,\n",
    "                  mini_batch_size=32,\n",
    "                  mini_batch_chunk_size=1,  # remove this parameter to speed up computation if you have a big GPU\n",
    "                  max_epochs = 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('dataweek2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4600a15e5b6b646e3db8b2d8ebb346444f23fb424fa93d368313be03df29f350"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
