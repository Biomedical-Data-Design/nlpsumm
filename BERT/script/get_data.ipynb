{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f12cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from os import listdir\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from flair.data import Sentence\n",
    "import flair\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from difflib import SequenceMatcher\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65c0c47",
   "metadata": {},
   "source": [
    "# Split text and tag\n",
    "\n",
    "Smoker and Family_hist can have no tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1a3360",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"C:/Users/Leste/Desktop/BDD data/extracted/training-RiskFactors-Complete-Set1/\"\n",
    "names = [f for f in listdir(file_path) if f.endswith('.xml')]\n",
    "train = pd.DataFrame(np.zeros((len(names), 2), dtype=object), columns=['text', 'annotation'])\n",
    "n = 0\n",
    "for name in names:\n",
    "    tree = ET.parse(file_path + name)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    ## Get the text\n",
    "    nt = re.sub('\\n',' ',root[0].text)\n",
    "    nt = re.sub('\\t',' ',nt) \n",
    "    nt = re.sub('\"',\"'\",nt)\n",
    "    ## sample 214 has a weird character\n",
    "    #nt = re.sub('>','&gt;',nt) \n",
    "    #nt = re.sub('<','&lt;',nt)\n",
    "    train['text'][n] = nt\n",
    "    n+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b39436",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 'hi    i love you'\n",
    "re.sub('  +',' ',x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a827526",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "for name in names:\n",
    "    tree = ET.parse(file_path + name)\n",
    "    root = tree.getroot()\n",
    "    ## Get the labels\n",
    "    # skip FAMILY_HIST: doesn't have a text\n",
    "    #skip = [root[1][x].tag for x in range(len(root[1]))].index('FAMILY_HIST')\n",
    "    PHI = [root[1][x].tag for x in range(len(root[1]))].index('PHI')\n",
    "    # get all labels except for PHI\n",
    "    tag_list = []\n",
    "    for k in range(len(root[1])):\n",
    "        for m in range(len(root[1][k])):\n",
    "            if root[1][k][m].attrib.keys().__contains__('text') == False:\n",
    "                continue\n",
    "            tag_list.append((root[1][k][m].attrib['text'],root[1][k][m].tag))\n",
    "    # get PHI labels\n",
    "    for k in range(PHI,len(root[1])):\n",
    "        tag_list.append((root[1][k].attrib['text'],root[1][k].tag))\n",
    "    train['annotation'][n] = tag_list\n",
    "    n+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93ebd4f",
   "metadata": {},
   "source": [
    "# Functions to convert dataframe into input format\n",
    "## use wisely, very slow (520 files takes 40 min to create labels)\n",
    " Please use create_labs function instead of mark_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e3a81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matcher(string, pattern):\n",
    "    '''\n",
    "    Return the start and end index of any pattern present in the text.\n",
    "    '''\n",
    "    match_list = []\n",
    "    pattern = pattern.strip()\n",
    "    seqMatch = SequenceMatcher(None, string, pattern, autojunk=False)\n",
    "    match = seqMatch.find_longest_match(0, len(string), 0, len(pattern))\n",
    "    if (match.size == len(pattern)):\n",
    "        start = match.a\n",
    "        end = match.a + match.size\n",
    "        match_tup = (start, end)\n",
    "        string = string.replace(pattern, \"X\" * len(pattern), 1)\n",
    "        match_list.append(match_tup)\n",
    "        \n",
    "    return match_list, string\n",
    "\n",
    "\n",
    "def mark_sentence(s, match_list):\n",
    "    '''\n",
    "    Marks all the entities in the sentence as per the BIO scheme. \n",
    "    '''\n",
    "    word_dict = {}\n",
    "    for word in s.split():\n",
    "        word_dict[word] = 'O'\n",
    "        \n",
    "    for start, end, e_type in match_list:\n",
    "        temp_str = s[start:end]\n",
    "        tmp_list = temp_str.split()\n",
    "        if len(tmp_list) > 1:\n",
    "            word_dict[tmp_list[0]] = 'B-' + e_type\n",
    "            for w in tmp_list[1:]:\n",
    "                word_dict[w] = 'I-' + e_type\n",
    "        else:\n",
    "            word_dict[temp_str] = 'B-' + e_type\n",
    "    return word_dict\n",
    "\n",
    "## replace !!mark_sentence!! to better label the text with more than one word\n",
    "def create_labs(s,match_list):\n",
    "    labs = ['O' for i in range(len(s.split()))]\n",
    "    word_dict = pd.DataFrame({'word':s.split(),'label':labs})\n",
    "\n",
    "    for start, end, e_type in match_list:\n",
    "        index = len(re.findall(r' +',s[0:start]))-1\n",
    "        num_words = len(s[start:end].split())\n",
    "\n",
    "        if num_words > 1:\n",
    "            word_dict.loc[index,'label'] = 'B-' + e_type\n",
    "            for i in range(1,num_words):\n",
    "                word_dict.loc[index+i,'label'] = 'I-' + e_type\n",
    "        else:\n",
    "            word_dict.loc[index,'label'] = 'B-' + e_type\n",
    "    return word_dict\n",
    "\n",
    "def clean(text):\n",
    "    '''\n",
    "    Just a helper fuction to add a space before the punctuations for better tokenization\n",
    "    '''\n",
    "    filters = [\"!\", \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"/\", \"*\", \".\", \":\", \";\", \"<\", \"=\", \">\", \"?\", \"@\", \"[\",\n",
    "               \"\\\\\", \"]\", \"_\", \"`\", \"{\", \"}\", \"~\", \"'\"]\n",
    "    for i in text:\n",
    "        if i in filters:\n",
    "            text = text.replace(i, \" \" + i)\n",
    "            \n",
    "    return text\n",
    "\n",
    "def create_data(df, filepath):\n",
    "    '''\n",
    "    The function responsible for the creation of data in the said format.\n",
    "    '''\n",
    "    with open(filepath , 'w') as f:\n",
    "        for text, annotation in zip(df.text, df.annotation):\n",
    "            #text = clean(text)\n",
    "            #text_ = text    \n",
    "            print(train.index[train['text']== text].tolist())    \n",
    "            match_list = []\n",
    "            for i in annotation:\n",
    "                a, text_ = matcher(text, i[0])\n",
    "                match_list.append((a[0][0], a[0][1], i[1]))\n",
    "\n",
    "            d = create_labs(text, match_list)\n",
    "\n",
    "            #for i in d.keys():\n",
    "            #    f.writelines(i + ' ' + d[i] +'\\n')\n",
    "            #f.writelines('\\n')\n",
    "            for i in range(d.shape[0]):\n",
    "                f.writelines(d['word'][i] + ' ' + d['label'][i] +'\\n')\n",
    "            f.writelines('\\n')\n",
    "            \n",
    "def main():\n",
    "    ## An example dataframe.\n",
    "    #data = pd.DataFrame([[\"Horses are too tall and they pretend to care about your feelings\", [(\"Horses\", \"ANIMAL\")]],\n",
    "    #              [\"Who is Shaka Khan?\", [(\"Shaka Khan\", \"PERSON\")]],\n",
    "    #              [\"I like London and Berlin.\", [(\"London\", \"LOCATION\"), (\"Berlin\", \"LOCATION\")]],\n",
    "    #              [\"There is a banyan tree in the courtyard\", [(\"banyan tree\", \"TREE\")]]], columns=['text', 'annotation'])\n",
    "    data = train.copy()\n",
    "    ## path to save the txt file.\n",
    "    filepath = 'tttt.txt'\n",
    "    ## creating the file.\n",
    "    create_data(data, filepath)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538e09a0",
   "metadata": {},
   "source": [
    "# Bunch of code for testing errors\n",
    "## trash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e5bb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matcher(string, pattern):\n",
    "    '''\n",
    "    Return the start and end index of any pattern present in the text.\n",
    "    '''\n",
    "    match_list = []\n",
    "    pattern = pattern.strip()\n",
    "    seqMatch = SequenceMatcher(None, string, pattern, autojunk=False)\n",
    "    match = seqMatch.find_longest_match(0, len(string), 0, len(pattern))\n",
    "    if (match.size == len(pattern)):\n",
    "        start = match.a\n",
    "        end = match.a + match.size\n",
    "        match_tup = (start, end)\n",
    "        string = string.replace(pattern, \"X\" * len(pattern), 1)\n",
    "        match_list.append(match_tup)\n",
    "        \n",
    "    return match_list, string\n",
    "text = train['text'][0]\n",
    "annotation = train['annotation'][0]\n",
    "#pattern = pattern.strip()\n",
    "match_list = []\n",
    "for i in annotation:\n",
    "    a, text_ = matcher(text, i[0])\n",
    "    match_list.append((a[0][0], a[0][1], i[1]))\n",
    "\n",
    "\n",
    "match_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2201cccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labs(s,match_list):\n",
    "    labs = ['O' for i in range(len(s.split()))]\n",
    "    word_dict = pd.DataFrame({'word':s.split(),'label':labs})\n",
    "\n",
    "    for start, end, e_type in match_list:\n",
    "        index = len(re.findall(r' +',s[0:start]))-1\n",
    "        num_words = len(s[start:end].split())\n",
    "        index,num_words\n",
    "\n",
    "        if num_words > 1:\n",
    "            word_dict.loc[index,'label'] = 'B-' + e_type\n",
    "            for i in range(1,num_words):\n",
    "                word_dict.loc[index+i,'label'] = 'I-' + e_type\n",
    "        else:\n",
    "            word_dict.loc[index,'label'] = 'B-' + e_type\n",
    "    return word_dict\n",
    "\n",
    "x = create_labs(train['text'][0],match_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561cf224",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f855646",
   "metadata": {},
   "source": [
    "## test how to find the word using index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3754ca0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "a = s.split()\n",
    "a.index('smoking')\n",
    "len(re.findall(r' +',s[0:start]))\n",
    "a[86],len(s[start:end].split())\n",
    "#temp_str = pd.DataFrame(s.split(), columns=['word'])\n",
    "#temp_str['len'] = temp_str['word'].apply(lambda x: len(x))\n",
    "#temp_str['cumsum'] = temp_str['len'].cumsum()\n",
    "#temp_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2de0ab",
   "metadata": {},
   "source": [
    "## find which label went crazy and can't match text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7771983",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_list = []\n",
    "text = train['text'][176]\n",
    "#text = \"Horses are too tall and they pretend to care about your feelings\"\n",
    "annotation = train['annotation'][176]\n",
    "for i in annotation:\n",
    "    print(i)\n",
    "    a, text_ = matcher(text, i[0])\n",
    "    print(a)\n",
    "    match_list.append((a[0][0], a[0][1], i[1]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafaa13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'][214]\n",
    "#tree = ET.parse(file_path + names[62])\n",
    "#root = tree.getroot()\n",
    "#root[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f3dd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "names[214]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1f304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ET.parse(file_path + names[0])\n",
    "root = tree.getroot()\n",
    "## Get the labels\n",
    "# skip FAMILY_HIST: doesn't have a text\n",
    "#skip = [root[1][x].tag for x in range(len(root[1]))].index('FAMILY_HIST')\n",
    "PHI = [root[1][x].tag for x in range(len(root[1]))].index('PHI')\n",
    "# get all labels except for PHI\n",
    "tag_list = []\n",
    "root[1][5][0].attrib['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5abe06",
   "metadata": {},
   "source": [
    "# create a list of tag as tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b84b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.datasets import ColumnCorpus\n",
    "# define columns\n",
    "columns = {0 : 'text', 1 : 'ner'}\n",
    "# directory where the data resides\n",
    "data_folder = 'C:/Users/Leste/Desktop/BDD data'\n",
    "# initializing the corpus\n",
    "corpus: Corpus = ColumnCorpus(data_folder, columns,\n",
    "                              train_file = 'train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23eb1177",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.train[0].to_tagged_string('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5aa772",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"C:/Users/Leste/Desktop/BDD data/extracted/training-RiskFactors-Complete-Set1/\"\n",
    "names = [f for f in listdir(file_path) if f.endswith('.xml')]\n",
    "tree = ET.parse(file_path + names[0])\n",
    "root = tree.getroot()\n",
    "\n",
    "## skip FAMILY_HIST: doesn't have a text\n",
    "skip = [root[1][x].tag for x in range(len(root[1]))].index('FAMILY_HIST')\n",
    "PHI = [root[1][x].tag for x in range(len(root[1]))].index('PHI')\n",
    "\n",
    "## get all labels except for PHI\n",
    "tag_list = []\n",
    "for n in range(len(root[1])):\n",
    "    if n == skip:\n",
    "        continue\n",
    "    for m in range(len(root[1][n])):\n",
    "        tag_list.append((root[1][n][m].attrib['text'],root[1][n][m].tag))\n",
    "\n",
    "## get PHI labels\n",
    "for n in range(PHI,len(root[1])):\n",
    "    tag_list.append((root[1][n].attrib['text'],root[1][n].tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d8dbe9",
   "metadata": {},
   "source": [
    "## clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b14f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "file_path = \"C:/Users/Leste/Desktop/BDD data/extracted/training-RiskFactors-Complete-Set1/\"\n",
    "names = [f for f in listdir(file_path) if f.endswith('.xml')]\n",
    "tree = ET.parse(file_path + names[0])\n",
    "root = tree.getroot()\n",
    "nt = re.sub('\\n','',root[0].text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72322f28",
   "metadata": {},
   "source": [
    "## Use trained model from flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac7ffed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.models import SequenceTagger\n",
    "from flair.tokenization import SegtokSentenceSplitter\n",
    "splitter = SegtokSentenceSplitter()\n",
    "tagger = SequenceTagger.load('ner')\n",
    "test = text_list[names[0]]\n",
    "sentences = splitter.split(test)\n",
    "sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8afa84",
   "metadata": {},
   "source": [
    "## Try training NER with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533dcfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import TransformerWordEmbeddings\n",
    "from flair.models import SequenceTagger\n",
    "from flair.trainers import ModelTrainer\n",
    "from flair.datasets import UD_ENGLISH\n",
    "# 1. get the corpus\n",
    "corpus = UD_ENGLISH().downsample(0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94621d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "print(pd.DataFrame(corpus.test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cd688e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. what label do we want to predict?\n",
    "label_type = 'upos'\n",
    "\n",
    "\n",
    "# 3. make the label dictionary from the corpus\n",
    "label_dict = corpus.make_label_dictionary(label_type=label_type)\n",
    "print(label_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eae9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. initialize fine-tuneable transformer embeddings WITH document context\n",
    "embeddings = TransformerWordEmbeddings(model='xlm-roberta-large',\n",
    "                                       layers=\"-1\",\n",
    "                                       subtoken_pooling=\"first\",\n",
    "                                       fine_tune=True,\n",
    "                                       use_context=True,\n",
    "                                       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a929f9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. initialize bare-bones sequence tagger (no CRF, no RNN, no reprojection)\n",
    "tagger = SequenceTagger(hidden_size=256,\n",
    "                        embeddings=embeddings,\n",
    "                        tag_dictionary=label_dict,\n",
    "                        tag_type=label_type,\n",
    "                        use_crf=False,\n",
    "                        use_rnn=False,\n",
    "                        reproject_embeddings=False,\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a755c1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6. initialize trainer\n",
    "trainer = ModelTrainer(tagger, corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5553834b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 7. run fine-tuning\n",
    "trainer.fine_tune('resources/taggers/example-roberta',\n",
    "                  learning_rate=0.1,\n",
    "                  mini_batch_size=32,\n",
    "                  mini_batch_chunk_size=1,  # remove this parameter to speed up computation if you have a big GPU\n",
    "                  max_epochs = 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('dataweek2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4600a15e5b6b646e3db8b2d8ebb346444f23fb424fa93d368313be03df29f350"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
