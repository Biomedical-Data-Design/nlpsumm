{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f65c0c47",
   "metadata": {},
   "source": [
    "# Split text and tag\n",
    "\n",
    "Smoker and Family_hist can have no tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be1a3360",
   "metadata": {},
   "outputs": [],
   "source": [
    "#according to flair corpus \"UD_ENGLISH\", train:test:dev = 6:1:1\n",
    "from os import listdir\n",
    "from sklearn.model_selection import train_test_split\n",
    "file_path_1 = \"C:/Users/Leste/OneDrive - Johns Hopkins/Desktop/BDD data/extracted/training-RiskFactors-Complete-Set1/\"\n",
    "file_path_2 = \"C:/Users/Leste/OneDrive - Johns Hopkins/Desktop/BDD data/extracted/training-RiskFactors-Complete-Set2/\"\n",
    "file_path_3 = \"C:/Users/Leste/OneDrive - Johns Hopkins/Desktop/BDD data/extracted/testing-RiskFactors-Complete/\"\n",
    "names_1 = [f for f in listdir(file_path_1) if f.endswith('.xml')]\n",
    "names_2 = [f for f in listdir(file_path_2) if f.endswith('.xml')]\n",
    "names_3 = [f for f in listdir(file_path_3) if f.endswith('.xml')] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28f8a76c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "521"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(names_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45a6f885",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#train_1_df = pd.DataFrame(np.zeros((len(train_n_1), 2), dtype=object), columns=['text', 'annotation'])\n",
    "#dev_1_df = pd.DataFrame(np.zeros((len(dev_n_1), 2), dtype=object), columns=['text', 'annotation'])\n",
    "all1_df = pd.DataFrame(np.zeros((len(names_1), 2), dtype=object), columns=['text', 'annotation'])\n",
    "all2_df = pd.DataFrame(np.zeros((len(names_2), 2), dtype=object), columns=['text', 'annotation'])\n",
    "all3_df = pd.DataFrame(np.zeros((len(names_3), 2), dtype=object), columns=['text', 'annotation'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18994392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "def to_df(names,df, file_path,PHI_status = True):\n",
    "    #get text\n",
    "    n = 0\n",
    "    for name in names:\n",
    "        tree = ET.parse(file_path + name)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        ## Get the text\n",
    "        nt = re.sub('\\n',' ',root[0].text)\n",
    "        nt = re.sub('\\t',' ',nt) \n",
    "        nt = re.sub('\"',\"'\",nt)\n",
    "        ## sample 214 has a weird character\n",
    "        nt = re.sub('>','&gt;',nt) \n",
    "        nt = re.sub('<','&lt;',nt)\n",
    "        ## new wired character\n",
    "        nt = re.sub('Â',' ',nt)\n",
    "        nt = re.sub('â',' ',nt)\n",
    "        nt = re.sub('€',' ',nt)\n",
    "        nt = re.sub('™',' ',nt)\n",
    "        df['text'][n] = nt\n",
    "        n+=1\n",
    "    \n",
    "    #get annotations\n",
    "    n = 0\n",
    "    for name in names:\n",
    "        tree = ET.parse(file_path + name)\n",
    "        root = tree.getroot()\n",
    "        ## Get the labels\n",
    "\n",
    "        tag_list = []\n",
    "        # get PHI labels if there are any\n",
    "        if PHI_status == True:\n",
    "            PHI = [root[1][x].tag for x in range(len(root[1]))].index('PHI')\n",
    "            for k in range(PHI,len(root[1])):\n",
    "                tag_list.append((root[1][k].attrib['text'],root[1][k].tag))\n",
    "        \n",
    "        # get the rest of labels\n",
    "        for k in range(len(root[1])):\n",
    "            for m in range(len(root[1][k])):\n",
    "                if root[1][k][m].attrib.keys().__contains__('text') == False:\n",
    "                    continue\n",
    "                tag_list.append((root[1][k][m].attrib['text'],root[1][k][m].tag))\n",
    "        \n",
    "        df['annotation'][n] = tag_list\n",
    "        n+=1\n",
    "    return df\n",
    "\n",
    "#train_1 = to_df(train_n_1,train_1_df, file_path_1, PHI_status = False)\n",
    "#dev_1 = to_df(dev_n_1,dev_1_df, file_path_1, PHI_status = False)\n",
    "#test_1 = to_df(test_n_1,test_df_1, file_path_1)\n",
    "#train_2 = to_df(train_n_2,train_df_2, file_path_2, PHI_status = False)\n",
    "#dev_2 = to_df(dev_n_2,dev_df_2, file_path_2, PHI_status = False)\n",
    "#test_2 = to_df(test_n_2,test_df_2, file_path_2, PHI_status = False)\n",
    "all_1 = to_df(names_1,all1_df, file_path_1, PHI_status = False)\n",
    "#all_2 = to_df(names_2,all2_df, file_path_2, PHI_status = False)\n",
    "#all_3 = to_df(names_3,all3_df, file_path_3, PHI_status = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b40ace78",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([all_1, all_2], ignore_index=True)\n",
    "test = all_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93ebd4f",
   "metadata": {},
   "source": [
    "# Functions to convert dataframe into input format\n",
    "520 files takes 3 min\\\n",
    "Please use create_labs function instead of mark_sentences\\\n",
    "source: https://medium.com/thecyphy/training-custom-ner-model-using-flair-df1f9ea9c762\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e3a81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from difflib import SequenceMatcher\n",
    "import pickle\n",
    "\n",
    "def matcher(string, pattern):\n",
    "    '''\n",
    "    Return the start and end index of any pattern present in the text.\n",
    "    '''\n",
    "    match_list = []\n",
    "    pattern = pattern.strip()\n",
    "    seqMatch = SequenceMatcher(None, string, pattern, autojunk=False)\n",
    "    match = seqMatch.find_longest_match(0, len(string), 0, len(pattern))\n",
    "    if (match.size == len(pattern)):\n",
    "        start = match.a\n",
    "        end = match.a + match.size\n",
    "        match_tup = (start, end)\n",
    "        string = string.replace(pattern, \"X\" * len(pattern), 1)\n",
    "        match_list.append(match_tup)\n",
    "        \n",
    "    return match_list, string\n",
    "\n",
    "\n",
    "#def mark_sentence(s, match_list):\n",
    "#    '''\n",
    "#    Marks all the entities in the sentence as per the BIO scheme. \n",
    "#    '''\n",
    "#    word_dict = {}\n",
    "#    for word in s.split():\n",
    "#        word_dict[word] = 'O'\n",
    "#        \n",
    "#    for start, end, e_type in match_list:\n",
    "#        temp_str = s[start:end]\n",
    "#        tmp_list = temp_str.split()\n",
    "#        if len(tmp_list) > 1:\n",
    "#            word_dict[tmp_list[0]] = 'B-' + e_type\n",
    "#            for w in tmp_list[1:]:\n",
    "#                word_dict[w] = 'I-' + e_type\n",
    "#        else:\n",
    "#            word_dict[temp_str] = 'B-' + e_type\n",
    "#    return word_dict\n",
    "\n",
    "## replace !!mark_sentence!! to better label the text with more than one word\n",
    "\"\"\" def create_labs(s,match_list):\n",
    "    labs = ['O' for i in range(len(s.split()))]\n",
    "    word_dict = pd.DataFrame({'word':s.split(),'label':labs})\n",
    "\n",
    "    for start, end, e_type in match_list:\n",
    "        index = len(re.findall(r' +',s[0:start]))-1\n",
    "        num_words = len(s[start:end].split())\n",
    "\n",
    "        if num_words > 1:\n",
    "            word_dict.loc[index,'label'] = 'B-' + e_type\n",
    "            for i in range(1,num_words):\n",
    "                word_dict.loc[index+i,'label'] = 'I-' + e_type\n",
    "        else:\n",
    "            word_dict.loc[index,'label'] = 'B-' + e_type\n",
    "    return word_dict \"\"\"\n",
    "\n",
    "def create_labs(s,match_list):\n",
    "    labs = ['O' for i in range(len(s.split()))]\n",
    "    word_dict = pd.DataFrame({'word':s.split(),'label':labs})\n",
    "\n",
    "    for start, end, e_type in match_list:\n",
    "        index = len(re.findall(r' +',s[0:start]))-1\n",
    "        num_words = len(s[start:end].split())\n",
    "\n",
    "        if num_words > 1:\n",
    "            word_dict.loc[index,'label'] = e_type\n",
    "            for i in range(1,num_words):\n",
    "                word_dict.loc[index+i,'label'] = e_type\n",
    "        else:\n",
    "            word_dict.loc[index,'label'] = e_type\n",
    "    return word_dict\n",
    "\n",
    "def clean(text):\n",
    "    '''\n",
    "    Just a helper fuction to add a space before the punctuations for better tokenization\n",
    "    '''\n",
    "    filters = [\"!\", \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"/\", \"*\", \".\", \":\", \";\", \"<\", \"=\", \">\", \"?\", \"@\", \"[\",\n",
    "               \"\\\\\", \"]\", \"_\", \"`\", \"{\", \"}\", \"~\", \"'\"]\n",
    "    for i in text:\n",
    "        if i in filters:\n",
    "            text = text.replace(i, \" \" + i)\n",
    "            \n",
    "    return text\n",
    "\n",
    "def to_txt(df, filepath):\n",
    "    '''\n",
    "    The function responsible for the creation of data in the said format.\n",
    "    '''\n",
    "    with open(filepath , 'w') as f:\n",
    "        for text, annotation in zip(df.text, df.annotation):\n",
    "            #text = clean(text)\n",
    "            #text_ = text    \n",
    "            print(df.index[df['text']== text].tolist())    \n",
    "            match_list = []\n",
    "            for i in annotation:\n",
    "                a, text_ = matcher(text, i[0])\n",
    "                match_list.append((a[0][0], a[0][1], i[1]))\n",
    "\n",
    "            d = create_labs(text, match_list)\n",
    "\n",
    "            #for i in d.keys():\n",
    "            #    f.writelines(i + ' ' + d[i] +'\\n')\n",
    "            #f.writelines('\\n')\n",
    "            for i in range(d.shape[0]):\n",
    "                f.writelines(d['word'][i] + ' ' + d['label'][i] +'\\n')\n",
    "            f.writelines('\\n')\n",
    "def to_csv(df, filepath):\n",
    "    for text, annotation in zip(df.text, df.annotation):\n",
    "            #text = clean(text)\n",
    "            #text_ = text    \n",
    "            print(df.index[df['text']== text].tolist())    \n",
    "            match_list = []\n",
    "            for i in annotation:\n",
    "                a, text_ = matcher(text, i[0])\n",
    "                match_list.append((a[0][0], a[0][1], i[1]))\n",
    "\n",
    "            d = create_labs(text, match_list)\n",
    "            d.to_csv(filepath, index = False, header = False, mode = 'a')\n",
    "            \n",
    "def main(input,save_path):\n",
    "    ## An example dataframe.\n",
    "    #data = pd.DataFrame([[\"Horses are too tall and they pretend to care about your feelings\", [(\"Horses\", \"ANIMAL\")]],\n",
    "    #              [\"Who is Shaka Khan?\", [(\"Shaka Khan\", \"PERSON\")]],\n",
    "    #              [\"I like London and Berlin.\", [(\"London\", \"LOCATION\"), (\"Berlin\", \"LOCATION\")]],\n",
    "    #              [\"There is a banyan tree in the courtyard\", [(\"banyan tree\", \"TREE\")]]], columns=['text', 'annotation'])\n",
    "    data = input\n",
    "    ## path to save the txt file.\n",
    "    ## creating the file.\n",
    "    to_txt(data, save_path)\n",
    "    \n",
    "#if __name__ == '__main__':\n",
    "path = 'C:/Users/Leste/OneDrive - Johns Hopkins/Documents/GitHub/nlpsumm/BERT/data_processed/'\n",
    "\n",
    "#main(train,path+'train.txt')\n",
    "main(all_1,path+'train_1(no_BIO).txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f03766e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from difflib import SequenceMatcher\n",
    "import pickle\n",
    "\n",
    "def matcher(string, pattern):\n",
    "    '''\n",
    "    Return the start and end index of any pattern present in the text.\n",
    "    '''\n",
    "    match_list = []\n",
    "    pattern = pattern.strip()\n",
    "    seqMatch = SequenceMatcher(None, string, pattern, autojunk=False)\n",
    "    match = seqMatch.find_longest_match(0, len(string), 0, len(pattern))\n",
    "    if (match.size == len(pattern)):\n",
    "        start = match.a\n",
    "        end = match.a + match.size\n",
    "        match_tup = (start, end)\n",
    "        string = string.replace(pattern, \"X\" * len(pattern), 1)\n",
    "        match_list.append(match_tup)\n",
    "        \n",
    "    return match_list, string\n",
    "text = all_1['text'][0]\n",
    "annotation = all_1['annotation'][0]\n",
    "match_list = []\n",
    "for i in annotation:\n",
    "    a, text_ = matcher(text, i[0])\n",
    "    match_list.append((a[0][0], a[0][1], i[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f839ad0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labs(s,match_list):\n",
    "    labs = ['O' for i in range(len(s.split()))]\n",
    "    word_dict = pd.DataFrame({'word':s.split(),'label':labs})\n",
    "\n",
    "    for start, end, e_type in match_list:\n",
    "        index = len(re.findall(r' +',s[0:start]))-1\n",
    "        num_words = len(s[start:end].split())\n",
    "\n",
    "        if num_words > 1:\n",
    "            word_dict.loc[index,'label'] = e_type\n",
    "            for i in range(1,num_words):\n",
    "                word_dict.loc[index+i,'label'] = e_type\n",
    "        else:\n",
    "            word_dict.loc[index,'label'] = e_type\n",
    "    return word_dict\n",
    "d = create_labs(text, match_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2edab570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ZESTRIL'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_1['text'][0][match_list[0][0]:match_list[0][1]]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5dd1d84",
   "metadata": {},
   "source": [
    "# Test which tag is wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "5a0af627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231-01.xml\n",
      "[(974, 990)]\n",
      "[(974, 990)]\n",
      "[(974, 990)]\n",
      "[(2605, 2615)]\n",
      "[(974, 990)]\n",
      "[(974, 990)]\n",
      "[(974, 990)]\n",
      "[(2605, 2615)]\n",
      "[(974, 990)]\n",
      "[(974, 990)]\n",
      "[(974, 990)]\n",
      "[(2605, 2615)]\n",
      "[(756, 759)]\n",
      "[(756, 759)]\n",
      "[(756, 759)]\n",
      "[(756, 759)]\n",
      "[(1094, 1100)]\n",
      "[(1094, 1100)]\n",
      "[(756, 759)]\n",
      "[(756, 759)]\n",
      "[(756, 759)]\n",
      "[(756, 759)]\n",
      "[(1066, 1073)]\n",
      "[(1066, 1073)]\n",
      "[(1066, 1073)]\n",
      "[(1124, 1136)]\n",
      "[(1124, 1131)]\n",
      "[(1066, 1073)]\n",
      "[(1066, 1073)]\n",
      "[(1066, 1073)]\n",
      "[(1094, 1100)]\n",
      "[(1094, 1100)]\n",
      "[(1037, 1043)]\n",
      "[(1037, 1043)]\n",
      "[(1037, 1043)]\n",
      "[(1051, 1057)]\n",
      "[(1051, 1057)]\n",
      "[(756, 759)]\n",
      "[(756, 759)]\n",
      "[(756, 759)]\n",
      "[(756, 759)]\n",
      "[(1124, 1136)]\n",
      "[(1124, 1131)]\n",
      "[(1066, 1073)]\n",
      "[(1066, 1073)]\n",
      "[(1066, 1073)]\n",
      "[(1094, 1100)]\n",
      "[(1094, 1100)]\n",
      "[(1124, 1136)]\n",
      "[(1124, 1131)]\n",
      "[(1037, 1043)]\n",
      "[(1037, 1043)]\n",
      "[(1037, 1043)]\n",
      "[(1037, 1043)]\n",
      "[(1037, 1043)]\n",
      "[(1037, 1043)]\n",
      "[(1051, 1057)]\n",
      "[(1051, 1057)]\n",
      "[(1051, 1057)]\n",
      "[(1051, 1057)]\n",
      "[(222, 225)]\n",
      "[(222, 225)]\n",
      "[(222, 225)]\n",
      "[(222, 225)]\n",
      "[(222, 225)]\n",
      "[(1615, 1626)]\n",
      "[(1619, 1626)]\n",
      "[(1614, 1626)]\n",
      "[(222, 225)]\n",
      "[(222, 225)]\n",
      "[(222, 225)]\n",
      "[(222, 225)]\n",
      "[(222, 225)]\n",
      "[(222, 225)]\n",
      "[(222, 225)]\n",
      "[(222, 225)]\n",
      "[(222, 225)]\n",
      "[(222, 225)]\n",
      "[(204, 207)]\n",
      "[(204, 207)]\n",
      "[(204, 207)]\n",
      "[(2631, 2649)]\n",
      "[(2631, 2649)]\n",
      "[(2631, 2649)]\n",
      "[(2631, 2649)]\n",
      "[(204, 207)]\n",
      "[(204, 207)]\n",
      "[(204, 207)]\n",
      "[(2631, 2649)]\n",
      "[(2631, 2649)]\n",
      "[(204, 207)]\n",
      "[(204, 207)]\n",
      "[(204, 207)]\n",
      "[(1327, 1339)]\n",
      "[(1327, 1339)]\n",
      "[(218, 220)]\n",
      "[(218, 220)]\n",
      "[(218, 220)]\n",
      "[(778, 782)]\n",
      "[(778, 782)]\n",
      "[(218, 220)]\n",
      "[(218, 220)]\n",
      "[(218, 220)]\n",
      "[(778, 782)]\n",
      "[(778, 782)]\n",
      "[(218, 220)]\n",
      "[(218, 220)]\n",
      "[(218, 220)]\n",
      "[(778, 782)]\n",
      "[(778, 782)]\n"
     ]
    }
   ],
   "source": [
    "print(names_3[262])\n",
    "match_list = []\n",
    "text = all_3['text'][262]\n",
    "annotation = all_3['annotation'][262]\n",
    "for i in annotation:\n",
    "    a, text_ = matcher(text, i[0])\n",
    "    print(a)\n",
    "    match_list.append((a[0][0], a[0][1], i[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8afa84",
   "metadata": {},
   "source": [
    "## Try training NER with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a929f9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-04 17:16:37,617 Reading data from C:\\Users\\Leste\\OneDrive - Johns Hopkins\\Documents\\GitHub\\nlpsumm\\BERT\\data_processed\n",
      "2023-02-04 17:16:37,619 Train: C:\\Users\\Leste\\OneDrive - Johns Hopkins\\Documents\\GitHub\\nlpsumm\\BERT\\data_processed\\all2.txt\n",
      "2023-02-04 17:16:37,621 Dev: None\n",
      "2023-02-04 17:16:37,623 Test: None\n"
     ]
    }
   ],
   "source": [
    "from flair.embeddings import TransformerWordEmbeddings\n",
    "from flair.models import SequenceTagger\n",
    "from flair.trainers import ModelTrainer\n",
    "from flair.data import Corpus\n",
    "from flair.datasets import ColumnCorpus\n",
    "\n",
    "# 1. get the corpus (create my corpus)\n",
    "#file location\n",
    "data_folder = 'C:/Users/Leste/OneDrive - Johns Hopkins/Documents/GitHub/nlpsumm/BERT/data_processed'\n",
    "\n",
    "# column format indicating which columns hold the text and label(s)\n",
    "columns = {0: 'text', 1: 'ner'}\n",
    "\n",
    "corpus: Corpus = ColumnCorpus(data_folder,columns,\n",
    "                              train_file = 'all2.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5a363dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-04 17:11:10,224 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-04 17:11:10,233 Dictionary created for label 'ner' with 3 values: SMOKER (seen 1 times), MEDICATION (seen 1 times)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-04 17:11:17,961 SequenceTagger predicts: Dictionary with 3 tags: <unk>, SMOKER, MEDICATION\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2. what label do we want to predict?\n",
    "label_type = 'ner'\n",
    "\n",
    "# 3. make the label dictionary from the corpus\n",
    "label_dict = corpus.make_label_dictionary(label_type=label_type)\n",
    "\n",
    "\n",
    "# 4. initialize fine-tuneable transformer embeddings WITH document context\n",
    "embeddings = TransformerWordEmbeddings(model='xlm-roberta-large',\n",
    "                                       layers=\"-1\",\n",
    "                                       subtoken_pooling=\"first\",\n",
    "                                       fine_tune=True,\n",
    "                                       use_context=True)\n",
    "\n",
    "# 5. initialize bare-bones sequence tagger (no CRF, no RNN, no reprojection)\n",
    "tagger = SequenceTagger(hidden_size=256,\n",
    "                        embeddings=embeddings,\n",
    "                        tag_dictionary=label_dict,\n",
    "                        tag_type=label_type,\n",
    "                        use_crf=False,\n",
    "                        use_rnn=False,\n",
    "                        reproject_embeddings=False\n",
    "                        )\n",
    "# 6. initialize trainer\n",
    "trainer = ModelTrainer(tagger, corpus)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('dataweek2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4600a15e5b6b646e3db8b2d8ebb346444f23fb424fa93d368313be03df29f350"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
