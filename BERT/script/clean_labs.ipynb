{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fae50b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "\n",
    "file_path_1 = \"C:/Users/Leste/OneDrive - Johns Hopkins/Desktop/BDD data/extracted/training-RiskFactors-Complete-Set1/\"\n",
    "file_path_2 = \"C:/Users/Leste/OneDrive - Johns Hopkins/Desktop/BDD data/extracted/training-RiskFactors-Complete-Set2/\"\n",
    "file_path_3 = \"C:/Users/Leste/OneDrive - Johns Hopkins/Desktop/BDD data/extracted/testing-RiskFactors-Complete/\"\n",
    "names_1 = [f for f in listdir(file_path_1) if f.endswith('.xml')]\n",
    "names_2 = [f for f in listdir(file_path_2) if f.endswith('.xml')]\n",
    "names_3 = [f for f in listdir(file_path_3) if f.endswith('.xml')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67ffed84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(521, 269, 514)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(names_1), len(names_2), len(names_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5531c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "all1_df = pd.DataFrame(np.zeros((len(names_1), 3), dtype=object), columns=['text', 'annotation','loc'])\n",
    "all2_df = pd.DataFrame(np.zeros((len(names_2), 3), dtype=object), columns=['text', 'annotation','loc'])\n",
    "test_df = pd.DataFrame(np.zeros((len(names_3), 3), dtype=object), columns=['text', 'annotation','loc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce1caa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "def to_df(names,df, file_path,PHI_status = True):\n",
    "    #get text\n",
    "    n = 0\n",
    "    for name in names:\n",
    "        tree = ET.parse(file_path + name)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        ## Get the text\n",
    "        nt = re.sub('\\n',' ',root[0].text)\n",
    "        nt = re.sub('\\t',' ',nt) \n",
    "        nt = re.sub('\"',\"'\",nt)\n",
    "        ## sample 214 has a weird character\n",
    "        nt = re.sub('>','&gt;',nt) \n",
    "        nt = re.sub('<','&lt;',nt)\n",
    "        ## new wired character\n",
    "        nt = re.sub('Â','',nt)\n",
    "        nt = re.sub('â','',nt)\n",
    "        nt = re.sub('€','',nt)\n",
    "        nt = re.sub('™','',nt)\n",
    "        df['text'][n] = nt\n",
    "        n+=1\n",
    "    \n",
    "    #get annotations\n",
    "    n = 0\n",
    "    for name in names:\n",
    "        tree = ET.parse(file_path + name)\n",
    "        root = tree.getroot()\n",
    "        ## Get the labels\n",
    "\n",
    "        tag_list = []\n",
    "        loc_list = []\n",
    "        # get PHI labels if there are any\n",
    "        if PHI_status == True:\n",
    "            PHI = [root[1][x].tag for x in range(len(root[1]))].index('PHI')\n",
    "            for k in range(PHI,len(root[1])):\n",
    "                tag_list.append((root[1][k].attrib['text'],root[1][k].tag))\n",
    "        \n",
    "        # get the rest of labels\n",
    "        for k in range(len(root[1])):\n",
    "            if root[1][k].tag == 'SMOKER':\n",
    "                continue\n",
    "            \n",
    "            for m in range(len(root[1][k])):\n",
    "                if root[1][k][m].attrib.keys().__contains__('text') == False:\n",
    "                    continue\n",
    "                tag_list.append((root[1][k][m].attrib['text'],root[1][k][m].tag))\n",
    "                loc_list.append((root[1][k][m].attrib['start'],root[1][k][m].attrib['end']))\n",
    "        df['annotation'][n] = tag_list\n",
    "        df['loc'][n] = loc_list\n",
    "        n+=1\n",
    "    return df\n",
    "\n",
    "all_1 = to_df(names_1,all1_df, file_path_1, PHI_status = False)\n",
    "all_2 = to_df(names_2,all2_df, file_path_2, PHI_status = False)\n",
    "test_df = to_df(names_3,test_df, file_path_3, PHI_status = False)\n",
    "train_df = pd.concat([all_1, all_2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2830bba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(ls1, ls2):\n",
    "    index = [i for i, x in enumerate([x == ls2[0] for x in ls1]) if x]\n",
    "    if index[-1] == len(ls1)-1:\n",
    "        index = index[:-1]\n",
    "    cut = 10000\n",
    "    eva_temp = 10000\n",
    "    for i in index:\n",
    "        if (len(ls1) -i) < len(ls2):\n",
    "            l = len(ls1) -i\n",
    "        else:\n",
    "            l = len(ls2)\n",
    "        eva = sum([ls1[x+i] != ls2[x] for x in range(l)])\n",
    "        if eva < eva_temp:\n",
    "            cut = i\n",
    "            eva_temp = eva\n",
    "    out = \"\".join(ls1[0:cut]+ls2)\n",
    "    return out\n",
    "\n",
    "def pre_process(input):\n",
    "    df = input.copy()\n",
    "\n",
    "    for i in range(df.shape[0]):  \n",
    "        for j in range(len(df['annotation'][i])):\n",
    "                ## preprocess tagged text and location\n",
    "                # remove extra spaces in the beginning and end of the annotation\n",
    "                if re.search(\"^ +.*\",df['annotation'][i][j][0]) != None or re.search(\".* +$\",df['annotation'][i][j][0]) != None:\n",
    "                    front = len(df['annotation'][i][j][0]) - len(re.sub(\"^ +\",\"\",df['annotation'][i][j][0]))\n",
    "                    end = len(df['annotation'][i][j][0]) - len(re.sub(\" +$\",\"\",df['annotation'][i][j][0]))\n",
    "                    df['loc'][i][j] = (str(int(df['loc'][i][j][0])+front),str(int(df['loc'][i][j][1])-end))\n",
    "                    df['annotation'][i][j] = (re.sub(\" +$\",\"\", re.sub(\"^ +\",\"\",df['annotation'][i][j][0])),df['annotation'][i][j][1])\n",
    "                if int(df['loc'][i][j][0]) == df['text'][i].find('Record'):\n",
    "                    df['loc'][i][j] = ('','')\n",
    "                    df['annotation'][i][j] = ('','')\n",
    "                    \n",
    "    return df\n",
    "\n",
    "def rm_dup(input):\n",
    "    df = input.copy()\n",
    "    for i in range(df.shape[0]):\n",
    "\n",
    "        for j in range(len(df['annotation'][i])):\n",
    "\n",
    "            for k in np.arange(j+1,len(df['annotation'][i])):\n",
    "                # move on if the compared lables are (\"\",\"\")\n",
    "                if df['loc'][i][j] == ('',''):\n",
    "                    break\n",
    "                if df['loc'][i][k] == ('',''):\n",
    "                    continue\n",
    "\n",
    "                # find location contained within each other\n",
    "                if int(df['loc'][i][j][0]) >= int(df['loc'][i][k][0]) and int(df['loc'][i][j][1]) <= int(df['loc'][i][k][1]):\n",
    "                    df['loc'][i][j] = ('','')\n",
    "                    df['annotation'][i][j] = ('','')\n",
    "                    continue\n",
    "                \n",
    "                elif int(df['loc'][i][j][0]) <= int(df['loc'][i][k][0]) and int(df['loc'][i][j][1]) >= int(df['loc'][i][k][1]):\n",
    "                    df['loc'][i][k] = ('','')\n",
    "                    df['annotation'][i][k] = ('','')\n",
    "                    continue\n",
    "\n",
    "                # find location that overlap\n",
    "                if int(df['loc'][i][j][0]) < int(df['loc'][i][k][0]) and int(df['loc'][i][j][1]) < int(df['loc'][i][k][1]) and int(df['loc'][i][j][1]) > int(df['loc'][i][k][0]):\n",
    "                    #print(i,j,k)\n",
    "                    new_s = intersection(df['annotation'][i][j][0],df['annotation'][i][k][0])\n",
    "                    df['annotation'][i][j] = (new_s,df['annotation'][i][j][1])\n",
    "                    df['loc'][i][j] = (df['loc'][i][j][0],df['loc'][i][k][1])\n",
    "                    df['loc'][i][k] = ('','')\n",
    "                    df['annotation'][i][k] = ('','')\n",
    "                    continue\n",
    "            \n",
    "                elif int(df['loc'][i][j][0]) > int(df['loc'][i][k][0]) and int(df['loc'][i][j][1]) > int(df['loc'][i][k][1]) and int(df['loc'][i][j][0]) < int(df['loc'][i][k][1]):\n",
    "                    #print(i,j,k)\n",
    "                    #the order here matters, the first one should be the left most\n",
    "                    new_s = intersection(df['annotation'][i][k][0],df['annotation'][i][j][0])\n",
    "                    df['annotation'][i][k] = (new_s,df['annotation'][i][k][1])\n",
    "                    df['loc'][i][k] = (df['loc'][i][k][0],df['loc'][i][j][1])\n",
    "                    df['loc'][i][j] = ('','')\n",
    "                    df['annotation'][i][j] = ('','')\n",
    "                    continue\n",
    "\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f9b8f926",
   "metadata": {},
   "source": [
    "# preprocess df and run two pass on rm_dup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e0d6505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "790"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pre_process(train_df)\n",
    "train_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaf8c6c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "790"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_n = rm_dup(train_df)\n",
    "train_n.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f004c49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "790"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_train = train_n.copy()\n",
    "train_n = rm_dup(train_n)\n",
    "train_n.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93d5fe9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "514"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pre_process(test_df)\n",
    "test_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02e8876f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "514"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_n = rm_dup(test_df)\n",
    "test_n.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b263764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "514"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_n = rm_dup(test_n)\n",
    "test_n.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "237e555a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "790"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for j in range(train_n.shape[0]):\n",
    "    train_n['annotation'][j] = [train_n['annotation'][j][i] for i in range(len(train_n['annotation'][j])) if train_n['annotation'][j][i][0] != '']\n",
    "    train_n['loc'][j] = [train_n['loc'][j][i] for i in range(len(train_n['loc'][j])) if train_n['loc'][j][i][0] != '']\n",
    "    start = np.array([int(train_n['loc'][j][i][0]) for i in range(len(train_n['loc'][j]))])\n",
    "    order = start.argsort()\n",
    "    train_n['annotation'][j] = [train_n['annotation'][j][i] for i in order]\n",
    "    train_n['loc'][j] = [train_n['loc'][j][i] for i in order]\n",
    "train = train_n.drop('loc', axis=1)\n",
    "train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffe1f83c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "514"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "for j in range(test_n.shape[0]):\n",
    "    test_n['annotation'][j] = [test_n['annotation'][j][i] for i in range(len(test_n['annotation'][j])) if test_n['annotation'][j][i][0] != '']\n",
    "    test_n['loc'][j] = [test_n['loc'][j][i] for i in range(len(test_n['loc'][j])) if test_n['loc'][j][i][0] != '']\n",
    "    start = np.array([int(test_n['loc'][j][i][0]) for i in range(len(test_n['loc'][j]))])\n",
    "    order = start.argsort()\n",
    "    test_n['annotation'][j] = [test_n['annotation'][j][i] for i in order]\n",
    "    test_n['loc'][j] = [test_n['loc'][j][i] for i in order]\n",
    "test = test_n.drop('loc', axis=1)\n",
    "test.shape[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2af75526",
   "metadata": {},
   "source": [
    "# modify testing text (change coronary arterary disease to CAD ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c374587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words('english')\n",
    "negat = [\"no\", \"nor\", \"not\",\"don't\",\"didn't\",\"doesn't\",\"isn't\",\"aren't\",\"wasn't\",\"weren't\",\"haven't\",\"hasn't\",\"hadn't\",\"won't\",\"wouldn't\",\"shouldn't\",\"can't\",\"couldn't\",\"mustn\",\"mustn't\",\"mightn't\",\"mightn't\",\"needn't\",\"needn't\",\"oughtn't\",\"shan't\",\"shan't\",\"shouldn't\",\"wasn't\",\"weren't\",\"won't\",\"wouldn't\",\"t\",\"shouldn\",\"wasn\",\"weren\",\"won\",\"wouldn\",\"can\",\"couldn\",\"didn\",\"doesn\",\"hadn\",\"hasn\",\"haven\",\"isn\",\"mightn\",\"mustn\",\"needn\",\"oughtn\",\"shan\",\"shouldn\",\"wasn\",\"weren\",\"won\",\"wouldn\"]\n",
    "sw_n = [w for w in sw if w not in negat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "938911bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(test.shape[0]):\n",
    "    word_tokens = test['text'][i].strip().split()\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in sw_n]\n",
    "    test['text'][i] = ' '.join(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a21f974",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['text'] = test['text'].apply(lambda x: re.sub('coronary artery disease','CAD',x))\n",
    "test['text'] = test['text'].apply(lambda x: re.sub('Coronary artery disease','CAD',x))\n",
    "test['text'] = test['text'].apply(lambda x: re.sub('Coronary Artery Disease','CAD',x))\n",
    "test['text'] = test['text'].apply(lambda x: re.sub('Blood Pressure','BP',x))\n",
    "test['text'] = test['text'].apply(lambda x: re.sub('blood pressure','BP',x))\n",
    "test['text'] = test['text'].apply(lambda x: re.sub('Blood pressure','BP',x))\n",
    "test['text'] = test['text'].apply(lambda x: re.sub('blood Pressure','BP',x))\n",
    "test['text'] = test['text'].apply(lambda x: re.sub('&#8211','',x))\n",
    "#test['text'] = test['text'].apply(lambda x: re.sub(' p\\.o\\. ','per oral',x))\n",
    "#test['text'] = test['text'].apply(lambda x: re.sub(' h/o ','had',x))\n",
    "#test['text'] = test['text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbd19792",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(test.shape[0]):\n",
    "    for x in range(len(test['annotation'][i])):\n",
    "        word_tokens = test['annotation'][i][x][0].strip().split()\n",
    "        # converts the words in word_tokens to lower case and then checks whether \n",
    "        #they are present in stop_words or not\n",
    "        filtered_sentence = [w for w in word_tokens if not w.lower() in sw_n]\n",
    "        tagged_things = ' '.join(filtered_sentence)\n",
    "        tagged_things = re.sub('coronary artery disease','CAD',tagged_things)\n",
    "        tagged_things = re.sub('Coronary artery disease','CAD',tagged_things)\n",
    "        tagged_things = re.sub('Coronary Artery Disease','CAD',tagged_things)\n",
    "        tagged_things = re.sub('Blood Pressure','BP',tagged_things)\n",
    "        tagged_things = re.sub('blood pressure','BP',tagged_things)\n",
    "        tagged_things = re.sub('Blood pressure','BP',tagged_things)\n",
    "        tagged_things = re.sub('blood Pressure','BP',tagged_things)\n",
    "        tagged_things = re.sub('&#8211','',tagged_things)\n",
    "        #tagged_things = re.sub(' p\\.o\\. ',' per oral ',tagged_things)\n",
    "        #tagged_things = re.sub(' h/o ','had',tagged_things)\n",
    "        test['annotation'][i][x] = (tagged_things,test['annotation'][i][x][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c79b838d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words in text to match tags\n",
    "for i in range(train.shape[0]):\n",
    "    word_tokens = train['text'][i].strip().split()\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in sw_n]\n",
    "    train['text'][i] = ' '.join(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d08d3e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(lambda x: re.sub('coronary artery disease','CAD',x))\n",
    "train['text'] = train['text'].apply(lambda x: re.sub('Coronary artery disease','CAD',x))\n",
    "train['text'] = train['text'].apply(lambda x: re.sub('Coronary Artery Disease','CAD',x))\n",
    "train['text'] = train['text'].apply(lambda x: re.sub('Blood Pressure','BP',x))\n",
    "train['text'] = train['text'].apply(lambda x: re.sub('blood pressure','BP',x))\n",
    "train['text'] = train['text'].apply(lambda x: re.sub('Blood pressure','BP',x))\n",
    "train['text'] = train['text'].apply(lambda x: re.sub('blood Pressure','BP',x))\n",
    "train['text'] = train['text'].apply(lambda x: re.sub('&#8211','',x))\n",
    "#train['text'] = train['text'].apply(lambda x: re.sub(' p\\.o\\. ',' per oral ',x))\n",
    "#train['text'] = train['text'].apply(lambda x: re.sub(' h/o ',' had ',x))\n",
    "#train['text'] = train['text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7620c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(train.shape[0]):\n",
    "    for x in range(len(train['annotation'][i])):\n",
    "        word_tokens = train['annotation'][i][x][0].strip().split()\n",
    "        # converts the words in word_tokens to lower case and then checks whether \n",
    "        #they are present in stop_words or not\n",
    "        filtered_sentence = [w for w in word_tokens if not w.lower() in sw_n]\n",
    "        tagged_things = ' '.join(filtered_sentence)\n",
    "        tagged_things = re.sub('coronary artery disease','CAD',tagged_things)\n",
    "        tagged_things = re.sub('Coronary artery disease','CAD',tagged_things)\n",
    "        tagged_things = re.sub('Coronary Artery Disease','CAD',tagged_things)\n",
    "        tagged_things = re.sub('Blood Pressure','BP',tagged_things)\n",
    "        tagged_things = re.sub('blood pressure','BP',tagged_things)\n",
    "        tagged_things = re.sub('Blood pressure','BP',tagged_things)\n",
    "        tagged_things = re.sub('blood Pressure','BP',tagged_things)\n",
    "        tagged_things = re.sub('&#8211','',tagged_things)\n",
    "        #tagged_things = re.sub(' p\\.o\\. ',' per oral ',tagged_things)\n",
    "        #tagged_things = re.sub(' h/o ',' had ',tagged_things)\n",
    "        train['annotation'][i][x] = (tagged_things,train['annotation'][i][x][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d5ce72da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "[4]\n",
      "[5]\n",
      "[6]\n",
      "[7]\n",
      "[8]\n",
      "[9]\n",
      "[10]\n",
      "[11]\n",
      "[12]\n",
      "[13]\n",
      "[14]\n",
      "[15]\n",
      "[16]\n",
      "[17]\n",
      "[18]\n",
      "[19]\n",
      "[20]\n",
      "[21]\n",
      "[22]\n",
      "[23]\n",
      "[24]\n",
      "[25]\n",
      "[26]\n",
      "[27]\n",
      "[28]\n",
      "[29]\n",
      "[30]\n",
      "[31]\n",
      "[32]\n",
      "[33]\n",
      "[34]\n",
      "[35]\n",
      "[36]\n",
      "[37]\n",
      "[38]\n",
      "[39]\n",
      "[40]\n",
      "[41]\n",
      "[42]\n",
      "[43]\n",
      "[44]\n",
      "[45]\n",
      "[46]\n",
      "[47]\n",
      "[48]\n",
      "[49]\n",
      "[50]\n",
      "[51]\n",
      "[52]\n",
      "[53]\n",
      "[54]\n",
      "[55]\n",
      "[56]\n",
      "[57]\n",
      "[58]\n",
      "[59]\n",
      "[60]\n",
      "[61]\n",
      "[62]\n",
      "[63]\n",
      "[64]\n",
      "[65]\n",
      "[66]\n",
      "[67]\n",
      "[68]\n",
      "[69]\n",
      "[70]\n",
      "[71]\n",
      "[72]\n",
      "[73]\n",
      "[74]\n",
      "[75]\n",
      "[76]\n",
      "[77]\n",
      "[78]\n",
      "[79]\n",
      "[80]\n",
      "[81]\n",
      "[82]\n",
      "[83]\n",
      "[84]\n",
      "[85]\n",
      "[86]\n",
      "[87]\n",
      "[88]\n",
      "[89]\n",
      "[90]\n",
      "[91]\n",
      "[92]\n",
      "[93]\n",
      "[94]\n",
      "[95]\n",
      "[96]\n",
      "[97]\n",
      "[98]\n",
      "[99]\n",
      "[100]\n",
      "[101]\n",
      "[102]\n",
      "[103]\n",
      "[104]\n",
      "[105]\n",
      "[106]\n",
      "[107]\n",
      "[108]\n",
      "[109]\n",
      "[110]\n",
      "[111]\n",
      "[112]\n",
      "[113]\n",
      "[114]\n",
      "[115]\n",
      "[116]\n",
      "[117]\n",
      "[118]\n",
      "[119]\n",
      "[120]\n",
      "[121]\n",
      "[122]\n",
      "[123]\n",
      "[124]\n",
      "[125]\n",
      "[126]\n",
      "[127]\n",
      "[128]\n",
      "[129]\n",
      "[130]\n",
      "[131]\n",
      "[132]\n",
      "[133]\n",
      "[134]\n",
      "[135]\n",
      "[136]\n",
      "[137]\n",
      "[138]\n",
      "[139]\n",
      "[140]\n",
      "[141]\n",
      "[142]\n",
      "[143]\n",
      "[144]\n",
      "[145]\n",
      "[146]\n",
      "[147]\n",
      "[148]\n",
      "[149]\n",
      "[150]\n",
      "[151]\n",
      "[152]\n",
      "[153]\n",
      "[154]\n",
      "[155]\n",
      "[156]\n",
      "[157]\n",
      "[158]\n",
      "[159]\n",
      "[160]\n",
      "[161]\n",
      "[162]\n",
      "[163]\n",
      "[164]\n",
      "[165]\n",
      "[166]\n",
      "[167]\n",
      "[168]\n",
      "[169]\n",
      "[170]\n",
      "[171]\n",
      "[172]\n",
      "[173]\n",
      "[174]\n",
      "[175]\n",
      "[176]\n",
      "[177]\n",
      "[178]\n",
      "[179]\n",
      "[180]\n",
      "[181]\n",
      "[182]\n",
      "[183]\n",
      "[184]\n",
      "[185]\n",
      "[186]\n",
      "[187]\n",
      "[188]\n",
      "[189]\n",
      "[190]\n",
      "[191]\n",
      "[192]\n",
      "[193]\n",
      "[194]\n",
      "[195]\n",
      "[196]\n",
      "[197]\n",
      "[198]\n",
      "[199]\n",
      "[200]\n",
      "[201]\n",
      "[202]\n",
      "[203]\n",
      "[204]\n",
      "[205]\n",
      "[206]\n",
      "[207]\n",
      "[208]\n",
      "[209]\n",
      "[210]\n",
      "[211]\n",
      "[212]\n",
      "[213]\n",
      "[214]\n",
      "[215]\n",
      "[216]\n",
      "[217]\n",
      "[218]\n",
      "[219]\n",
      "[220]\n",
      "[221]\n",
      "[222]\n",
      "[223]\n",
      "[224]\n",
      "[225]\n",
      "[226]\n",
      "[227]\n",
      "[228]\n",
      "[229]\n",
      "[230]\n",
      "[231]\n",
      "[232]\n",
      "[233]\n",
      "[234]\n",
      "[235]\n",
      "[236]\n",
      "[237]\n",
      "[238]\n",
      "[239]\n",
      "[240]\n",
      "[241]\n",
      "[242]\n",
      "[243]\n",
      "[244]\n",
      "[245]\n",
      "[246]\n",
      "[247]\n",
      "[248]\n",
      "[249]\n",
      "[250]\n",
      "[251]\n",
      "[252]\n",
      "[253]\n",
      "[254]\n",
      "[255]\n",
      "[256]\n",
      "[257]\n",
      "[258]\n",
      "[259]\n",
      "[260]\n",
      "[261]\n",
      "[262]\n",
      "[263]\n",
      "[264]\n",
      "[265]\n",
      "[266]\n",
      "[267]\n",
      "[268]\n",
      "[269]\n",
      "[270]\n",
      "[271]\n",
      "[272]\n",
      "[273]\n",
      "[274]\n",
      "[275]\n",
      "[276]\n",
      "[277]\n",
      "[278]\n",
      "[279]\n",
      "[280]\n",
      "[281]\n",
      "[282]\n",
      "[283]\n",
      "[284]\n",
      "[285]\n",
      "[286]\n",
      "[287]\n",
      "[288]\n",
      "[289]\n",
      "[290]\n",
      "[291]\n",
      "[292]\n",
      "[293]\n",
      "[294]\n",
      "[295]\n",
      "[296]\n",
      "[297]\n",
      "[298]\n",
      "[299]\n",
      "[300]\n",
      "[301]\n",
      "[302]\n",
      "[303]\n",
      "[304]\n",
      "[305]\n",
      "[306]\n",
      "[307]\n",
      "[308]\n",
      "[309]\n",
      "[310]\n",
      "[311]\n",
      "[312]\n",
      "[313]\n",
      "[314]\n",
      "[315]\n",
      "[316]\n",
      "[317]\n",
      "[318]\n",
      "[319]\n",
      "[320]\n",
      "[321]\n",
      "[322]\n",
      "[323]\n",
      "[324]\n",
      "[325]\n",
      "[326]\n",
      "[327]\n",
      "[328]\n",
      "[329]\n",
      "[330]\n",
      "[331]\n",
      "[332]\n",
      "[333]\n",
      "[334]\n",
      "[335]\n",
      "[336]\n",
      "[337]\n",
      "[338]\n",
      "[339]\n",
      "[340]\n",
      "[341]\n",
      "[342]\n",
      "[343]\n",
      "[344]\n",
      "[345]\n",
      "[346]\n",
      "[347]\n",
      "[348]\n",
      "[349]\n",
      "[350]\n",
      "[351]\n",
      "[352]\n",
      "[353]\n",
      "[354]\n",
      "[355]\n",
      "[356]\n",
      "[357]\n",
      "[358]\n",
      "[359]\n",
      "[360]\n",
      "[361]\n",
      "[362]\n",
      "[363]\n",
      "[364]\n",
      "[365]\n",
      "[366]\n",
      "[367]\n",
      "[368]\n",
      "[369]\n",
      "[370]\n",
      "[371]\n",
      "[372]\n",
      "[373]\n",
      "[374]\n",
      "[375]\n",
      "[376]\n",
      "[377]\n",
      "[378]\n",
      "[379]\n",
      "[380]\n",
      "[381]\n",
      "[382]\n",
      "[383]\n",
      "[384]\n",
      "[385]\n",
      "[386]\n",
      "[387]\n",
      "[388]\n",
      "[389]\n",
      "[390]\n",
      "[391]\n",
      "[392]\n",
      "[393]\n",
      "[394]\n",
      "[395]\n",
      "[396]\n",
      "[397]\n",
      "[398]\n",
      "[399]\n",
      "[400]\n",
      "[401]\n",
      "[402]\n",
      "[403]\n",
      "[404]\n",
      "[405]\n",
      "[406]\n",
      "[407]\n",
      "[408]\n",
      "[409]\n",
      "[410]\n",
      "[411]\n",
      "[412]\n",
      "[413]\n",
      "[414]\n",
      "[415]\n",
      "[416]\n",
      "[417]\n",
      "[418]\n",
      "[419]\n",
      "[420]\n",
      "[421]\n",
      "[422]\n",
      "[423]\n",
      "[424]\n",
      "[425]\n",
      "[426]\n",
      "[427]\n",
      "[428]\n",
      "[429]\n",
      "[430]\n",
      "[431]\n",
      "[432]\n",
      "[433]\n",
      "[434]\n",
      "[435]\n",
      "[436]\n",
      "[437]\n",
      "[438]\n",
      "[439]\n",
      "[440]\n",
      "[441]\n",
      "[442]\n",
      "[443]\n",
      "[444]\n",
      "[445]\n",
      "[446]\n",
      "[447]\n",
      "[448]\n",
      "[449]\n",
      "[450]\n",
      "[451]\n",
      "[452]\n",
      "[453]\n",
      "[454]\n",
      "[455]\n",
      "[456]\n",
      "[457]\n",
      "[458]\n",
      "[459]\n",
      "[460]\n",
      "[461]\n",
      "[462]\n",
      "[463]\n",
      "[464]\n",
      "[465]\n",
      "[466]\n",
      "[467]\n",
      "[468]\n",
      "[469]\n",
      "[470]\n",
      "[471]\n",
      "[472]\n",
      "[473]\n",
      "[474]\n",
      "[475]\n",
      "[476]\n",
      "[477]\n",
      "[478]\n",
      "[479]\n",
      "[480]\n",
      "[481]\n",
      "[482]\n",
      "[483]\n",
      "[484]\n",
      "[485]\n",
      "[486]\n",
      "[487]\n",
      "[488]\n",
      "[489]\n",
      "[490]\n",
      "[491]\n",
      "[492]\n",
      "[493]\n",
      "[494]\n",
      "[495]\n",
      "[496]\n",
      "[497]\n",
      "[498]\n",
      "[499]\n",
      "[500]\n",
      "[501]\n",
      "[502]\n",
      "[503]\n",
      "[504]\n",
      "[505]\n",
      "[506]\n",
      "[507]\n",
      "[508]\n",
      "[509]\n",
      "[510]\n",
      "[511]\n",
      "[512]\n",
      "[513]\n",
      "[514]\n",
      "[515]\n",
      "[516]\n",
      "[517]\n",
      "[518]\n",
      "[519]\n",
      "[520]\n",
      "[521]\n",
      "[522]\n",
      "[523]\n",
      "[524]\n",
      "[525]\n",
      "[526]\n",
      "[527]\n",
      "[528]\n",
      "[529]\n",
      "[530]\n",
      "[531]\n",
      "[532]\n",
      "[533]\n",
      "[534]\n",
      "[535]\n",
      "[536]\n",
      "[537]\n",
      "[538]\n",
      "[539]\n",
      "[540]\n",
      "[541]\n",
      "[542]\n",
      "[543]\n",
      "[544]\n",
      "[545]\n",
      "[546]\n",
      "[547]\n",
      "[548]\n",
      "[549]\n",
      "[550]\n",
      "[551]\n",
      "[552]\n",
      "[553]\n",
      "[554]\n",
      "[555]\n",
      "[556]\n",
      "[557]\n",
      "[558]\n",
      "[559]\n",
      "[560]\n",
      "[561]\n",
      "[562]\n",
      "[563]\n",
      "[564]\n",
      "[565]\n",
      "[566]\n",
      "[567]\n",
      "[568]\n",
      "[569]\n",
      "[570]\n",
      "[571]\n",
      "[572]\n",
      "[573]\n",
      "[574]\n",
      "[575]\n",
      "[576]\n",
      "[577]\n",
      "[578]\n",
      "[579]\n",
      "[580]\n",
      "[581]\n",
      "[582]\n",
      "[583]\n",
      "[584]\n",
      "[585]\n",
      "[586]\n",
      "[587]\n",
      "[588]\n",
      "[589]\n",
      "[590]\n",
      "[591]\n",
      "[592]\n",
      "[593]\n",
      "[594]\n",
      "[595]\n",
      "[596]\n",
      "[597]\n",
      "[598]\n",
      "[599]\n",
      "[600]\n",
      "[601]\n",
      "[602]\n",
      "[603]\n",
      "[604]\n",
      "[605]\n",
      "[606]\n",
      "[607]\n",
      "[608]\n",
      "[609]\n",
      "[610]\n",
      "[611]\n",
      "[612]\n",
      "[613]\n",
      "[614]\n",
      "[615]\n",
      "[616]\n",
      "[617]\n",
      "[618]\n",
      "[619]\n",
      "[620]\n",
      "[621]\n",
      "[622]\n",
      "[623]\n",
      "[624]\n",
      "[625]\n",
      "[626]\n",
      "[627]\n",
      "[628]\n",
      "[629]\n",
      "[630]\n",
      "[631]\n",
      "[632]\n",
      "[633]\n",
      "[634]\n",
      "[635]\n",
      "[636]\n",
      "[637]\n",
      "[638]\n",
      "[639]\n",
      "[640]\n",
      "[641]\n",
      "[642]\n",
      "[643]\n",
      "[644]\n",
      "[645]\n",
      "[646]\n",
      "[647]\n",
      "[648]\n",
      "[649]\n",
      "[650]\n",
      "[651]\n",
      "[652]\n",
      "[653]\n",
      "[654]\n",
      "[655]\n",
      "[656]\n",
      "[657]\n",
      "[658]\n",
      "[659]\n",
      "[660]\n",
      "[661]\n",
      "[662]\n",
      "[663]\n",
      "[664]\n",
      "[665]\n",
      "[666]\n",
      "[667]\n",
      "[668]\n",
      "[669]\n",
      "[670]\n",
      "[671]\n",
      "[672]\n",
      "[673]\n",
      "[674]\n",
      "[675]\n",
      "[676]\n",
      "[677]\n",
      "[678]\n",
      "[679]\n",
      "[680]\n",
      "[681]\n",
      "[682]\n",
      "[683]\n",
      "[684]\n",
      "[685]\n",
      "[686]\n",
      "[687]\n",
      "[688]\n",
      "[689]\n",
      "[690]\n",
      "[691]\n",
      "[692]\n",
      "[693]\n",
      "[694]\n",
      "[695]\n",
      "[696]\n",
      "[697]\n",
      "[698]\n",
      "[699]\n",
      "[700]\n",
      "[701]\n",
      "[702]\n",
      "[703]\n",
      "[704]\n",
      "[705]\n",
      "[706]\n",
      "[707]\n",
      "[708]\n",
      "[709]\n",
      "[710]\n",
      "[711]\n",
      "[712]\n",
      "[713]\n",
      "[714]\n",
      "[715]\n",
      "[716]\n",
      "[717]\n",
      "[718]\n",
      "[719]\n",
      "[720]\n",
      "[721]\n",
      "[722]\n",
      "[723]\n",
      "[724]\n",
      "[725]\n",
      "[726]\n",
      "[727]\n",
      "[728]\n",
      "[729]\n",
      "[730]\n",
      "[731]\n",
      "[732]\n",
      "[733]\n",
      "[734]\n",
      "[735]\n",
      "[736]\n",
      "[737]\n",
      "[738]\n",
      "[739]\n",
      "[740]\n",
      "[741]\n",
      "[742]\n",
      "[743]\n",
      "[744]\n",
      "[745]\n",
      "[746]\n",
      "[747]\n",
      "[748]\n",
      "[749]\n",
      "[750]\n",
      "[751]\n",
      "[752]\n",
      "[753]\n",
      "[754]\n",
      "[755]\n",
      "[756]\n",
      "[757]\n",
      "[758]\n",
      "[759]\n",
      "[760]\n",
      "[761]\n",
      "[762]\n",
      "[763]\n",
      "[764]\n",
      "[765]\n",
      "[766]\n",
      "[767]\n",
      "[768]\n",
      "[769]\n",
      "[770]\n",
      "[771]\n",
      "[772]\n",
      "[773]\n",
      "[774]\n",
      "[775]\n",
      "[776]\n",
      "[777]\n",
      "[778]\n",
      "[779]\n",
      "[780]\n",
      "[781]\n",
      "[782]\n",
      "[783]\n",
      "[784]\n",
      "[785]\n",
      "[786]\n",
      "[787]\n",
      "[788]\n",
      "[789]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from difflib import SequenceMatcher\n",
    "import pickle\n",
    "\n",
    "def matcher(string, pattern):\n",
    "    '''\n",
    "    Return the start and end index of any pattern present in the text.\n",
    "    '''\n",
    "    match_list = []\n",
    "    pattern = pattern.strip()\n",
    "    seqMatch = SequenceMatcher(None, string, pattern, autojunk=False)\n",
    "    match = seqMatch.find_longest_match(0, len(string), 0, len(pattern))\n",
    "    if (match.size == len(pattern)):\n",
    "        start = match.a\n",
    "        end = match.a + match.size\n",
    "        match_tup = (start, end)\n",
    "        string = string.replace(pattern, \"X\" * len(pattern), 1)\n",
    "        match_list.append(match_tup)\n",
    "\n",
    "    return match_list, string\n",
    "\n",
    "\n",
    "def create_labs(s,match_list):\n",
    "    labs = ['O' for i in range(len(s.split()))]\n",
    "    word_dict = pd.DataFrame({'word':s.split(),'label':labs})\n",
    "\n",
    "    for start, end, e_type in match_list:\n",
    "        index = len(re.findall(r' +',s[0:start]))\n",
    "        num_words = len(s[start:end].split())\n",
    "\n",
    "        if num_words > 1:\n",
    "            word_dict.loc[index,'label'] = e_type\n",
    "            for i in range(1,num_words):\n",
    "                word_dict.loc[index+i,'label'] = e_type\n",
    "        else:\n",
    "            word_dict.loc[index,'label'] = e_type\n",
    "    return word_dict\n",
    "\n",
    "\n",
    "def to_txt(df, filepath):\n",
    "    '''\n",
    "    The function responsible for the creation of data in the said format.\n",
    "    '''\n",
    "    with open(filepath , 'w') as f:\n",
    "        for text, annotation in zip(df.text, df.annotation):\n",
    "            text_ = text    \n",
    "            print(df.index[df['text']== text_].tolist())    \n",
    "            match_list = []\n",
    "            for i in annotation:\n",
    "                a,text_= matcher(text_, i[0])\n",
    "                match_list.append((a[0][0], a[0][1], i[1]))\n",
    "\n",
    "            d = create_labs(text, match_list)\n",
    "\n",
    "            for i in range(d.shape[0]):\n",
    "                f.writelines(d['word'][i] + ' ' + d['label'][i] +'\\n')\n",
    "            f.writelines('\\n')\n",
    "            \n",
    "def main(input,save_path):\n",
    "\n",
    "    data = input\n",
    "    to_txt(data, save_path)\n",
    "    \n",
    "#if __name__ == '__main__':\n",
    "path = 'C:/Users/Leste/OneDrive - Johns Hopkins/Documents/GitHub/nlpsumm/BERT/data_processed/'\n",
    "\n",
    "#main(train,path+'train.txt')\n",
    "main(train,path+'train(clean).txt')\n",
    "#main(test,path+'test(clean).txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8537d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['annotation'][0]\n",
    "test_df['text'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8cd60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(train_n['annotation'][[266]].tolist()), print(train_n['loc'][[266]].tolist())\n",
    "\n",
    "names_3[382]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d74fa0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('BP 170/80.', 'HYPERTENSION')\n",
      "('HCTZ', 'MEDICATION')\n",
      "('Hypertension', 'HYPERTENSION')\n",
      "('hyperlipidemia', 'HYPERLIPIDEMIA')\n",
      "('HTN', 'HYPERTENSION')\n",
      "('known hx CAD', 'CAD')\n",
      "('CAD', 'CAD')\n",
      "('s/p ant SEMI + stent LAD 2/67, Dr Oakley', 'CAD')\n",
      "('NORVASC (AMLODIPINE)', 'MEDICATION')\n",
      "('PLAVIX (CLOPIDOGREL)', 'MEDICATION')\n",
      "('ATENOLOL', 'MEDICATION')\n",
      "('ASA (ACETYLSALICYLIC ACID)', 'MEDICATION')\n",
      "('ZESTRIL (LISINOPRIL)', 'MEDICATION')\n",
      "('LIPITOR (ATORVASTATIN)', 'MEDICATION')\n",
      "('HCTZ (HYDROCHLOROTHIAZIDE)', 'MEDICATION')\n",
      "('NITROGLYCERIN 1/150 (0.4 MG)', 'MEDICATION')\n",
      "('150/70 repeat 145/80', 'HYPERTENSION')\n",
      "('CAD', 'CAD')\n"
     ]
    }
   ],
   "source": [
    "df = train.copy()\n",
    "annotation = df.annotation[0]\n",
    "text_ = df.text[0]\n",
    "\n",
    "match_list = []\n",
    "for i in annotation:\n",
    "    print(i)\n",
    "    a, text_ = matcher(text_, i[0])\n",
    "    match_list.append((a[0][0], a[0][1], i[1]))\n",
    "    d = create_labs(text_, match_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f09cb6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(104, 114, 'HYPERTENSION'),\n",
       " (135, 139, 'MEDICATION'),\n",
       " (488, 500, 'HYPERTENSION'),\n",
       " (535, 549, 'HYPERLIPIDEMIA'),\n",
       " (571, 574, 'HYPERTENSION'),\n",
       " (580, 592, 'CAD'),\n",
       " (682, 685, 'CAD'),\n",
       " (686, 726, 'CAD'),\n",
       " (777, 797, 'MEDICATION'),\n",
       " (820, 840, 'MEDICATION'),\n",
       " (853, 861, 'MEDICATION'),\n",
       " (885, 911, 'MEDICATION'),\n",
       " (936, 956, 'MEDICATION'),\n",
       " (980, 1002, 'MEDICATION'),\n",
       " (1026, 1052, 'MEDICATION'),\n",
       " (1076, 1104, 'MEDICATION'),\n",
       " (1196, 1216, 'HYPERTENSION'),\n",
       " (1388, 1391, 'CAD')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6e8d952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'known hx CAD'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text[0][580:592]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('dataweek2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4600a15e5b6b646e3db8b2d8ebb346444f23fb424fa93d368313be03df29f350"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
