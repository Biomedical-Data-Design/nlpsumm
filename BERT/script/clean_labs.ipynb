{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fae50b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "\n",
    "file_path_1 = \"C:/Users/Leste/OneDrive - Johns Hopkins/Desktop/BDD data/extracted/training-RiskFactors-Complete-Set1/\"\n",
    "file_path_2 = \"C:/Users/Leste/OneDrive - Johns Hopkins/Desktop/BDD data/extracted/training-RiskFactors-Complete-Set2/\"\n",
    "file_path_3 = \"C:/Users/Leste/OneDrive - Johns Hopkins/Desktop/BDD data/extracted/testing-RiskFactors-Complete/\"\n",
    "names_1 = [f for f in listdir(file_path_1) if f.endswith('.xml')]\n",
    "names_2 = [f for f in listdir(file_path_2) if f.endswith('.xml')]\n",
    "names_3 = [f for f in listdir(file_path_3) if f.endswith('.xml')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5531c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "all1_df = pd.DataFrame(np.zeros((len(names_1), 3), dtype=object), columns=['text', 'annotation','loc'])\n",
    "all2_df = pd.DataFrame(np.zeros((len(names_2), 3), dtype=object), columns=['text', 'annotation','loc'])\n",
    "test_df = pd.DataFrame(np.zeros((len(names_3), 3), dtype=object), columns=['text', 'annotation','loc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce1caa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "def to_df(names,df, file_path,PHI_status = True):\n",
    "    #get text\n",
    "    n = 0\n",
    "    for name in names:\n",
    "        tree = ET.parse(file_path + name)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        ## Get the text\n",
    "        nt = re.sub('\\n',' ',root[0].text)\n",
    "        nt = re.sub('\\t',' ',nt) \n",
    "        nt = re.sub('\"',\"'\",nt)\n",
    "        ## sample 214 has a weird character\n",
    "        nt = re.sub('>','&gt;',nt) \n",
    "        nt = re.sub('<','&lt;',nt)\n",
    "        ## new wired character\n",
    "        nt = re.sub('Â','',nt)\n",
    "        nt = re.sub('â','',nt)\n",
    "        nt = re.sub('€','',nt)\n",
    "        nt = re.sub('™','',nt)\n",
    "        df['text'][n] = nt\n",
    "        n+=1\n",
    "    \n",
    "    #get annotations\n",
    "    n = 0\n",
    "    for name in names:\n",
    "        tree = ET.parse(file_path + name)\n",
    "        root = tree.getroot()\n",
    "        ## Get the labels\n",
    "\n",
    "        tag_list = []\n",
    "        loc_list = []\n",
    "        # get PHI labels if there are any\n",
    "        if PHI_status == True:\n",
    "            PHI = [root[1][x].tag for x in range(len(root[1]))].index('PHI')\n",
    "            for k in range(PHI,len(root[1])):\n",
    "                tag_list.append((root[1][k].attrib['text'],root[1][k].tag))\n",
    "        \n",
    "        # get the rest of labels\n",
    "        for k in range(len(root[1])):\n",
    "            for m in range(len(root[1][k])):\n",
    "                if root[1][k][m].attrib.keys().__contains__('text') == False:\n",
    "                    continue\n",
    "                if root[1][k][m].attrib['text'] == 'Record':\n",
    "                    continue\n",
    "                tag_list.append((root[1][k][m].attrib['text'],root[1][k][m].tag))\n",
    "                loc_list.append((root[1][k][m].attrib['start'],root[1][k][m].attrib['end']))\n",
    "        df['annotation'][n] = tag_list\n",
    "        df['loc'][n] = loc_list\n",
    "        n+=1\n",
    "    return df\n",
    "\n",
    "all_1 = to_df(names_1,all1_df, file_path_1, PHI_status = False)\n",
    "all_2 = to_df(names_2,all2_df, file_path_2, PHI_status = False)\n",
    "test_df = to_df(names_3,test_df, file_path_3, PHI_status = False)\n",
    "train_df = pd.concat([all_1, all_2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2830bba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(ls1, ls2):\n",
    "    index = [i for i, x in enumerate([x == ls2[0] for x in ls1]) if x]\n",
    "    cut = 1000\n",
    "    for i in index:\n",
    "        eva = sum([ls1[x+i] != ls2[x] for x in range(len(ls1)-i)])\n",
    "        if eva < cut:\n",
    "            cut = i\n",
    "    out = \"\".join(ls1[0:cut]+ls2)\n",
    "    return out\n",
    "\n",
    "def rm_dup(df):\n",
    "    symbol = [\",\",\".\",\"-\"]\n",
    "    for i in range(df.shape[0]):\n",
    "\n",
    "        for j in range(len(df['annotation'][i])):\n",
    "\n",
    "            for k in np.arange(j+1,len(df['annotation'][i])):\n",
    "                # move on if the compared lables are (\"\",\"\")\n",
    "                if df['loc'][i][j] == ('',''):\n",
    "                    break\n",
    "                if df['loc'][i][k] == ('',''):\n",
    "                    continue\n",
    "\n",
    "                ## preprocess tagged text and location\n",
    "                # remove extra spaces in the beginning and end of the annotation\n",
    "                if re.search(\"^ +.*\",df['annotation'][i][j][0]) != None or re.search(\".* +$\",df['annotation'][i][j][0]) != None:\n",
    "                    front = len(df['annotation'][i][j][0]) - len(re.sub(\"^ +\",\"\",df['annotation'][i][j][0]))\n",
    "                    end = len(df['annotation'][i][j][0]) - len(re.sub(\" +$\",\"\",df['annotation'][i][j][0]))\n",
    "                    df['loc'][i][j] = (str(int(df['loc'][i][j][0])+front),str(int(df['loc'][i][j][1])-end))\n",
    "                    df['annotation'][i][j] = (re.sub(\" +$\",\"\", re.sub(\"^ +\",\"\",df['annotation'][i][j][0])),df['annotation'][i][j][1])\n",
    "\n",
    "                if re.search(\"^ +.*\",df['annotation'][i][k][0]) != None or re.search(\".* +$\",df['annotation'][i][k][0]) != None:\n",
    "                    front = len(df['annotation'][i][k][0]) - len(re.sub(\"^ +\",\"\",df['annotation'][i][k][0]))\n",
    "                    end = len(df['annotation'][i][k][0]) - len(re.sub(\" +$\",\"\",df['annotation'][i][k][0]))\n",
    "                    df['loc'][i][k] = (str(int(df['loc'][i][k][0])+front),str(int(df['loc'][i][k][1])-end))\n",
    "                    df['annotation'][i][k] = (re.sub(\" +$\",\"\", re.sub(\"^ +\",\"\",df['annotation'][i][k][0])),df['annotation'][i][k][1])\n",
    "\n",
    "                # find location contained within each other\n",
    "                if int(df['loc'][i][j][0]) >= int(df['loc'][i][k][0]) and int(df['loc'][i][j][1]) <= int(df['loc'][i][k][1]):\n",
    "                    df['loc'][i][j] = ('','')\n",
    "                    df['annotation'][i][j] = ('','')\n",
    "                    continue\n",
    "                \n",
    "                elif int(df['loc'][i][j][0]) <= int(df['loc'][i][k][0]) and int(df['loc'][i][j][1]) >= int(df['loc'][i][k][1]):\n",
    "                    df['loc'][i][k] = ('','')\n",
    "                    df['annotation'][i][k] = ('','')\n",
    "                    continue\n",
    "\n",
    "                # find location that overlap\n",
    "                if int(df['loc'][i][j][0]) < int(df['loc'][i][k][0]) and int(df['loc'][i][j][1]) < int(df['loc'][i][k][1]) and int(df['loc'][i][j][1]) > int(df['loc'][i][k][0]):\n",
    "                    #print(i,j,k)\n",
    "                    new_s = intersection(df['annotation'][i][j][0].split(),df['annotation'][i][k][0].split())\n",
    "                    df['annotation'][i][j] = (new_s,df['annotation'][i][j][1])\n",
    "                    df['loc'][i][j] = (df['loc'][i][j][0],df['loc'][i][k][1])\n",
    "                    df['loc'][i][k] = ('','')\n",
    "                    df['annotation'][i][k] = ('','')\n",
    "                    continue\n",
    "            \n",
    "                elif int(df['loc'][i][j][0]) > int(df['loc'][i][k][0]) and int(df['loc'][i][j][1]) > int(df['loc'][i][k][1]) and int(df['loc'][i][j][0]) < int(df['loc'][i][k][1]):\n",
    "                    #print(i,j,k)\n",
    "                    #the order here matters, the first one should be the left most\n",
    "                    new_s = intersection(df['annotation'][i][k][0].split(),df['annotation'][i][j][0].split())\n",
    "                    df['annotation'][i][k] = (new_s,df['annotation'][i][k][1])\n",
    "                    df['loc'][i][k] = (df['loc'][i][k][0],df['loc'][i][j][1])\n",
    "                    df['loc'][i][j] = ('','')\n",
    "                    df['annotation'][i][j] = ('','')\n",
    "                    continue\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61142c56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'apple is red and green'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = \"apple is red\"\n",
    "l2 = \"apple is red and green\"\n",
    "intersection(l1,l2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f9b8f926",
   "metadata": {},
   "source": [
    "# old code used to clean text, replaced by merge two sentence together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "501fe9d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Leste\\OneDrive - Johns Hopkins\\Documents\\GitHub\\nlpsumm\\BERT\\script\\clean_labs.ipynb Cell 7\u001b[0m in \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Leste/OneDrive%20-%20Johns%20Hopkins/Documents/GitHub/nlpsumm/BERT/script/clean_labs.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m## preprocess tagged text and location\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Leste/OneDrive%20-%20Johns%20Hopkins/Documents/GitHub/nlpsumm/BERT/script/clean_labs.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# remove extra spaces in the beginning and end of the annotation\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Leste/OneDrive%20-%20Johns%20Hopkins/Documents/GitHub/nlpsumm/BERT/script/clean_labs.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mif\u001b[39;00m re\u001b[39m.\u001b[39msearch(\u001b[39m\"\u001b[39m\u001b[39m^ +.*\u001b[39m\u001b[39m\"\u001b[39m,df[\u001b[39m'\u001b[39m\u001b[39mannotation\u001b[39m\u001b[39m'\u001b[39m][i][j][\u001b[39m0\u001b[39m]) \u001b[39m!=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m re\u001b[39m.\u001b[39msearch(\u001b[39m\"\u001b[39m\u001b[39m.* +$\u001b[39m\u001b[39m\"\u001b[39m,df[\u001b[39m'\u001b[39m\u001b[39mannotation\u001b[39m\u001b[39m'\u001b[39m][i][j][\u001b[39m0\u001b[39m]) \u001b[39m!=\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Leste/OneDrive%20-%20Johns%20Hopkins/Documents/GitHub/nlpsumm/BERT/script/clean_labs.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     front \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(df[\u001b[39m'\u001b[39m\u001b[39mannotation\u001b[39m\u001b[39m'\u001b[39m][i][j][\u001b[39m0\u001b[39m]) \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(re\u001b[39m.\u001b[39msub(\u001b[39m\"\u001b[39m\u001b[39m^ +\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,df[\u001b[39m'\u001b[39m\u001b[39mannotation\u001b[39m\u001b[39m'\u001b[39m][i][j][\u001b[39m0\u001b[39m]))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Leste/OneDrive%20-%20Johns%20Hopkins/Documents/GitHub/nlpsumm/BERT/script/clean_labs.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     end \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(df[\u001b[39m'\u001b[39m\u001b[39mannotation\u001b[39m\u001b[39m'\u001b[39m][i][j][\u001b[39m0\u001b[39m]) \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(re\u001b[39m.\u001b[39msub(\u001b[39m\"\u001b[39m\u001b[39m +$\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,df[\u001b[39m'\u001b[39m\u001b[39mannotation\u001b[39m\u001b[39m'\u001b[39m][i][j][\u001b[39m0\u001b[39m]))\n",
      "\u001b[1;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "                ## preprocess tagged text and location\n",
    "                # remove extra spaces in the beginning and end of the annotation\n",
    "                if re.search(\"^ +.*\",df['annotation'][i][j][0]) != None or re.search(\".* +$\",df['annotation'][i][j][0]) != None:\n",
    "                    front = len(df['annotation'][i][j][0]) - len(re.sub(\"^ +\",\"\",df['annotation'][i][j][0]))\n",
    "                    end = len(df['annotation'][i][j][0]) - len(re.sub(\" +$\",\"\",df['annotation'][i][j][0]))\n",
    "                    df['loc'][i][j] = (str(int(df['loc'][i][j][0])+front),str(int(df['loc'][i][j][1])-end))\n",
    "                    df['annotation'][i][j] = (re.sub(\" +$\",\"\", re.sub(\"^ +\",\"\",df['annotation'][i][j][0])),df['annotation'][i][j][1])\n",
    "\n",
    "                if re.search(\"^ +.*\",df['annotation'][i][k][0]) != None or re.search(\".* +$\",df['annotation'][i][k][0]) != None:\n",
    "                    front = len(df['annotation'][i][k][0]) - len(re.sub(\"^ +\",\"\",df['annotation'][i][k][0]))\n",
    "                    end = len(df['annotation'][i][k][0]) - len(re.sub(\" +$\",\"\",df['annotation'][i][k][0]))\n",
    "                    df['loc'][i][k] = (str(int(df['loc'][i][k][0])+front),str(int(df['loc'][i][k][1])-end))\n",
    "                    df['annotation'][i][k] = (re.sub(\" +$\",\"\", re.sub(\"^ +\",\"\",df['annotation'][i][k][0])),df['annotation'][i][k][1])\n",
    "\n",
    "                # remove extra punctuation in the end of the annotation\n",
    "                if sum([df['annotation'][i][j][0][-1] == f for f in symbol]) > 0:\n",
    "                    df['annotation'][i][j] = (df['annotation'][i][j][0][:-1],df['annotation'][i][j][1])\n",
    "                    df['loc'][i][j] = (df['loc'][i][j][0],str(int(df['loc'][i][j][1])-1))\n",
    "                    \n",
    "                if sum([df['annotation'][i][j][0][-1] == f for f in symbol]) > 0:\n",
    "                    df['annotation'][i][k] = (df['annotation'][i][k][0][:-1],df['annotation'][i][k][1])\n",
    "                    df['loc'][i][k] = (df['loc'][i][k][0],str(int(df['loc'][i][k][1])-1))\n",
    "\n",
    "                if sum([df['annotation'][i][j][0][0] == f for f in symbol]) > 0:\n",
    "                    df['annotation'][i][j] = (df['annotation'][i][j][0][1:],df['annotation'][i][j][1])\n",
    "                    df['loc'][i][j] = (str(int(df['loc'][i][j][0])+1),df['loc'][i][j][1])\n",
    "            \n",
    "                if sum([df['annotation'][i][k][0][0] == f for f in symbol]) > 0:\n",
    "                    df['annotation'][i][k] = (df['annotation'][i][k][0][1:],df['annotation'][i][k][1])\n",
    "                    df['loc'][i][k] = (str(int(df['loc'][i][k][0])+1),df['loc'][i][k][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e0d6505",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_n = rm_dup(train_df)\n",
    "#test_n = rm_dup(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237e555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(train_n.shape[0]):\n",
    "    train_n['annotation'][j] = [train_n['annotation'][j][i] for i in range(len(train_n['annotation'][j])) if train_n['annotation'][j][i][0] != '']\n",
    "    train_n['loc'][j] = [train_n['loc'][j][i] for i in range(len(train_n['loc'][j])) if train_n['loc'][j][i][0] != '']\n",
    "    start = np.array([int(train_n['loc'][j][i][0]) for i in range(len(train_n['loc'][j]))])\n",
    "    order = start.argsort()\n",
    "    train_n['annotation'][j] = [train_n['annotation'][j][i] for i in order]\n",
    "    train_n['loc'][j] = [train_n['loc'][j][i] for i in order]\n",
    "train = train_n.drop('loc', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe1f83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for j in range(test_n.shape[0]):\n",
    "    test_n['annotation'][j] = [test_n['annotation'][j][i] for i in range(len(test_n['annotation'][j])) if test_n['annotation'][j][i][0] != '']\n",
    "    test_n['loc'][j] = [test_n['loc'][j][i] for i in range(len(test_n['loc'][j])) if test_n['loc'][j][i][0] != '']\n",
    "    start = np.array([int(test_n['loc'][j][i][0]) for i in range(len(test_n['loc'][j]))])\n",
    "    order = start.argsort()\n",
    "    test_n['annotation'][j] = [test_n['annotation'][j][i] for i in order]\n",
    "    test_n['loc'][j] = [test_n['loc'][j][i] for i in order]\n",
    "test = test_n.drop('loc', axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2af75526",
   "metadata": {},
   "source": [
    "# modify testing text (change coronary arterary disease to CAD ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a21f974",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['text'] = test['text'].apply(lambda x: re.sub('coronary artery disease','cad',x))\n",
    "test['text'] = test['text'].apply(lambda x: re.sub('Blood Pressure','bp',x))\n",
    "test['text'] = test['text'].apply(lambda x: re.sub('blood pressure','bp',x))\n",
    "test['text'] = test['text'].apply(lambda x: re.sub('Blood pressure','bp',x))\n",
    "test['text'] = test['text'].apply(lambda x: re.sub('blood Pressure','bp',x))\n",
    "test['text'] = test['text'].apply(lambda x: re.sub('Blood pressure','bp',x))\n",
    "test['text'] = test['text'].apply(lambda x: re.sub('&#8211','',x))\n",
    "#test['text'] = test['text'].apply(lambda x: re.sub(' p\\.o\\. ','per oral',x))\n",
    "#test['text'] = test['text'].apply(lambda x: re.sub(' h/o ','had',x))\n",
    "#test['text'] = test['text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735efd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "## word_tokenize() seperate (lisinopril) into (, lis..., )\n",
    "word_tokens = train['annotation'][0][2][0].strip().split()\n",
    "filtered_sentence = [w.lower() for w in word_tokens if not w.lower() in sw]\n",
    "filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98c74bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words('english')\n",
    "sw.remove(\"no\", \"nor\", \"not\",\"don't\",\"didn't\",\"doesn't\",\"isn't\",\"aren't\",\"wasn't\",\"weren't\",\"haven't\",\"hasn't\",\"hadn't\",\"won't\",\"wouldn't\",\"shouldn't\",\"can't\",\"couldn't\",\"mustn\",\"mustn't\",\"mightn't\",\"mightn't\",\"needn't\",\"needn't\",\"oughtn't\",\"shan't\",\"shan't\",\"shouldn't\",\"wasn't\",\"weren't\",\"won't\",\"wouldn't\",\"t\")\n",
    "for i in range(test.shape[0]):\n",
    "    for x in range(len(test['annotation'][i])):\n",
    "        word_tokens = test['annotation'][i][x][0].strip().split()\n",
    "        # converts the words in word_tokens to lower case and then checks whether \n",
    "        #they are present in stop_words or not\n",
    "        filtered_sentence = [w.lower() for w in word_tokens if not w.lower() in sw]\n",
    "        tagged_things = ' '.join(filtered_sentence)\n",
    "        tagged_things = re.sub('coronary artery disease','cad',tagged_things)\n",
    "        tagged_things = re.sub('Blood Pressure','bp',tagged_things)\n",
    "        tagged_things = re.sub('blood pressure','bp',tagged_things)\n",
    "        tagged_things = re.sub('Blood pressure','bp',tagged_things)\n",
    "        tagged_things = re.sub('blood Pressure','bp',tagged_things)\n",
    "        tagged_things = re.sub('&#8211','',tagged_things)\n",
    "        #tagged_things = re.sub(' p\\.o\\. ',' per oral ',tagged_things)\n",
    "        #tagged_things = re.sub(' h/o ','had',tagged_things)\n",
    "        test['annotation'][i][x] = (tagged_things,test['annotation'][i][x][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08d3e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(lambda x: re.sub('coronary artery disease','cad',x))\n",
    "train['text'] = train['text'].apply(lambda x: re.sub('Blood Pressure','bp',x))\n",
    "train['text'] = train['text'].apply(lambda x: re.sub('blood pressure','bp',x))\n",
    "train['text'] = train['text'].apply(lambda x: re.sub('Blood pressure','bp',x))\n",
    "train['text'] = train['text'].apply(lambda x: re.sub('blood Pressure','bp',x))\n",
    "train['text'] = train['text'].apply(lambda x: re.sub('Blood pressure','bp',x))\n",
    "train['text'] = train['text'].apply(lambda x: re.sub('&#8211','',x))\n",
    "#train['text'] = train['text'].apply(lambda x: re.sub(' p\\.o\\. ',' per oral ',x))\n",
    "#train['text'] = train['text'].apply(lambda x: re.sub(' h/o ',' had ',x))\n",
    "#train['text'] = train['text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7620c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(train.shape[0]):\n",
    "    for x in range(len(train['annotation'][i])):\n",
    "        word_tokens = train['annotation'][i][x][0].strip().split()\n",
    "        # converts the words in word_tokens to lower case and then checks whether \n",
    "        #they are present in stop_words or not\n",
    "        filtered_sentence = [w.lower() for w in word_tokens if not w.lower() in sw]\n",
    "        tagged_things = ' '.join(filtered_sentence)\n",
    "        tagged_things = re.sub('coronary artery disease','cad',tagged_things)\n",
    "        tagged_things = re.sub('Blood Pressure','bp',tagged_things)\n",
    "        tagged_things = re.sub('blood pressure','bp',tagged_things)\n",
    "        tagged_things = re.sub('Blood pressure','bp',tagged_things)\n",
    "        tagged_things = re.sub('blood Pressure','bp',tagged_things)\n",
    "        tagged_things = re.sub('&#8211','',tagged_things)\n",
    "        #tagged_things = re.sub(' p\\.o\\. ',' per oral ',tagged_things)\n",
    "        #tagged_things = re.sub(' h/o ',' had ',tagged_things)\n",
    "        train['annotation'][i][x] = (tagged_things,train['annotation'][i][x][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb100f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words in text to match tags\n",
    "for i in range(train.shape[0]):\n",
    "    word_tokens = train['text'][i].strip().split()\n",
    "    filtered_sentence = [w.lower() for w in word_tokens if not w.lower() in sw]\n",
    "    train['text'][i] = ' '.join(filtered_sentence)\n",
    "\n",
    "for i in range(test.shape[0]):\n",
    "    word_tokens = train['text'][i].strip().split()\n",
    "    filtered_sentence = [w.lower() for w in word_tokens if not w.lower() in sw]\n",
    "    test['text'][i] = ' '.join(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bbeea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sw)\n",
    "sw.remove(\"no\", \"nor\", \"not\",\"don't\",\"didn't\",\"doesn't\",\"isn't\",\"aren't\",\"wasn't\",\"weren't\",\"haven't\",\"hasn't\",\"hadn't\",\"won't\",\"wouldn't\",\"shouldn't\",\"can't\",\"couldn't\",\"mustn\",\"mustn't\",\"mightn't\",\"mightn't\",\"needn't\",\"needn't\",\"oughtn't\",\"shan't\",\"shan't\",\"shouldn't\",\"wasn't\",\"weren't\",\"won't\",\"wouldn't\",\"t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ce72da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from difflib import SequenceMatcher\n",
    "import pickle\n",
    "\n",
    "def matcher(string, pattern):\n",
    "    '''\n",
    "    Return the start and end index of any pattern present in the text.\n",
    "    '''\n",
    "    match_list = []\n",
    "    pattern = pattern.strip()\n",
    "    seqMatch = SequenceMatcher(None, string, pattern, autojunk=False)\n",
    "    match = seqMatch.find_longest_match(0, len(string), 0, len(pattern))\n",
    "    if (match.size == len(pattern)):\n",
    "        start = match.a\n",
    "        end = match.a + match.size\n",
    "        match_tup = (start, end)\n",
    "        string = string.replace(pattern, \"X\" * len(pattern), 1)\n",
    "        match_list.append(match_tup)\n",
    "\n",
    "    return match_list, string\n",
    "\n",
    "\n",
    "def create_labs(s,match_list):\n",
    "    labs = ['O' for i in range(len(s.split()))]\n",
    "    word_dict = pd.DataFrame({'word':s.split(),'label':labs})\n",
    "\n",
    "    for start, end, e_type in match_list:\n",
    "        index = len(re.findall(r' +',s[0:start]))-1\n",
    "        num_words = len(s[start:end].split())\n",
    "\n",
    "        if num_words > 1:\n",
    "            word_dict.loc[index,'label'] = e_type\n",
    "            for i in range(1,num_words):\n",
    "                word_dict.loc[index+i,'label'] = e_type\n",
    "        else:\n",
    "            word_dict.loc[index,'label'] = e_type\n",
    "    return word_dict\n",
    "\n",
    "def clean(text):\n",
    "    '''\n",
    "    Just a helper fuction to add a space before the punctuations for better tokenization\n",
    "    '''\n",
    "    filters = [\"!\", \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"/\", \"*\", \".\", \":\", \";\", \"<\", \"=\", \">\", \"?\", \"@\", \"[\",\n",
    "               \"\\\\\", \"]\", \"_\", \"`\", \"{\", \"}\", \"~\", \"'\"]\n",
    "    for i in text:\n",
    "        if i in filters:\n",
    "            text = text.replace(i, \" \" + i)\n",
    "            \n",
    "    return text\n",
    "\n",
    "def to_txt(df, filepath):\n",
    "    '''\n",
    "    The function responsible for the creation of data in the said format.\n",
    "    '''\n",
    "    with open(filepath , 'w') as f:\n",
    "        for text, annotation in zip(df.text, df.annotation):\n",
    "            #text = clean(text)\n",
    "            text_ = text    \n",
    "            print(df.index[df['text']== text_].tolist())    \n",
    "            match_list = []\n",
    "            for i in annotation:\n",
    "                a,text_= matcher(text_, i[0])\n",
    "                match_list.append((a[0][0], a[0][1], i[1]))\n",
    "\n",
    "            d = create_labs(text_, match_list)\n",
    "\n",
    "            for i in range(d.shape[0]):\n",
    "                f.writelines(d['word'][i] + ' ' + d['label'][i] +'\\n')\n",
    "            f.writelines('\\n')\n",
    "            \n",
    "def main(input,save_path):\n",
    "\n",
    "    data = input\n",
    "    to_txt(data, save_path)\n",
    "    \n",
    "#if __name__ == '__main__':\n",
    "path = 'C:/Users/Leste/OneDrive - Johns Hopkins/Documents/GitHub/nlpsumm/BERT/data_processed/'\n",
    "\n",
    "#main(train,path+'train.txt')\n",
    "main(train,path+'train.txt')\n",
    "#main(test,path+'test_clean(no_BIO).txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8cd60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_n['annotation'][[26]].tolist())\n",
    "print(train_n['loc'][[26]].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114ec6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"repeat cath in 12/94 which revealed in stent restenosis of prior LAD stent\"\n",
    "b = \"repeat cath in 12/94 which revealed in stent restenosis of prior LAD stent.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d74fa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train.copy()\n",
    "annotation = df.annotation[26]\n",
    "text_ = df.text[26]\n",
    "\n",
    "match_list = []\n",
    "for i in annotation:\n",
    "    print(i)\n",
    "    a, text_ = matcher(text_, i[0])\n",
    "    match_list.append((a[0][0], a[0][1], i[1]))\n",
    "    d = create_labs(text_, match_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24892c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b6f2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"Today he arrived via SDA for cath that showed his native disease to be severe with a 70% stenosis\"\n",
    "b = \"cath that showed his native disease to be severe with a 70% stenosis involving the ostium of the first diagonal branch. The mid-distal LAD is small and tapering with diffuse disease  up to 80%. The first diagonal has a high grade ostial stenosis. LCX has 40-50% stenosis proximal to the first OM. RCA has 80% stenosis proximally with competitive flow from the SVG. The LIMA-&gt; D1 is patent and the SVG-&gt; RCA is patent. The SVG-&gt; LAD is not visualized\"\n",
    "s = [a,b]\n",
    "new_s = sorted(s, key=lambda x:s[0].index(x[0]))\n",
    "a = new_s[0]\n",
    "b = new_s[-1]\n",
    "final_s = a[:a.index(b[0])]+b\n",
    "final_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028f712e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2fedf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking p.o. in training labels\n",
    "for i in range(test.shape[0]):\n",
    "    for x in range(len(train['annotation'][i])):\n",
    "        y=re.search(\"&#8211\",train['annotation'][i][x][0])\n",
    "        if y:\n",
    "            print(train['annotation'][i][x][0])\n",
    "            print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91df2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(test.shape[0]):\n",
    "    x = re.search(\"&#8211\",test['text'][i])\n",
    "    if x:\n",
    "        print(x)\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db01a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_3[513]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('dataweek2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4600a15e5b6b646e3db8b2d8ebb346444f23fb424fa93d368313be03df29f350"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
