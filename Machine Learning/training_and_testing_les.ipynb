{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(316727, 2)\n",
      "(489694, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#read txt file\n",
    "def read_txt(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        data = f.readlines()\n",
    "    return data\n",
    "\n",
    "# read the training and testing data\n",
    "traininglist = read_txt('../BERT/data_processed/train(no_BIO).txt')\n",
    "testinglist = read_txt('../BERT/data_processed/test(no_BIO).txt')\n",
    "\n",
    "for i in range(len(traininglist)):\n",
    "    traininglist[i] = traininglist[i].split()\n",
    "\n",
    "for i in range(len(testinglist)):\n",
    "    testinglist[i] = testinglist[i].split()\n",
    "\n",
    "# convert the data into dataframe\n",
    "testingdataframe = pd.DataFrame(testinglist,columns=['text','label'])\n",
    "trainingdataframe = pd.DataFrame(traininglist, columns=['text', 'label'])\n",
    "\n",
    "print(testingdataframe.shape)\n",
    "print(trainingdataframe.shape)\n",
    "\n",
    "df_training = trainingdataframe.fillna(method='ffill')\n",
    "df_testing = testingdataframe.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(            label  counts\n",
       " 0             CAD    5620\n",
       " 1        DIABETES    2284\n",
       " 2     FAMILY_HIST     254\n",
       " 3  HYPERLIPIDEMIA     590\n",
       " 4    HYPERTENSION    2188\n",
       " 5      MEDICATION    6090\n",
       " 6               O  470029\n",
       " 7           OBESE     292\n",
       " 8          SMOKER    2347,\n",
       "             label  counts\n",
       " 0             CAD    3951\n",
       " 1        DIABETES    1676\n",
       " 2     FAMILY_HIST     214\n",
       " 3  HYPERLIPIDEMIA     493\n",
       " 4    HYPERTENSION    1263\n",
       " 5      MEDICATION    4148\n",
       " 6               O  303157\n",
       " 7           OBESE     168\n",
       " 8          SMOKER    1657)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check label counts\n",
    "df_training.groupby('label').size().reset_index(name='counts'), df_testing.groupby('label').size().reset_index(name='counts')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below convert data into binary tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(  label  counts\n",
       " 0   CAD    3951\n",
       " 1     O  312776,\n",
       "   label  counts\n",
       " 0   CAD    5620\n",
       " 1     O  484074)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# into binary tag\n",
    "df_training.loc[df_training.label != 'CAD',\"label\"] = 'O'\n",
    "df_testing.loc[df_testing.label != 'CAD',\"label\"] = 'O'\n",
    "\n",
    "# check label counts\n",
    "df_testing.groupby('label').size().reset_index(name='counts'),df_training.groupby('label').size().reset_index(name='counts')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize and prepare data for ML input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leste\\AppData\\Local\\Temp\\ipykernel_18028\\813917887.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  tt = x_train.append(x_test)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['CAD', 'DIABETES', 'FAMILY_HIST', 'HYPERLIPIDEMIA', 'HYPERTENSION',\n",
       "       'MEDICATION', 'O', 'OBESE', 'SMOKER'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "\n",
    "x_train = df_training.drop('label',axis=1)\n",
    "y_train = df_training.label.values\n",
    "x_test = df_testing.drop('label',axis=1)\n",
    "y_test = df_testing.label.values\n",
    "\n",
    "# conbine train and test into one to vectorize to ensure same dimension\n",
    "v = DictVectorizer(sparse=True)\n",
    "tt = x_train.append(x_test)\n",
    "ttt =  v.fit_transform(tt.to_dict('records'))\n",
    "x_train = ttt[:x_train.shape[0]]\n",
    "x_test = ttt[x_train.shape[0]:]\n",
    "\n",
    "# define labels\n",
    "classes = np.unique(y_train)\n",
    "#new_classes.pop() #remove the last element 'O' from the list\n",
    "classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tried to only use testing, similar result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array(['CAD', 'O'], dtype=object), array([  2733, 218975], dtype=int64)),\n",
       " (array(['CAD', 'O'], dtype=object), array([ 1218, 93801], dtype=int64)))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingtext_with_o = df_testing.drop('label',axis=1)  #drop the label column\n",
    "traininglabel_with_o = df_testing.label.values\n",
    "\n",
    "v = DictVectorizer(sparse=True) #sparse=True means the output is a sparse matrix\n",
    "trainingtext_with_o = v.fit_transform(trainingtext_with_o.to_dict('records')) #to_dict('records') means the output is a list of dictionaries\n",
    "#print(trainingtext_with_o)\n",
    "\n",
    "classes = np.unique(traininglabel_with_o) #get the unique labels as classes for performance evaluation\n",
    "classes = classes.tolist() #convert the numpy array to list\n",
    "#print(classes)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(trainingtext_with_o, traininglabel_with_o, test_size = 0.3, random_state=0) \n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape\n",
    "# check new label counts\n",
    "np.unique(y_train, return_counts=True), np.unique(y_test, return_counts=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## upsampling; downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218975 2733 5466\n",
      "(array(['CAD'], dtype=object), array([2733], dtype=int64))\n",
      "(array(['O'], dtype=object), array([218975], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "from scipy.sparse import vstack\n",
    "import random\n",
    "random.seed(72)\n",
    "\n",
    "def downsample(x_train, y_train):\n",
    "    # make two dataframes, each with only one class \n",
    "    majority_df = x_train[y_train == \"O\"]\n",
    "    minority_df = x_train[y_train != \"O\"]\n",
    "    row = np.random.permutation(majority_df.shape[0])[:2*minority_df.shape[0]]\n",
    "    print(majority_df.shape[0], minority_df.shape[0], row.shape[0])\n",
    "\n",
    "    # remember the true label because need to combine with the downsampled majority class labels later\n",
    "    t_lab = y_train[y_train == \"CAD\"]\n",
    "    print(np.unique(t_lab, return_counts=True))\n",
    "    print(np.unique(y_train[y_train == \"O\"], return_counts=True))\n",
    "\n",
    "    # downsample majority class\n",
    "    d_majority_df = majority_df[row]\n",
    "\n",
    "\n",
    "    # combine minority class with downsampled majority class\n",
    "    x_train = vstack((minority_df,d_majority_df))\n",
    "\n",
    "    # combine the true label for y\n",
    "    y_train = np.concatenate((t_lab,y_train[y_train == \"O\"][row]))\n",
    "    \n",
    "    return x_train, y_train\n",
    "\n",
    "# downsample the training data\n",
    "x_train, y_train = downsample(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array(['CAD', 'O'], dtype=object), array([  5620, 484074], dtype=int64)),\n",
       " (array(['CAD', 'O'], dtype=object), array([  3951, 312776], dtype=int64)))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape\n",
    "\n",
    "# check new label counts\n",
    "np.unique(y_train, return_counts=True), np.unique(y_test, return_counts=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML classifiers!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 9.17, NNZs: 84, Bias: -0.040000, T: 489694, Avg. loss: 0.000246\n",
      "Total training time: 0.20 seconds.\n",
      "Norm: 67.58, NNZs: 4567, Bias: 0.010000, T: 489694, Avg. loss: 0.022582\n",
      "Total training time: 0.20 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   9 | elapsed:    0.3s remaining:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   9 | elapsed:    0.3s remaining:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   9 | elapsed:    0.3s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   9 | elapsed:    0.3s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   9 | elapsed:    0.3s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of   9 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:    0.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 7.07, NNZs: 50, Bias: -0.080000, T: 489694, Avg. loss: 0.000449\n",
      "Total training time: 0.24 seconds.\n",
      "Norm: 19.87, NNZs: 395, Bias: -0.070000, T: 489694, Avg. loss: 0.003240\n",
      "Total training time: 0.19 seconds.\n",
      "Norm: 37.46, NNZs: 1403, Bias: -0.010000, T: 489694, Avg. loss: 0.005298\n",
      "Total training time: 0.27 seconds.\n",
      "Norm: 24.76, NNZs: 613, Bias: -0.050000, T: 489694, Avg. loss: 0.002597\n",
      "Total training time: 0.26 seconds.\n",
      "Norm: 31.29, NNZs: 979, Bias: -0.150000, T: 489694, Avg. loss: 0.008481\n",
      "Total training time: 0.29 seconds.\n",
      "Norm: 12.57, NNZs: 158, Bias: -0.080000, T: 489694, Avg. loss: 0.000502\n",
      "Total training time: 0.21 seconds.\n",
      "Norm: 30.66, NNZs: 940, Bias: -0.020000, T: 489694, Avg. loss: 0.002235\n",
      "Total training time: 0.27 seconds.\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "           CAD       0.27      0.16      0.20      3951\n",
      "      DIABETES       0.43      0.37      0.39      1676\n",
      "   FAMILY_HIST       0.08      0.01      0.02       214\n",
      "HYPERLIPIDEMIA       0.54      0.32      0.40       493\n",
      "  HYPERTENSION       0.24      0.43      0.31      1263\n",
      "    MEDICATION       0.32      0.55      0.40      4148\n",
      "             O       0.97      0.97      0.97    303157\n",
      "         OBESE       0.63      0.51      0.56       168\n",
      "        SMOKER       0.36      0.20      0.26      1657\n",
      "\n",
      "      accuracy                           0.94    316727\n",
      "     macro avg       0.43      0.39      0.39    316727\n",
      "  weighted avg       0.95      0.94      0.94    316727\n",
      "\n"
     ]
    }
   ],
   "source": [
    "per = Perceptron(verbose=10, n_jobs=-1, max_iter=5) #this is the perceptron model\n",
    "per.partial_fit(x_train, y_train,classes=classes)\n",
    "y_pred=per.predict(x_test)\n",
    "print (classification_report(y_pred=per.predict(x_test), y_true=y_test, labels=classes)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../BERT/perceptron_pred.txt', 'w') as f:\n",
    "    for i in range(len(y_pred)):\n",
    "        f.write(df_testing['text'][i] + ' ' + y_test[i] + ' ' + y_pred[i] + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert text level annotation into doc level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Record</td>\n",
       "      <td>SMOKER</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>date:</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2069-04-07</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mr.</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Villegas</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316722</th>\n",
       "      <td>Team</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316723</th>\n",
       "      <td>4</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316724</th>\n",
       "      <td>Beeper</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316725</th>\n",
       "      <td>#07736</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316726</th>\n",
       "      <td>#07736</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>316727 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0       1  2\n",
       "0           Record  SMOKER  O\n",
       "1            date:       O  O\n",
       "2       2069-04-07       O  O\n",
       "3              Mr.       O  O\n",
       "4         Villegas       O  O\n",
       "...            ...     ... ..\n",
       "316722        Team       O  O\n",
       "316723           4       O  O\n",
       "316724      Beeper       O  O\n",
       "316725      #07736       O  O\n",
       "316726      #07736       O  O\n",
       "\n",
       "[316727 rows x 3 columns]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from os import listdir\n",
    "\n",
    "# read in the prediction file\n",
    "pred = pd.read_csv('../BERT/perceptron_pred.txt', sep=' ', header=None)\n",
    "file_path_3 = \"C:/Users/Leste/OneDrive - Johns Hopkins/Desktop/BDD data/extracted/testing-RiskFactors-Complete/\"\n",
    "pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_doc_tag (pred, file_path_3, type = 2):\n",
    "    # get the text and label\n",
    "    text = ' '.join(pred[0].astype(str))\n",
    "    label = pred[type]\n",
    "\n",
    "    # get the file name\n",
    "    test_name = [f for f in listdir(file_path_3) if f.endswith('.xml')]\n",
    "    test_name = [re.sub(r'\\.xml', '', x) for x in test_name]\n",
    "\n",
    "    # create a dataframe to store the annotation\n",
    "    all_df = pd.DataFrame(np.zeros((len(test_name), 2), dtype=object), columns=['file', 'annotation'])\n",
    "\n",
    "    # get the index of the first line of each file\n",
    "    date = (pred[0].str.contains(\"date:\")).values\n",
    "    temp = [i for i, x in enumerate(date) if x]\n",
    "    loc = [temp[i]-1 for i in range(len(temp)) if pred[0][temp[i]-1] == 'Record']\n",
    "    print(loc)\n",
    "\n",
    "    # get the annotation for each file\n",
    "    tag = [np.setdiff1d(label[loc[x]:loc[x+1]].unique(),\"O\") for x in range(len(loc)-1)]\n",
    "    tag.append(np.setdiff1d(label[loc[-1]:].unique(),\"O\"))\n",
    "    print(tag)\n",
    "\n",
    "    # add the annotation to the dataframe\n",
    "    all_df['annotation'] = tag\n",
    "    all_df['file'] = test_name\n",
    "    return(all_df)\n",
    "\n",
    "\n",
    "#df_pred = find_doc_tag(pred, file_path_3, type = 2)\n",
    "df_orig = find_doc_tag(pred, file_path_3, type = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>annotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>110-01</td>\n",
       "      <td>[HYPERTENSION, MEDICATION, SMOKER]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110-02</td>\n",
       "      <td>[HYPERTENSION, SMOKER]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>110-03</td>\n",
       "      <td>[CAD, DIABETES, HYPERLIPIDEMIA, HYPERTENSION, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>110-04</td>\n",
       "      <td>[CAD, DIABETES, HYPERLIPIDEMIA, HYPERTENSION, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>111-01</td>\n",
       "      <td>[DIABETES, HYPERTENSION, MEDICATION, SMOKER]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>388-05</td>\n",
       "      <td>[DIABETES, HYPERLIPIDEMIA, HYPERTENSION, MEDIC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>389-01</td>\n",
       "      <td>[SMOKER]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>389-02</td>\n",
       "      <td>[DIABETES, HYPERTENSION, MEDICATION, SMOKER]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>389-03</td>\n",
       "      <td>[DIABETES, MEDICATION, SMOKER]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>389-04</td>\n",
       "      <td>[DIABETES, HYPERLIPIDEMIA, HYPERTENSION, MEDIC...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>514 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       file                                         annotation\n",
       "0    110-01                 [HYPERTENSION, MEDICATION, SMOKER]\n",
       "1    110-02                             [HYPERTENSION, SMOKER]\n",
       "2    110-03  [CAD, DIABETES, HYPERLIPIDEMIA, HYPERTENSION, ...\n",
       "3    110-04  [CAD, DIABETES, HYPERLIPIDEMIA, HYPERTENSION, ...\n",
       "4    111-01       [DIABETES, HYPERTENSION, MEDICATION, SMOKER]\n",
       "..      ...                                                ...\n",
       "509  388-05  [DIABETES, HYPERLIPIDEMIA, HYPERTENSION, MEDIC...\n",
       "510  389-01                                           [SMOKER]\n",
       "511  389-02       [DIABETES, HYPERTENSION, MEDICATION, SMOKER]\n",
       "512  389-03                     [DIABETES, MEDICATION, SMOKER]\n",
       "513  389-04  [DIABETES, HYPERLIPIDEMIA, HYPERTENSION, MEDIC...\n",
       "\n",
       "[514 rows x 2 columns]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get doc level annotationi from testing file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from os import listdir\n",
    "import re\n",
    "\n",
    "file_path_3 = \"C:/Users/Leste/OneDrive - Johns Hopkins/Desktop/BDD data/extracted/testing-RiskFactors-Complete/\"\n",
    "test_name = [f for f in listdir(file_path_3) if f.endswith('.xml')]\n",
    "\n",
    "df_orig = pd.DataFrame(np.zeros((len(test_name), 2), dtype=object), columns=['file', 'annotation'])\n",
    "\n",
    "for i in range(len(test_name)):\n",
    "    tree = ET.parse(file_path_3 + test_name[i])\n",
    "    root = tree.getroot()\n",
    "    tag = [root[1][k][m].tag for k in range(len(root[1])) for m in range(len(root[1][k])) if root[1][k][m].attrib.keys().__contains__('text') == True]\n",
    "    df_orig['annotation'][i] = np.unique(np.array(tag))\n",
    "    df_orig['file'][i] = re.sub(r'\\.xml','',test_name[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CAD       0.52      0.04      0.08      3951\n",
      "           O       0.99      1.00      0.99    312776\n",
      "\n",
      "    accuracy                           0.99    316727\n",
      "   macro avg       0.75      0.52      0.54    316727\n",
      "weighted avg       0.98      0.99      0.98    316727\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#support vector machine\n",
    "svm = SGDClassifier(alpha=.00001, max_iter=100,penalty=\"elasticnet\")\n",
    "svm.partial_fit(x_train, y_train,classes=classes)\n",
    "\n",
    "print (classification_report(y_pred=svm.predict(x_test), y_true=y_test, labels=classes))\n",
    "#labels=classes means the performance evaluation is based on all the labels \n",
    "#labels=new_classes means the performance evaluation is based on all the labels except 'O'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CAD       0.45      0.05      0.09      3951\n",
      "           O       0.99      1.00      0.99    312776\n",
      "\n",
      "    accuracy                           0.99    316727\n",
      "   macro avg       0.72      0.53      0.54    316727\n",
      "weighted avg       0.98      0.99      0.98    316727\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#naive bayes\n",
    "nb = MultinomialNB(alpha=.0005)\n",
    "nb.partial_fit(x_train, y_train,classes=classes)\n",
    "\n",
    "print (classification_report(y_pred=nb.predict(x_test), y_true=y_test, labels=classes))\n",
    "#labels=classes means the performance evaluation is based on all the labels \n",
    "#labels=new_classes means the performance evaluation is based on all the labels except 'O'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=20,random_state=553)\n",
    "rf.fit(x_train, y_train)\n",
    "\n",
    "print (classification_report(y_pred=rf.predict(x_test), y_true=y_test, labels=classes))\n",
    "#labels=classes means the performance evaluation is based on all the labels \n",
    "#labels=new_classes means the performance evaluation is based on all the labels except 'O'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_train, y_train)\n",
    "\n",
    "print (classification_report(y_pred=lr.predict(x_test), y_true=y_test, labels=new_classes))\n",
    "#labels=classes means the performance evaluation is based on all the labels \n",
    "#labels=new_classes means the performance evaluation is based on all the labels except 'O'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below you can train in different classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perceptron\n",
    "per_no = Perceptron(verbose=10, n_jobs=-1, max_iter=20) #this is the perceptron model\n",
    "per_no.partial_fit(x_without_o_train, y_without_o_train,classes=classes_without_o)\n",
    "\n",
    "print (classification_report(y_pred=per_no.predict(x_without_o_test), y_true=y_without_o_test, labels=classes_without_o))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#support vector machine\n",
    "svm_no = SGDClassifier(alpha=.00001, max_iter=100,penalty=\"elasticnet\")\n",
    "svm_no.partial_fit(x_without_o_train, y_without_o_train,classes=classes_without_o)\n",
    "\n",
    "print (classification_report(y_pred=svm_no.predict(x_without_o_test), y_true=y_without_o_test, labels=classes_without_o))\n",
    "\n",
    "print(precision_score(y_pred=svm_no.predict(x_without_o_test), y_true=y_without_o_test, labels=classes_without_o, average='micro'))\n",
    "print(recall_score(y_pred=svm_no.predict(x_without_o_test), y_true=y_without_o_test, labels=classes_without_o, average='micro'))\n",
    "print(f1_score(y_pred=svm_no.predict(x_without_o_test), y_true=y_without_o_test, labels=classes_without_o, average='micro'))\n",
    "print(precision_score(y_pred=svm_no.predict(x_without_o_test), y_true=y_without_o_test, labels=classes_without_o, average='macro'))\n",
    "print(recall_score(y_pred=svm_no.predict(x_without_o_test), y_true=y_without_o_test, labels=classes_without_o, average='macro'))\n",
    "print(f1_score(y_pred=svm_no.predict(x_without_o_test), y_true=y_without_o_test, labels=classes_without_o, average='macro'))\n",
    "print(precision_score(y_pred=svm_no.predict(x_without_o_test), y_true=y_without_o_test, labels=classes_without_o, average='weighted'))\n",
    "print(recall_score(y_pred=svm_no.predict(x_without_o_test), y_true=y_without_o_test, labels=classes_without_o, average='weighted'))\n",
    "print(f1_score(y_pred=svm_no.predict(x_without_o_test), y_true=y_without_o_test, labels=classes_without_o, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#naive bayes\n",
    "nb_no = MultinomialNB(alpha=0.01)\n",
    "nb_no.partial_fit(x_without_o_train, y_without_o_train,classes=classes_without_o)\n",
    "\n",
    "#打印分类报告，并保留四位小数\n",
    "print (classification_report(y_pred=nb_no.predict(x_without_o_test), y_true=y_without_o_test, labels=classes_without_o))\n",
    "\n",
    "print(precision_score(y_pred=nb_no.predict(x_without_o_test), y_true=y_without_o_test, labels=classes_without_o, average='micro'))\n",
    "print(recall_score(y_pred=nb_no.predict(x_without_o_test), y_true=y_without_o_test, labels=classes_without_o, average='micro'))\n",
    "print(f1_score(y_pred=nb_no.predict(x_without_o_test), y_true=y_without_o_test, labels=classes_without_o, average='micro'))\n",
    "print(precision_score(y_pred=nb_no.predict(x_without_o_test), y_true=y_without_o_test, labels=classes_without_o, average='macro'))\n",
    "print(recall_score(y_pred=nb_no.predict(x_without_o_test), y_true=y_without_o_test, labels=classes_without_o, average='macro'))\n",
    "print(f1_score(y_pred=nb_no.predict(x_without_o_test), y_true=y_without_o_test, labels=classes_without_o, average='macro'))\n",
    "print(precision_score(y_pred=nb_no.predict(x_without_o_test), y_true=y_without_o_test, labels=classes_without_o, average='weighted'))\n",
    "print(recall_score(y_pred=nb_no.predict(x_without_o_test), y_true=y_without_o_test, labels=classes_without_o, average='weighted'))\n",
    "print(f1_score(y_pred=nb_no.predict(x_without_o_test), y_true=y_without_o_test, labels=classes_without_o, average='weighted'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=30,random_state=533)\n",
    "rf.fit(x_without_o_train, y_without_o_train)\n",
    "\n",
    "print (classification_report(y_pred=rf.predict(x_without_o_test), y_true=y_without_o_test, labels=classes_without_o))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_without_o_train, y_without_o_train)\n",
    "\n",
    "print (classification_report(y_pred=lr.predict(x_without_o_test), y_true=y_without_o_test, labels=classes_without_o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(x_without_o_train, y_without_o_train)\n",
    "\n",
    "print (classification_report(y_pred=dt.predict(x_without_o_test), y_true=y_without_o_test, labels=classes_without_o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#knn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(x_without_o_train, y_without_o_train)\n",
    "\n",
    "print (classification_report(y_pred=knn.predict(x_without_o_test), y_true=y_without_o_test, labels=classes_without_o))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stanford NER tagger result calculation\n",
    "## just for calculating the accuracy of stanford NER tagger, not for training and testing of traditional machine learning classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_test当中除去o的数目\n",
    "total = y_test[y_test != 'O']\n",
    "print(len(total))\n",
    "\n",
    "TP = 2690\n",
    "FP = 1810\n",
    "FN = 3209\n",
    "TN = total.size - TP - FP - FN\n",
    "print(TP, FP, FN, TN)\n",
    "\n",
    "#calculate the accuracy\n",
    "accuracy = (TP + TN) / (TP + FP + FN + TN)\n",
    "print(accuracy)\n",
    "\n",
    "#calculate the precision\n",
    "precision = TP / (TP + FP)\n",
    "print(precision)\n",
    "\n",
    "#calculate the recall\n",
    "recall = TP / (TP + FN)\n",
    "print(recall)\n",
    "\n",
    "#calculate the F1 score\n",
    "F1 = 2 * precision * recall / (precision + recall)\n",
    "print(F1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read result.txt\n",
    "path = 'E:/JHU/课程/datadesign/NLP/machine_learning/stanford-ner-4.2.0/stanford-ner-2020-11-17/test'\n",
    "\n",
    "data = pd.read_csv(path + '/result_M40_N0_chris2useLC.txt', sep='\\t', header=None, names=['Entity', 'Percision', 'Recall', 'F1', 'TP', 'FP', 'FN'])\n",
    "#Drop the first row\n",
    "data = data.drop([0])\n",
    "\n",
    "print(data)\n",
    "print('')\n",
    "\n",
    "#Drop the last row\n",
    "data = data.drop([len(data)])\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "#convert the data type\n",
    "data['Percision'] = data['Percision'].astype(float)\n",
    "data['Recall'] = data['Recall'].astype(float)\n",
    "data['F1'] = data['F1'].astype(float)\n",
    "data['TP'] = data['TP'].astype(int)\n",
    "data['FP'] = data['FP'].astype(int)\n",
    "data['FN'] = data['FN'].astype(int)\n",
    "#print(data)\n",
    "\n",
    "#calculate the micro average\n",
    "TP = data['TP'].sum()\n",
    "FP = data['FP'].sum()\n",
    "FN = data['FN'].sum()\n",
    "micro_precision = TP / (TP + FP)\n",
    "micro_recall = TP / (TP + FN)\n",
    "micro_F1 = 2 * micro_precision * micro_recall / (micro_precision + micro_recall)\n",
    "\n",
    "#calculate the macro average\n",
    "macro_precision = data['Percision'].mean()\n",
    "macro_recall = data['Recall'].mean()\n",
    "macro_F1 = data['F1'].mean()\n",
    "\n",
    "#calculate the support\n",
    "support = data['TP']\n",
    "#print(support)\n",
    "#add the support to the dataframe\n",
    "data['support'] = support\n",
    "support_proportion = support / support.sum()\n",
    "#print(support_proportion)\n",
    "\n",
    "#calculate the weighted average\n",
    "weighted_precision = (data['Percision'] * support_proportion).sum()\n",
    "weighted_recall = (data['Recall'] * support_proportion).sum()\n",
    "weighted_F1 = (data['F1'] * support_proportion).sum()\n",
    "\n",
    "\n",
    "Evalution = pd.DataFrame(columns=['Entity', 'Percision', 'Recall', 'F1', 'TP', 'FP', 'FN','support'])\n",
    "Evalution.loc[len(Evalution)] = ['micro-average', micro_precision, micro_recall, micro_F1, TP, FP, FN, support.sum()]\n",
    "Evalution.loc[len(Evalution)] = ['macro-average', macro_precision, macro_recall, macro_F1, TP, FP, FN, support.sum()]\n",
    "Evalution.loc[len(Evalution)] = ['weighted-average', weighted_precision, weighted_recall, weighted_F1, TP, FP, FN, support.sum()]\n",
    "\n",
    "\n",
    "print(data)\n",
    "print('')\n",
    "print(Evalution)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataweek2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4600a15e5b6b646e3db8b2d8ebb346444f23fb424fa93d368313be03df29f350"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
