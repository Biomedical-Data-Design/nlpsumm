{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91f12cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from os import listdir\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65c0c47",
   "metadata": {},
   "source": [
    "# Split text and tag\n",
    "\n",
    "Smoker and Family_hist can have no tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be1a3360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(514, 2)\n"
     ]
    }
   ],
   "source": [
    "file_path = \"E:/JHU/课程/datadesign/NLP/data/testing-RiskFactors-Complete/\"\n",
    "names = [f for f in listdir(file_path) if f.endswith('.xml')]\n",
    "test = pd.DataFrame(np.zeros((len(names), 2), dtype=object), columns=['text', 'annotation'])\n",
    "print(test.shape)\n",
    "n = 0\n",
    "for name in names:\n",
    "    tree = ET.parse(file_path + name)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    ## Get the text\n",
    "    nt = re.sub('\\n',' ',root[0].text)\n",
    "    nt = re.sub('\\t',' ',nt) \n",
    "    nt = re.sub('\"',\"'\",nt)\n",
    "    ## sample 214 has a weird character\n",
    "    nt = re.sub('>','&gt;',nt) \n",
    "    nt = re.sub('<','&lt;',nt)\n",
    "    ## sample 262 has a weird character\n",
    "    nt = re.sub('Â',' ',nt)\n",
    "    nt = re.sub('â',' ',nt)\n",
    "    nt = re.sub('€',' ',nt)\n",
    "    nt = re.sub('™',' ',nt)\n",
    "    test['text'][n] = nt\n",
    "    n+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4a827526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>annotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Record date: 2069-04-07        Mr. Villegas...</td>\n",
       "      <td>[(aspirin, MEDICATION), (aspirin, MEDICATION),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Record date: 2075-01-07      NAME:    Ville...</td>\n",
       "      <td>[(Hypertension, HYPERTENSION), (Hypertension, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Record date: 2080-02-18     SDU JAR Admissi...</td>\n",
       "      <td>[(Hyperlipidemia , HYPERLIPIDEMIA), (Hyperlipi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Record date: 2080-10-01    Reason for Visit...</td>\n",
       "      <td>[(hyperlipidemia, HYPERLIPIDEMIA), (hyperlipid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Record date: 2083-07-20                    ...</td>\n",
       "      <td>[(Zestril,, MEDICATION), (Zestril, MEDICATION)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0     Record date: 2069-04-07        Mr. Villegas...   \n",
       "1     Record date: 2075-01-07      NAME:    Ville...   \n",
       "2     Record date: 2080-02-18     SDU JAR Admissi...   \n",
       "3     Record date: 2080-10-01    Reason for Visit...   \n",
       "4     Record date: 2083-07-20                    ...   \n",
       "\n",
       "                                          annotation  \n",
       "0  [(aspirin, MEDICATION), (aspirin, MEDICATION),...  \n",
       "1  [(Hypertension, HYPERTENSION), (Hypertension, ...  \n",
       "2  [(Hyperlipidemia , HYPERLIPIDEMIA), (Hyperlipi...  \n",
       "3  [(hyperlipidemia, HYPERLIPIDEMIA), (hyperlipid...  \n",
       "4  [(Zestril,, MEDICATION), (Zestril, MEDICATION)...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 0\n",
    "for name in names:\n",
    "    tree = ET.parse(file_path + name)\n",
    "    root = tree.getroot()\n",
    "    ## Get the labels\n",
    "    # skip FAMILY_HIST: doesn't have a text\n",
    "    #skip = [root[1][x].tag for x in range(len(root[1]))].index('FAMILY_HIST')\n",
    "    #PHI = [root[1][x].tag for x in range(len(root[1]))].index('PHI') \n",
    "    # get all labels except for PHI\n",
    "    tag_list = []\n",
    "    for k in range(len(root[1])):\n",
    "        for m in range(len(root[1][k])):\n",
    "            if root[1][k][m].attrib.keys().__contains__('text') == False:\n",
    "                continue\n",
    "            tag_list.append((root[1][k][m].attrib['text'],root[1][k][m].tag))\n",
    "    # get PHI labels\n",
    "    #for k in range(PHI,len(root[1])):\n",
    "    #    tag_list.append((root[1][k].attrib['text'],root[1][k].tag))\n",
    "    test['annotation'][n] = tag_list\n",
    "    n+=1\n",
    "\n",
    "test.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93ebd4f",
   "metadata": {},
   "source": [
    "# Functions to convert dataframe into input format\n",
    "## use wisely, very slow \n",
    " Please use create_labs function instead of mark_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d4e3a81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "[4]\n",
      "[5]\n",
      "[6]\n",
      "[7]\n",
      "[8]\n",
      "[9]\n",
      "[10]\n",
      "[11]\n",
      "[12]\n",
      "[13]\n",
      "[14]\n",
      "[15]\n",
      "[16]\n",
      "[17]\n",
      "[18]\n",
      "[19]\n",
      "[20]\n",
      "[21]\n",
      "[22]\n",
      "[23]\n",
      "[24]\n",
      "[25]\n",
      "[26]\n",
      "[27]\n",
      "[28]\n",
      "[29]\n",
      "[30]\n",
      "[31]\n",
      "[32]\n",
      "[33]\n",
      "[34]\n",
      "[35]\n",
      "[36]\n",
      "[37]\n",
      "[38]\n",
      "[39]\n",
      "[40]\n",
      "[41]\n",
      "[42]\n",
      "[43]\n",
      "[44]\n",
      "[45]\n",
      "[46]\n",
      "[47]\n",
      "[48]\n",
      "[49]\n",
      "[50]\n",
      "[51]\n",
      "[52]\n",
      "[53]\n",
      "[54]\n",
      "[55]\n",
      "[56]\n",
      "[57]\n",
      "[58]\n",
      "[59]\n",
      "[60]\n",
      "[61]\n",
      "[62]\n",
      "[63]\n",
      "[64]\n",
      "[65]\n",
      "[66]\n",
      "[67]\n",
      "[68]\n",
      "[69]\n",
      "[70]\n",
      "[71]\n",
      "[72]\n",
      "[73]\n",
      "[74]\n",
      "[75]\n",
      "[76]\n",
      "[77]\n",
      "[78]\n",
      "[79]\n",
      "[80]\n",
      "[81]\n",
      "[82]\n",
      "[83]\n",
      "[84]\n",
      "[85]\n",
      "[86]\n",
      "[87]\n",
      "[88]\n",
      "[89]\n",
      "[90]\n",
      "[91]\n",
      "[92]\n",
      "[93]\n",
      "[94]\n",
      "[95]\n",
      "[96]\n",
      "[97]\n",
      "[98]\n",
      "[99]\n",
      "[100]\n",
      "[101]\n",
      "[102]\n",
      "[103]\n",
      "[104]\n",
      "[105]\n",
      "[106]\n",
      "[107]\n",
      "[108]\n",
      "[109]\n",
      "[110]\n",
      "[111]\n",
      "[112]\n",
      "[113]\n",
      "[114]\n",
      "[115]\n",
      "[116]\n",
      "[117]\n",
      "[118]\n",
      "[119]\n",
      "[120]\n",
      "[121]\n",
      "[122]\n",
      "[123]\n",
      "[124]\n",
      "[125]\n",
      "[126]\n",
      "[127]\n",
      "[128]\n",
      "[129]\n",
      "[130]\n",
      "[131]\n",
      "[132]\n",
      "[133]\n",
      "[134]\n",
      "[135]\n",
      "[136]\n",
      "[137]\n",
      "[138]\n",
      "[139]\n",
      "[140]\n",
      "[141]\n",
      "[142]\n",
      "[143]\n",
      "[144]\n",
      "[145]\n",
      "[146]\n",
      "[147]\n",
      "[148]\n",
      "[149]\n",
      "[150]\n",
      "[151]\n",
      "[152]\n",
      "[153]\n",
      "[154]\n",
      "[155]\n",
      "[156]\n",
      "[157]\n",
      "[158]\n",
      "[159]\n",
      "[160]\n",
      "[161]\n",
      "[162]\n",
      "[163]\n",
      "[164]\n",
      "[165]\n",
      "[166]\n",
      "[167]\n",
      "[168]\n",
      "[169]\n",
      "[170]\n",
      "[171]\n",
      "[172]\n",
      "[173]\n",
      "[174]\n",
      "[175]\n",
      "[176]\n",
      "[177]\n",
      "[178]\n",
      "[179]\n",
      "[180]\n",
      "[181]\n",
      "[182]\n",
      "[183]\n",
      "[184]\n",
      "[185]\n",
      "[186]\n",
      "[187]\n",
      "[188]\n",
      "[189]\n",
      "[190]\n",
      "[191]\n",
      "[192]\n",
      "[193]\n",
      "[194]\n",
      "[195]\n",
      "[196]\n",
      "[197]\n",
      "[198]\n",
      "[199]\n",
      "[200]\n",
      "[201]\n",
      "[202]\n",
      "[203]\n",
      "[204]\n",
      "[205]\n",
      "[206]\n",
      "[207]\n",
      "[208]\n",
      "[209]\n",
      "[210]\n",
      "[211]\n",
      "[212]\n",
      "[213]\n",
      "[214]\n",
      "[215]\n",
      "[216]\n",
      "[217]\n",
      "[218]\n",
      "[219]\n",
      "[220]\n",
      "[221]\n",
      "[222]\n",
      "[223]\n",
      "[224]\n",
      "[225]\n",
      "[226]\n",
      "[227]\n",
      "[228]\n",
      "[229]\n",
      "[230]\n",
      "[231]\n",
      "[232]\n",
      "[233]\n",
      "[234]\n",
      "[235]\n",
      "[236]\n",
      "[237]\n",
      "[238]\n",
      "[239]\n",
      "[240]\n",
      "[241]\n",
      "[242]\n",
      "[243]\n",
      "[244]\n",
      "[245]\n",
      "[246]\n",
      "[247]\n",
      "[248]\n",
      "[249]\n",
      "[250]\n",
      "[251]\n",
      "[252]\n",
      "[253]\n",
      "[254]\n",
      "[255]\n",
      "[256]\n",
      "[257]\n",
      "[258]\n",
      "[259]\n",
      "[260]\n",
      "[261]\n",
      "[262]\n",
      "[263]\n",
      "[264]\n",
      "[265]\n",
      "[266]\n",
      "[267]\n",
      "[268]\n",
      "[269]\n",
      "[270]\n",
      "[271]\n",
      "[272]\n",
      "[273]\n",
      "[274]\n",
      "[275]\n",
      "[276]\n",
      "[277]\n",
      "[278]\n",
      "[279]\n",
      "[280]\n",
      "[281]\n",
      "[282]\n",
      "[283]\n",
      "[284]\n",
      "[285]\n",
      "[286]\n",
      "[287]\n",
      "[288]\n",
      "[289]\n",
      "[290]\n",
      "[291]\n",
      "[292]\n",
      "[293]\n",
      "[294]\n",
      "[295]\n",
      "[296]\n",
      "[297]\n",
      "[298]\n",
      "[299]\n",
      "[300]\n",
      "[301]\n",
      "[302]\n",
      "[303]\n",
      "[304]\n",
      "[305]\n",
      "[306]\n",
      "[307]\n",
      "[308]\n",
      "[309]\n",
      "[310]\n",
      "[311]\n",
      "[312]\n",
      "[313]\n",
      "[314]\n",
      "[315]\n",
      "[316]\n",
      "[317]\n",
      "[318]\n",
      "[319]\n",
      "[320]\n",
      "[321]\n",
      "[322]\n",
      "[323]\n",
      "[324]\n",
      "[325]\n",
      "[326]\n",
      "[327]\n",
      "[328]\n",
      "[329]\n",
      "[330]\n",
      "[331]\n",
      "[332]\n",
      "[333]\n",
      "[334]\n",
      "[335]\n",
      "[336]\n",
      "[337]\n",
      "[338]\n",
      "[339]\n",
      "[340]\n",
      "[341]\n",
      "[342]\n",
      "[343]\n",
      "[344]\n",
      "[345]\n",
      "[346]\n",
      "[347]\n",
      "[348]\n",
      "[349]\n",
      "[350]\n",
      "[351]\n",
      "[352]\n",
      "[353]\n",
      "[354]\n",
      "[355]\n",
      "[356]\n",
      "[357]\n",
      "[358]\n",
      "[359]\n",
      "[360]\n",
      "[361]\n",
      "[362]\n",
      "[363]\n",
      "[364]\n",
      "[365]\n",
      "[366]\n",
      "[367]\n",
      "[368]\n",
      "[369]\n",
      "[370]\n",
      "[371]\n",
      "[372]\n",
      "[373]\n",
      "[374]\n",
      "[375]\n",
      "[376]\n",
      "[377]\n",
      "[378]\n",
      "[379]\n",
      "[380]\n",
      "[381]\n",
      "[382]\n",
      "[383]\n",
      "[384]\n",
      "[385]\n",
      "[386]\n",
      "[387]\n",
      "[388]\n",
      "[389]\n",
      "[390]\n",
      "[391]\n",
      "[392]\n",
      "[393]\n",
      "[394]\n",
      "[395]\n",
      "[396]\n",
      "[397]\n",
      "[398]\n",
      "[399]\n",
      "[400]\n",
      "[401]\n",
      "[402]\n",
      "[403]\n",
      "[404]\n",
      "[405]\n",
      "[406]\n",
      "[407]\n",
      "[408]\n",
      "[409]\n",
      "[410]\n",
      "[411]\n",
      "[412]\n",
      "[413]\n",
      "[414]\n",
      "[415]\n",
      "[416]\n",
      "[417]\n",
      "[418]\n",
      "[419]\n",
      "[420]\n",
      "[421]\n",
      "[422]\n",
      "[423]\n",
      "[424]\n",
      "[425]\n",
      "[426]\n",
      "[427]\n",
      "[428]\n",
      "[429]\n",
      "[430]\n",
      "[431]\n",
      "[432]\n",
      "[433]\n",
      "[434]\n",
      "[435]\n",
      "[436]\n",
      "[437]\n",
      "[438]\n",
      "[439]\n",
      "[440]\n",
      "[441]\n",
      "[442]\n",
      "[443]\n",
      "[444]\n",
      "[445]\n",
      "[446]\n",
      "[447]\n",
      "[448]\n",
      "[449]\n",
      "[450]\n",
      "[451]\n",
      "[452]\n",
      "[453]\n",
      "[454]\n",
      "[455]\n",
      "[456]\n",
      "[457]\n",
      "[458]\n",
      "[459]\n",
      "[460]\n",
      "[461]\n",
      "[462]\n",
      "[463]\n",
      "[464]\n",
      "[465]\n",
      "[466]\n",
      "[467]\n",
      "[468]\n",
      "[469]\n",
      "[470]\n",
      "[471]\n",
      "[472]\n",
      "[473]\n",
      "[474]\n",
      "[475]\n",
      "[476]\n",
      "[477]\n",
      "[478]\n",
      "[479]\n",
      "[480]\n",
      "[481]\n",
      "[482]\n",
      "[483]\n",
      "[484]\n",
      "[485]\n",
      "[486]\n",
      "[487]\n",
      "[488]\n",
      "[489]\n",
      "[490]\n",
      "[491]\n",
      "[492]\n",
      "[493]\n",
      "[494]\n",
      "[495]\n",
      "[496]\n",
      "[497]\n",
      "[498]\n",
      "[499]\n",
      "[500]\n",
      "[501]\n",
      "[502]\n",
      "[503]\n",
      "[504]\n",
      "[505]\n",
      "[506]\n",
      "[507]\n",
      "[508]\n",
      "[509]\n",
      "[510]\n",
      "[511]\n",
      "[512]\n",
      "[513]\n"
     ]
    }
   ],
   "source": [
    "def matcher(string, pattern):\n",
    "    '''\n",
    "    Return the start and end index of any pattern present in the text.\n",
    "    '''\n",
    "    match_list = []\n",
    "    pattern = pattern.strip()\n",
    "    seqMatch = SequenceMatcher(None, string, pattern, autojunk=False)\n",
    "    match = seqMatch.find_longest_match(0, len(string), 0, len(pattern))\n",
    "    if (match.size == len(pattern)):\n",
    "        start = match.a\n",
    "        end = match.a + match.size\n",
    "        match_tup = (start, end)\n",
    "        string = string.replace(pattern, \"X\" * len(pattern), 1)\n",
    "        match_list.append(match_tup)\n",
    "        \n",
    "    return match_list, string\n",
    "\n",
    "\n",
    "def mark_sentence(s, match_list):\n",
    "    '''\n",
    "    Marks all the entities in the sentence as per the BIO scheme. \n",
    "    '''\n",
    "    word_dict = {}\n",
    "    for word in s.split():\n",
    "        word_dict[word] = 'O'\n",
    "        \n",
    "    for start, end, e_type in match_list:\n",
    "        temp_str = s[start:end]\n",
    "        tmp_list = temp_str.split()\n",
    "        if len(tmp_list) > 1:\n",
    "            word_dict[tmp_list[0]] = 'B-' + e_type\n",
    "            for w in tmp_list[1:]:\n",
    "                word_dict[w] = 'I-' + e_type\n",
    "        else:\n",
    "            word_dict[temp_str] = 'B-' + e_type\n",
    "    return word_dict\n",
    "\n",
    "## replace !!mark_sentence!! to better label the text with more than one word\n",
    "def create_labs(s,match_list):\n",
    "    labs = ['O' for i in range(len(s.split()))]\n",
    "    word_dict = pd.DataFrame({'word':s.split(),'label':labs})\n",
    "\n",
    "    for start, end, e_type in match_list:\n",
    "        index = len(re.findall(r' +',s[0:start]))-1\n",
    "        num_words = len(s[start:end].split())\n",
    "\n",
    "        if num_words > 1:\n",
    "            word_dict.loc[index,'label'] = 'B-' + e_type\n",
    "            for i in range(1,num_words):\n",
    "                word_dict.loc[index+i,'label'] = 'I-' + e_type\n",
    "        else:\n",
    "            word_dict.loc[index,'label'] = 'B-' + e_type\n",
    "    return word_dict\n",
    "\n",
    "def clean(text):\n",
    "    '''\n",
    "    Just a helper fuction to add a space before the punctuations for better tokenization\n",
    "    '''\n",
    "    filters = [\"!\", \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"/\", \"*\", \".\", \":\", \";\", \"<\", \"=\", \">\", \"?\", \"@\", \"[\",\n",
    "               \"\\\\\", \"]\", \"_\", \"`\", \"{\", \"}\", \"~\", \"'\"]\n",
    "    for i in text:\n",
    "        if i in filters:\n",
    "            text = text.replace(i, \" \" + i)\n",
    "            \n",
    "    return text\n",
    "\n",
    "def create_data(df, filepath):\n",
    "    '''\n",
    "    The function responsible for the creation of data in the said format.\n",
    "    '''\n",
    "\n",
    "    with open(filepath , 'w') as f:\n",
    "        for text, annotation in zip(df.text, df.annotation): \n",
    "            #text = clean(text)\n",
    "            #text_ = text    \n",
    "            print(test.index[test['text']== text].tolist())    \n",
    "            match_list = []\n",
    "            for i in annotation: \n",
    "                a, text_ = matcher(text, i[0])\n",
    "                match_list.append((a[0][0], a[0][1], i[1]))\n",
    "        \n",
    "            d = create_labs(text, match_list)\n",
    "\n",
    "            #for i in d.keys():\n",
    "            #    f.writelines(i + ' ' + d[i] +'\\n')\n",
    "            #f.writelines('\\n')\n",
    "            for i in range(d.shape[0]):\n",
    "                f.writelines(d['word'][i] + ' ' + d['label'][i] +'\\n')\n",
    "            f.writelines('\\n')\n",
    "def main():\n",
    "    ## An example dataframe.\n",
    "    #data = pd.DataFrame([[\"Horses are too tall and they pretend to care about your feelings\", [(\"Horses\", \"ANIMAL\")]],\n",
    "    #              [\"Who is Shaka Khan?\", [(\"Shaka Khan\", \"PERSON\")]],\n",
    "    #              [\"I like London and Berlin.\", [(\"London\", \"LOCATION\"), (\"Berlin\", \"LOCATION\")]],\n",
    "    #              [\"There is a banyan tree in the courtyard\", [(\"banyan tree\", \"TREE\")]]], columns=['text', 'annotation'])\n",
    "    data = test.copy()\n",
    "    #print(data.head())\n",
    "    ## path to save the txt file.\n",
    "    filepath = 'E:/JHU/课程/datadesign/NLP/machine_learning/testdata.txt'\n",
    "    ## creating the file.\n",
    "    create_data(data, filepath)\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "adae8747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(316727, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Record</td>\n",
       "      <td>B-SMOKER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>date:</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2069-04-07</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mr.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Villegas</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         text     label\n",
       "0      Record  B-SMOKER\n",
       "1       date:         O\n",
       "2  2069-04-07         O\n",
       "3         Mr.         O\n",
       "4    Villegas         O"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read txt file\n",
    "def read_txt(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        data = f.readlines()\n",
    "    return data\n",
    "\n",
    "list = read_txt('E:/JHU/课程/datadesign/NLP/machine_learning/testdata.txt')\n",
    "#print(list)\n",
    "for i in range(len(list)):\n",
    "    list[i] = list[i].split()\n",
    "#print(list)\n",
    "dataframe = pd.DataFrame(list,columns=['text','label'])\n",
    "print(dataframe.shape)\n",
    "dataframe.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe773fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25925, 17)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "df = dataframe.fillna(method='ffill')\n",
    "df.text.nunique(), df.label.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ea5ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-CAD</td>\n",
       "      <td>507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B-DIABETES</td>\n",
       "      <td>308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B-FAMILY_HIST</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B-HYPERLIPIDEMIA</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B-HYPERTENSION</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B-MEDICATION</td>\n",
       "      <td>1283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B-OBESE</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>B-SMOKER</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I-CAD</td>\n",
       "      <td>2406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I-DIABETES</td>\n",
       "      <td>414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I-FAMILY_HIST</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I-HYPERLIPIDEMIA</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>I-HYPERTENSION</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>I-MEDICATION</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>I-OBESE</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>I-SMOKER</td>\n",
       "      <td>596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>O</td>\n",
       "      <td>151769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               label  counts\n",
       "0              B-CAD     507\n",
       "1         B-DIABETES     308\n",
       "2      B-FAMILY_HIST      15\n",
       "3   B-HYPERLIPIDEMIA     167\n",
       "4     B-HYPERTENSION     314\n",
       "5       B-MEDICATION    1283\n",
       "6            B-OBESE      50\n",
       "7           B-SMOKER     287\n",
       "8              I-CAD    2406\n",
       "9         I-DIABETES     414\n",
       "10     I-FAMILY_HIST     114\n",
       "11  I-HYPERLIPIDEMIA      88\n",
       "12    I-HYPERTENSION     201\n",
       "13      I-MEDICATION     363\n",
       "14           I-OBESE       7\n",
       "15          I-SMOKER     596\n",
       "16                 O  151769"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('label').size().reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc63a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              text\n",
      "0           Record\n",
      "1            date:\n",
      "2       2069-04-07\n",
      "3              Mr.\n",
      "4         Villegas\n",
      "...            ...\n",
      "158884    Resident\n",
      "158885       Pager\n",
      "158886           #\n",
      "158887       04873\n",
      "158888       04873\n",
      "\n",
      "[158889 rows x 1 columns]\n",
      "  (0, 12421)\t1.0\n",
      "  (1, 17008)\t1.0\n",
      "  (2, 3219)\t1.0\n",
      "  (3, 10728)\t1.0\n",
      "  (4, 14144)\t1.0\n",
      "  (5, 19915)\t1.0\n",
      "  (6, 23595)\t1.0\n",
      "  (7, 24870)\t1.0\n",
      "  (8, 9343)\t1.0\n",
      "  (9, 19072)\t1.0\n",
      "  (10, 21375)\t1.0\n",
      "  (11, 23595)\t1.0\n",
      "  (12, 19271)\t1.0\n",
      "  (13, 23827)\t1.0\n",
      "  (14, 11112)\t1.0\n",
      "  (15, 6029)\t1.0\n",
      "  (16, 24763)\t1.0\n",
      "  (17, 25622)\t1.0\n",
      "  (18, 14918)\t1.0\n",
      "  (19, 19081)\t1.0\n",
      "  (20, 24211)\t1.0\n",
      "  (21, 19284)\t1.0\n",
      "  (22, 11926)\t1.0\n",
      "  (23, 21538)\t1.0\n",
      "  (24, 19284)\t1.0\n",
      "  :\t:\n",
      "  (158864, 1645)\t1.0\n",
      "  (158865, 13604)\t1.0\n",
      "  (158866, 19069)\t1.0\n",
      "  (158867, 15657)\t1.0\n",
      "  (158868, 17394)\t1.0\n",
      "  (158869, 25732)\t1.0\n",
      "  (158870, 7930)\t1.0\n",
      "  (158871, 9703)\t1.0\n",
      "  (158872, 12596)\t1.0\n",
      "  (158873, 14105)\t1.0\n",
      "  (158874, 13225)\t1.0\n",
      "  (158875, 8535)\t1.0\n",
      "  (158876, 14614)\t1.0\n",
      "  (158877, 9765)\t1.0\n",
      "  (158878, 9755)\t1.0\n",
      "  (158879, 10322)\t1.0\n",
      "  (158880, 14105)\t1.0\n",
      "  (158881, 24413)\t1.0\n",
      "  (158882, 983)\t1.0\n",
      "  (158883, 9776)\t1.0\n",
      "  (158884, 12498)\t1.0\n",
      "  (158885, 11696)\t1.0\n",
      "  (158886, 0)\t1.0\n",
      "  (158887, 1461)\t1.0\n",
      "  (158888, 1461)\t1.0\n"
     ]
    }
   ],
   "source": [
    "X = df.drop('label',axis=1)\n",
    "print (X)\n",
    "\n",
    "v = DictVectorizer(sparse=True)\n",
    "X = v.fit_transform(X.to_dict('records'))\n",
    "print(X)\n",
    "y = df.label.values\n",
    "\n",
    "classes = np.unique(y)\n",
    "classes = classes.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c4c306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-CAD',\n",
       " 'B-DIABETES',\n",
       " 'B-FAMILY_HIST',\n",
       " 'B-HYPERLIPIDEMIA',\n",
       " 'B-HYPERTENSION',\n",
       " 'B-MEDICATION',\n",
       " 'B-OBESE',\n",
       " 'B-SMOKER',\n",
       " 'I-CAD',\n",
       " 'I-DIABETES',\n",
       " 'I-FAMILY_HIST',\n",
       " 'I-HYPERLIPIDEMIA',\n",
       " 'I-HYPERTENSION',\n",
       " 'I-MEDICATION',\n",
       " 'I-OBESE',\n",
       " 'I-SMOKER']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_classes = classes.copy()\n",
    "new_classes.pop()\n",
    "new_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538e09a0",
   "metadata": {},
   "source": [
    "# Bunch of code for testing errors\n",
    "## trash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e5bb45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1516, 1523, 'MEDICATION'),\n",
       " (1516, 1523, 'MEDICATION'),\n",
       " (1516, 1523, 'MEDICATION'),\n",
       " (1516, 1523, 'MEDICATION'),\n",
       " (397, 405, 'MEDICATION'),\n",
       " (397, 405, 'MEDICATION'),\n",
       " (397, 405, 'MEDICATION'),\n",
       " (1516, 1523, 'MEDICATION'),\n",
       " (1516, 1523, 'MEDICATION'),\n",
       " (397, 405, 'MEDICATION'),\n",
       " (397, 405, 'MEDICATION'),\n",
       " (397, 405, 'MEDICATION'),\n",
       " (397, 405, 'MEDICATION'),\n",
       " (397, 405, 'MEDICATION'),\n",
       " (397, 405, 'MEDICATION'),\n",
       " (410, 422, 'HYPERTENSION'),\n",
       " (410, 422, 'HYPERTENSION'),\n",
       " (410, 422, 'HYPERTENSION'),\n",
       " (410, 422, 'HYPERTENSION'),\n",
       " (410, 422, 'HYPERTENSION'),\n",
       " (410, 422, 'HYPERTENSION'),\n",
       " (410, 422, 'HYPERTENSION'),\n",
       " (410, 422, 'HYPERTENSION'),\n",
       " (410, 422, 'HYPERTENSION'),\n",
       " (3, 9, 'SMOKER')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def matcher(string, pattern):\n",
    "    '''\n",
    "    Return the start and end index of any pattern present in the text.\n",
    "    '''\n",
    "    match_list = []\n",
    "    pattern = pattern.strip()\n",
    "    seqMatch = SequenceMatcher(None, string, pattern, autojunk=False)\n",
    "    match = seqMatch.find_longest_match(0, len(string), 0, len(pattern))\n",
    "    if (match.size == len(pattern)):\n",
    "        start = match.a\n",
    "        end = match.a + match.size\n",
    "        match_tup = (start, end)\n",
    "        string = string.replace(pattern, \"X\" * len(pattern), 1)\n",
    "        match_list.append(match_tup)\n",
    "        \n",
    "    return match_list, string\n",
    "text = test['text'][0]\n",
    "annotation = test['annotation'][0]\n",
    "#pattern = pattern.strip()\n",
    "match_list = []\n",
    "for i in annotation:\n",
    "    a, text_ = matcher(text, i[0])\n",
    "    match_list.append((a[0][0], a[0][1], i[1]))\n",
    "\n",
    "\n",
    "match_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2201cccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labs(s,match_list):\n",
    "    labs = ['O' for i in range(len(s.split()))]\n",
    "    word_dict = pd.DataFrame({'word':s.split(),'label':labs})\n",
    "\n",
    "    for start, end, e_type in match_list:\n",
    "        index = len(re.findall(r' +',s[0:start]))-1\n",
    "        num_words = len(s[start:end].split())\n",
    "        index,num_words\n",
    "\n",
    "        if num_words > 1:\n",
    "            word_dict.loc[index,'label'] = 'B-' + e_type\n",
    "            for i in range(1,num_words):\n",
    "                word_dict.loc[index+i,'label'] = 'I-' + e_type\n",
    "        else:\n",
    "            word_dict.loc[index,'label'] = 'B-' + e_type\n",
    "    return word_dict\n",
    "\n",
    "x = create_labs(test['text'][0],match_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561cf224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Record</td>\n",
       "      <td>B-SMOKER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>date:</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2069-04-07</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mr.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Villegas</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>reviewed</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>by</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>Attending</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>Provider</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>********</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>295 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           word     label\n",
       "0        Record  B-SMOKER\n",
       "1         date:         O\n",
       "2    2069-04-07         O\n",
       "3           Mr.         O\n",
       "4      Villegas         O\n",
       "..          ...       ...\n",
       "290    reviewed         O\n",
       "291          by         O\n",
       "292   Attending         O\n",
       "293    Provider         O\n",
       "294    ********         O\n",
       "\n",
       "[295 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f855646",
   "metadata": {},
   "source": [
    "## test how to find the word using index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3754ca0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32me:\\JHU\\课程\\datadesign\\NLP\\machine_learning\\get_testing_data.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/JHU/%E8%AF%BE%E7%A8%8B/datadesign/NLP/machine_learning/get_testing_data.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m a \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49msplit()\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/JHU/%E8%AF%BE%E7%A8%8B/datadesign/NLP/machine_learning/get_testing_data.ipynb#X33sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m a\u001b[39m.\u001b[39mindex(\u001b[39m'\u001b[39m\u001b[39msmoking\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/JHU/%E8%AF%BE%E7%A8%8B/datadesign/NLP/machine_learning/get_testing_data.ipynb#X33sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mlen\u001b[39m(re\u001b[39m.\u001b[39mfindall(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m +\u001b[39m\u001b[39m'\u001b[39m,x[\u001b[39m0\u001b[39m:start]))\n",
      "File \u001b[1;32md:\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:5575\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5568\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   5569\u001b[0m     name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_names_set\n\u001b[0;32m   5570\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\n\u001b[0;32m   5571\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessors\n\u001b[0;32m   5572\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis\u001b[39m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   5573\u001b[0m ):\n\u001b[0;32m   5574\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[name]\n\u001b[1;32m-> 5575\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getattribute__\u001b[39;49m(\u001b[39mself\u001b[39;49m, name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "a = s.split()\n",
    "a.index('smoking')\n",
    "len(re.findall(r' +',s[0:start]))\n",
    "a[86],len(s[start:end].split())\n",
    "#temp_str = pd.DataFrame(s.split(), columns=['word'])\n",
    "#temp_str['len'] = temp_str['word'].apply(lambda x: len(x))\n",
    "#temp_str['cumsum'] = temp_str['len'].cumsum()\n",
    "#temp_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2de0ab",
   "metadata": {},
   "source": [
    "## find which label went crazy and can't match text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a7771983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('High cholesterol', 'HYPERLIPIDEMIA')\n",
      "[(974, 990)]\n",
      "('High cholesterol ', 'HYPERLIPIDEMIA')\n",
      "[(974, 990)]\n",
      "('High cholesterol', 'HYPERLIPIDEMIA')\n",
      "[(974, 990)]\n",
      "('high chol,', 'HYPERLIPIDEMIA')\n",
      "[(2605, 2615)]\n",
      "('High cholesterol', 'HYPERLIPIDEMIA')\n",
      "[(974, 990)]\n",
      "('High cholesterol ', 'HYPERLIPIDEMIA')\n",
      "[(974, 990)]\n",
      "('High cholesterol', 'HYPERLIPIDEMIA')\n",
      "[(974, 990)]\n",
      "('high chol,', 'HYPERLIPIDEMIA')\n",
      "[(2605, 2615)]\n",
      "('High cholesterol', 'HYPERLIPIDEMIA')\n",
      "[(974, 990)]\n",
      "('High cholesterol ', 'HYPERLIPIDEMIA')\n",
      "[(974, 990)]\n",
      "('High cholesterol', 'HYPERLIPIDEMIA')\n",
      "[(974, 990)]\n",
      "('high chol,', 'HYPERLIPIDEMIA')\n",
      "[(2605, 2615)]\n",
      "('ASA', 'MEDICATION')\n",
      "[(756, 759)]\n",
      "('ASA', 'MEDICATION')\n",
      "[(756, 759)]\n",
      "('ASA', 'MEDICATION')\n",
      "[(756, 759)]\n",
      "('ASA', 'MEDICATION')\n",
      "[(756, 759)]\n",
      "('Plavix', 'MEDICATION')\n",
      "[(1094, 1100)]\n",
      "('Plavix', 'MEDICATION')\n",
      "[(1094, 1100)]\n",
      "('ASA', 'MEDICATION')\n",
      "[(756, 759)]\n",
      "('ASA', 'MEDICATION')\n",
      "[(756, 759)]\n",
      "('ASA', 'MEDICATION')\n",
      "[(756, 759)]\n",
      "('ASA', 'MEDICATION')\n",
      "[(756, 759)]\n",
      "('Norvasc', 'MEDICATION')\n",
      "[(1066, 1073)]\n",
      "('Norvasc', 'MEDICATION')\n",
      "[(1066, 1073)]\n",
      "('Norvasc', 'MEDICATION')\n",
      "[(1066, 1073)]\n",
      "('Insulin pump', 'MEDICATION')\n",
      "[(1124, 1136)]\n",
      "('Insulin', 'MEDICATION')\n",
      "[(1124, 1131)]\n",
      "('Norvasc', 'MEDICATION')\n",
      "[(1066, 1073)]\n",
      "('Norvasc', 'MEDICATION')\n",
      "[(1066, 1073)]\n",
      "('Norvasc', 'MEDICATION')\n",
      "[(1066, 1073)]\n",
      "('Plavix', 'MEDICATION')\n",
      "[(1094, 1100)]\n",
      "('Plavix', 'MEDICATION')\n",
      "[(1094, 1100)]\n",
      "('Toprol', 'MEDICATION')\n",
      "[(1037, 1043)]\n",
      "('Toprol', 'MEDICATION')\n",
      "[(1037, 1043)]\n",
      "('Toprol', 'MEDICATION')\n",
      "[(1037, 1043)]\n",
      "('Diovan', 'MEDICATION')\n",
      "[(1051, 1057)]\n",
      "('Diovan', 'MEDICATION')\n",
      "[(1051, 1057)]\n",
      "('ASA', 'MEDICATION')\n",
      "[(756, 759)]\n",
      "('ASA', 'MEDICATION')\n",
      "[(756, 759)]\n",
      "('ASA', 'MEDICATION')\n",
      "[(756, 759)]\n",
      "('ASA', 'MEDICATION')\n",
      "[(756, 759)]\n",
      "('Insulin pump', 'MEDICATION')\n",
      "[(1124, 1136)]\n",
      "('Insulin', 'MEDICATION')\n",
      "[(1124, 1131)]\n",
      "('Norvasc', 'MEDICATION')\n",
      "[(1066, 1073)]\n",
      "('Norvasc', 'MEDICATION')\n",
      "[(1066, 1073)]\n",
      "('Norvasc', 'MEDICATION')\n",
      "[(1066, 1073)]\n",
      "('Plavix', 'MEDICATION')\n",
      "[(1094, 1100)]\n",
      "('Plavix', 'MEDICATION')\n",
      "[(1094, 1100)]\n",
      "('Insulin pump', 'MEDICATION')\n",
      "[(1124, 1136)]\n",
      "('Insulin', 'MEDICATION')\n",
      "[(1124, 1131)]\n",
      "('Toprol', 'MEDICATION')\n",
      "[(1037, 1043)]\n",
      "('Toprol', 'MEDICATION')\n",
      "[(1037, 1043)]\n",
      "('Toprol', 'MEDICATION')\n",
      "[(1037, 1043)]\n",
      "('Toprol', 'MEDICATION')\n",
      "[(1037, 1043)]\n",
      "('Toprol', 'MEDICATION')\n",
      "[(1037, 1043)]\n",
      "('Toprol', 'MEDICATION')\n",
      "[(1037, 1043)]\n",
      "('Diovan', 'MEDICATION')\n",
      "[(1051, 1057)]\n",
      "('Diovan', 'MEDICATION')\n",
      "[(1051, 1057)]\n",
      "('Diovan', 'MEDICATION')\n",
      "[(1051, 1057)]\n",
      "('Diovan', 'MEDICATION')\n",
      "[(1051, 1057)]\n",
      "('HTN', 'HYPERTENSION')\n",
      "[(222, 225)]\n",
      "('HTN', 'HYPERTENSION')\n",
      "[(222, 225)]\n",
      "('HTN', 'HYPERTENSION')\n",
      "[(222, 225)]\n",
      "('HTN', 'HYPERTENSION')\n",
      "[(222, 225)]\n",
      "('HTN', 'HYPERTENSION')\n",
      "[(222, 225)]\n",
      "('BP: 150/100', 'HYPERTENSION')\n",
      "[(1615, 1626)]\n",
      "('150/100  ', 'HYPERTENSION')\n",
      "[(1619, 1626)]\n",
      "(' -BP: 150/100', 'HYPERTENSION')\n",
      "[(1614, 1626)]\n",
      "('HTN', 'HYPERTENSION')\n",
      "[(222, 225)]\n",
      "('HTN', 'HYPERTENSION')\n",
      "[(222, 225)]\n",
      "('HTN', 'HYPERTENSION')\n",
      "[(222, 225)]\n",
      "('HTN', 'HYPERTENSION')\n",
      "[(222, 225)]\n",
      "('HTN', 'HYPERTENSION')\n",
      "[(222, 225)]\n",
      "('HTN', 'HYPERTENSION')\n",
      "[(222, 225)]\n",
      "('HTN', 'HYPERTENSION')\n",
      "[(222, 225)]\n",
      "('HTN', 'HYPERTENSION')\n",
      "[(222, 225)]\n",
      "('HTN', 'HYPERTENSION')\n",
      "[(222, 225)]\n",
      "('HTN', 'HYPERTENSION')\n",
      "[(222, 225)]\n",
      "('CAD', 'CAD')\n",
      "[(204, 207)]\n",
      "('CAD', 'CAD')\n",
      "[(204, 207)]\n",
      "('CAD', 'CAD')\n",
      "[(204, 207)]\n",
      "('progressive angina ', 'CAD')\n",
      "[(2631, 2649)]\n",
      "('progressive angina', 'CAD')\n",
      "[(2631, 2649)]\n",
      "('progressive angina ', 'CAD')\n",
      "[(2631, 2649)]\n",
      "('progressive angina', 'CAD')\n",
      "[(2631, 2649)]\n",
      "('CAD', 'CAD')\n",
      "[(204, 207)]\n",
      "('CAD', 'CAD')\n",
      "[(204, 207)]\n",
      "('CAD', 'CAD')\n",
      "[(204, 207)]\n",
      "('progressive angina ', 'CAD')\n",
      "[(2631, 2649)]\n",
      "('progressive angina', 'CAD')\n",
      "[(2631, 2649)]\n",
      "('CAD', 'CAD')\n",
      "[(204, 207)]\n",
      "('CAD', 'CAD')\n",
      "[(204, 207)]\n",
      "('CAD', 'CAD')\n",
      "[(204, 207)]\n",
      "('Never smoked', 'SMOKER')\n",
      "[(1327, 1339)]\n",
      "('Never smoked', 'SMOKER')\n",
      "[(1327, 1339)]\n",
      "('DM', 'DIABETES')\n",
      "[(218, 220)]\n",
      "('DM', 'DIABETES')\n",
      "[(218, 220)]\n",
      "('DM', 'DIABETES')\n",
      "[(218, 220)]\n",
      "('IDDM', 'DIABETES')\n",
      "[(778, 782)]\n",
      "('IDDMm', 'DIABETES')\n",
      "[]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32me:\\JHU\\课程\\datadesign\\NLP\\machine_learning\\get_testing_data.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/JHU/%E8%AF%BE%E7%A8%8B/datadesign/NLP/machine_learning/get_testing_data.ipynb#X24sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m a, text_ \u001b[39m=\u001b[39m matcher(text, i[\u001b[39m0\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/JHU/%E8%AF%BE%E7%A8%8B/datadesign/NLP/machine_learning/get_testing_data.ipynb#X24sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(a) \n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/JHU/%E8%AF%BE%E7%A8%8B/datadesign/NLP/machine_learning/get_testing_data.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m match_list\u001b[39m.\u001b[39mappend((a[\u001b[39m0\u001b[39;49m][\u001b[39m0\u001b[39m], a[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m], i[\u001b[39m1\u001b[39m]))\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "match_list = []\n",
    "text = test['text'][262]\n",
    "#text = \"Horses are too tall and they pretend to care about your feelings\"\n",
    "annotation = test['annotation'][262]\n",
    "for i in annotation:\n",
    "    print(i)\n",
    "    a, text_ = matcher(text, i[0])\n",
    "    print(a) \n",
    "    match_list.append((a[0][0], a[0][1], i[1]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eafaa13d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"   Record date: 2094-08-11  CARDIOLOGY  COQUILLE VALLEY HOSPITAL    Reason for visit:     Evaluation for cardiac catheterization with Dr. Irvin Vitale.     Interval History:     Multiple risk factors for CAD including DM, HTN, CRI. Now with complaints of exertional dyspnea and fatigue. Progressed and she was evaluated by Bonnie Eaves.     8/25 Bruce protocol was postive at 9 minutes, 64% PMHR. Complaints of dyspnea. EKG with borderline changes. Nuclear images with moderate sized defect of anteroapical zone with partial reperfusion. Some scar. Mild LV dysfunction with EF 45-50% and apical dyskinesis.     Arranged for elective cardiac catheterization with Dr Vitale.     Past medical history:     CVA of left PCA territory 2093. No residual. Rx with ASA, folate, niaspan  IDDM diagnosed 30 yrs ago  CRI with creat baseline 3.7 (followed by Orlando Ernst) and recent eval by Dr Ratliff for transplant. +proteinuria with nephrotic syndrome. Donor kidney lined up.  HTN  High cholesterol  Diabetic retinopathy  Anemia    Medications: Toprol 25 QD  Diovan 160 QD  Norvasc 10 QD  Lasix 80 QD  Plavix 75 QD  ASA 81 QD  MVI  Insulin pump  Vytorin QD    Family history:     Mom A&W in her 60's with HTN. Dad A&W.     Social history:     Works as Patternmaker at IMN.   volunteer firefighter. Single and lives with her parents.   Never smoked. No ETOH.     Review of systems:     no peripheral edema currently (but had it in the past). No fever, chills, sweating. Problems with gastroparesis and is planning to have a gastric pacemaker inserted (has been having delayed spikes in insulin).     Physical examination:  -BP: 150/100  L  154-90 R  -Pulse: 70    -resp. rate: 16    -weight: 153    -BMI:     -General appearance:   No acute distress.  -Skin:   No rashes, anicteric.  -Heent:   Unremarkable  -Neck:   Carotids 2+ without bruits. JVP no jugular venous distention  -Chest:   Clear to auscultation and percussion.  -Cardiac:   Left ventricular impulse discrete and nondisplaced. Regular rate and rhythm, normal S1 and S2, with no S3 or S4. There were no murmurs, clicks or rubs.  -Abdomen:   Normal bowel sounds, soft and nontender, with no hepatosplenomegaly or masses appreciated.  -Extremities:   No cyanosis, clubbing or edema. 2+ femoral pulses without bruits. 2+ pedal pulses.  -Neuro:   A&O x3, CN 2-12 grossly intact. Reflexes 2+ and symmetric x 4 extremities. Toes B downgoing.    EKG:           Selected recent labs:   8/4/94:   WBC 7.1, Crit 35.2, Plts 274.   INR 0.9. Na 139, K 4.3, Creat 3.4.   AST/ALT 39-30.     Assessment and plan:     40 y.o. with multiple risk factors for CAD. HTN, high chol, IDDM. Now with progressive angina and a positive ETT. Plan for cath. She will probably end up on dialysis given severe renal failure. Pretreated with musomyst.     Give bicarb in holding room.     Impression:     Further plan per Dr Vitale and Dr Rollins.   Follow up with Dr Bonnie Eaves.       Frances Travis Potts NP  23119    Signed electronically by   Frances T Potts NP  on  Aug 11, 2094         \""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['text'][262]\n",
    "#tree = ET.parse(file_path + names[62])\n",
    "#root = tree.getroot()\n",
    "#root[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06f3dd94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'231-01.xml'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names[262]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e1f304d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Atenolol'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = ET.parse(file_path + names[0])\n",
    "root = tree.getroot()\n",
    "## Get the labels\n",
    "# skip FAMILY_HIST: doesn't have a text\n",
    "#skip = [root[1][x].tag for x in range(len(root[1]))].index('FAMILY_HIST')\n",
    "#PHI = [root[1][x].tag for x in range(len(root[1]))].index('PHI')\n",
    "# get all labels except for PHI\n",
    "tag_list = []\n",
    "root[1][5][0].attrib['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc82e213",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "49cb93f377a7abe7414b7b0f21fb3017538004a126cf690fb524202736b7fb92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
